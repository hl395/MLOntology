{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global variabls\n",
    "\n",
    "directory_path = \"D:/MLOntology/\"\n",
    "data_path = directory_path + \"data/\"\n",
    "vector_model_path = directory_path +\"vectorModel/\"\n",
    "cnn_model_path = directory_path +\"cnnModel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conceptLabelDict={}\n",
    "errors=[]\n",
    "\n",
    "def read_label(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptLabelDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "label_file = data_path + \"ontClassLabels_july2017.txt\"\n",
    "read_label(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['37225000', '52860004', 1], ['159386001', '159385002', 1], ['233836002', '233835003', 1], ['233836002', '304914007', 1], ['224923003', '224717003', 1]]\n",
      "502459\n"
     ]
    }
   ],
   "source": [
    "conceptPairDict={}\n",
    "errors=[]\n",
    "conceptPairList=[]\n",
    "\n",
    "def read_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptPairList.append([splitted[1], splitted[2].replace(\"\\r\\n\", \"\"), 1])\n",
    "#                 conceptPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "pair_file = data_path + \"ontHierarchy_july2017.txt\"\n",
    "read_pair(pair_file)\n",
    "\n",
    "first2pairs = conceptPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['273187009', '272765000', 0], ['272877001', '272765000', 0], ['273216002', '272765000', 0], ['273125004', '272765000', 0], ['272973003', '272765000', 0]]\n",
      "6167243\n",
      "502459\n"
     ]
    }
   ],
   "source": [
    "conceptNotPairDict={}\n",
    "conceptNotPairList=[]\n",
    "\n",
    "def read_not_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                conceptNotPairList.append([splitted[0], splitted[1].replace(\"\\r\\n\", \"\"), 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "notPair_file = data_path + \"taxNotPairs_july2017.txt\"\n",
    "read_not_pair(notPair_file)\n",
    "\n",
    "# first2pairs = {k: conceptNotPairDict[k] for k in list(conceptNotPairDict)[10:15]}\n",
    "first2pairs =conceptNotPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptNotPairList))\n",
    "\n",
    "# In-place shuffle\n",
    "random.shuffle(conceptNotPairList)\n",
    "conceptNotPairList = conceptNotPairList[:len(conceptPairList)]\n",
    "\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('237267007', 0.5851410627365112),\n",
      " ('722912007', 0.5491856336593628),\n",
      " ('722913002', 0.5397146344184875),\n",
      " ('446466006', 0.5239850878715515),\n",
      " ('10759711000119103', 0.5149962902069092),\n",
      " ('267262008', 0.5099971890449524),\n",
      " ('10759661000119108', 0.5064620971679688),\n",
      " ('177130000', 0.5053495168685913),\n",
      " ('10759611000119105', 0.5002419352531433),\n",
      " ('12729009', 0.5001755952835083)]\n"
     ]
    }
   ],
   "source": [
    "vector_model_file = vector_model_path + \"model0\"\n",
    "\n",
    "vector_model = gensim.models.Doc2Vec.load(vector_model_file)\n",
    "\n",
    "inferred_vector = vector_model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(vector_model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/MLOntology/model1\"\n",
    "\n",
    "# model = gensim.models.Doc2Vec.load(path)\n",
    "\n",
    "# inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "# pprint(model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "feature_number = 1024\n",
    "\n",
    "def readFromPairList(id_pair_list, id_notPair_list):\n",
    "    pair_list = id_pair_list + id_notPair_list\n",
    "    random.shuffle(pair_list)\n",
    "    idpairs_list =[]\n",
    "    label_list =[]\n",
    "    for i, line in enumerate(pair_list):      \n",
    "        idpairs_list.append([line[0], line[1]])\n",
    "        label_list.append(line[2])\n",
    "    return idpairs_list, label_list\n",
    "\n",
    "idpairs_list, label_list= readFromPairList(conceptPairList, conceptNotPairList)\n",
    "\n",
    "print(label_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['228284005', '236636005'], ['119754009', '120154005'], ['274530001', '168572009'], ['66214007', '6058003'], ['58764007', '61011009'], ['403498005', '118930001'], ['73544002', '64915003'], ['119885009', '118832005'], ['92166000', '92103006'], ['124202004', '124276000'], ['329464003', '423853002'], ['386789004', '365601007'], ['706790004', '469991006'], ['386433009', '386432004'], ['417928002', '284622002'], ['105296006', '438830009'], ['296709001', '296714002'], ['271223005', '574005'], ['61837008', '41898006'], ['409894009', '387085005']]\n",
      "[['228284005', '236636005'], ['119754009', '120154005'], ['274530001', '168572009'], ['66214007', '6058003'], ['58764007', '61011009'], ['403498005', '118930001'], ['73544002', '64915003'], ['119885009', '118832005'], ['92166000', '92103006'], ['124202004', '124276000'], ['329464003', '423853002'], ['386789004', '365601007'], ['706790004', '469991006'], ['386433009', '386432004'], ['417928002', '284622002'], ['105296006', '438830009'], ['296709001', '296714002'], ['271223005', '574005'], ['61837008', '41898006'], ['409894009', '387085005']]\n",
      "[1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "[0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(idpairs_list, label_list, test_size = 0.2)\n",
    "print(X_train[:20])\n",
    "print(X_train[:20])\n",
    "print(y_train[:20])\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_number = 1024\n",
    "\n",
    "def getInferredVector(concept_id, conceptLabelDict, model):\n",
    "    concept_label = conceptLabelDict[concept_id]\n",
    "    concept_vector= model.infer_vector(concept_label.split())\n",
    "    return concept_vector\n",
    "\n",
    "def getVectorFromModel(pair_list, conceptLabelDict, model):\n",
    "    vector_list =[]\n",
    "    for i, line in enumerate(pair_list):        \n",
    "        a= getInferredVector(line[0], conceptLabelDict, model)\n",
    "        b= getInferredVector(line[1], conceptLabelDict, model)\n",
    "        c = np.array((a, b))\n",
    "#         test_list.append(np.reshape(c, feature_number))\n",
    "        vector_list.append(np.reshape(c, feature_number, order='F'))\n",
    "    return vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 100\n",
      "Train Loss: 0.69118553 Acc: [0.51]\n",
      "None 200\n",
      "Train Loss: 0.690885 Acc: [0.46]\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "'''\n",
    "In the data, there are 2 classes and every sample has 512 features\n",
    "'''\n",
    "# DATA_DIR = ''\n",
    "CLASS_NUM = 2       #there are 2 classes\n",
    "FEATURE_NUM = 1024   \n",
    "TRAIN_ITER = 300    #the number of iterations for training\n",
    "display_step = 100        #how many iterations to display the results\n",
    "train_batch_size = 100\n",
    "\n",
    "\n",
    "train_feature = X_train     #training features (list of list)\n",
    "train_y = y_train        #training lables    (list)\n",
    "test_feature = X_test       #test features  (list of list)\n",
    "test_y = y_test         #test labels    (list)\n",
    "\n",
    "\n",
    "y_m = np.eye(2)[train_y]\n",
    "test_y_m = np.eye(2)[test_y]\n",
    "\n",
    "'''\n",
    "y = wx+b        (vectors)\n",
    "'''\n",
    "#function to get variables 'w'\n",
    "def weight_variable(shape, num):\n",
    "    initial = tf.truncated_normal(shape, stddev=1/num)\n",
    "    return tf.Variable(initial, name='weight')\n",
    "\n",
    "#the bias 'b' in the equations\n",
    "def bias_variable(shape, num):\n",
    "    initial = tf.constant(0.0001, shape=shape)\n",
    "    return tf.Variable(initial, name='bias')\n",
    "\n",
    "#convolutional process\n",
    "def conv1d(x, W):\n",
    "    return tf.nn.conv1d(x, W, stride=1, padding='SAME')     #x: variable, w: weight, stride and padding (padding can be ignored currently) \n",
    "\n",
    "#pooling process\n",
    "def max_pool_1x1(x, shape):\n",
    "    x=tf.reshape(x,shape)       #it is transfered into four dimensions, but the other three are 1\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 4, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "'''\n",
    "The feature is 3 dimensional data.  [batch, length, channel] \n",
    "batch is usually ignored (for example there are 100 samples in a batch, so samples should not be modified mutually), length and channel are shown in the paper.\n",
    "At first, the length is 512, and channel is 1.\n",
    "Because our data are time series data, so length is enough, but for images, it may be [batch, length, width, channel]\n",
    "'''\n",
    "# the convolutional layer\n",
    "def layer(features, f, input_n, channel, hidden_units, layer_index):\n",
    "    \"\"\"Construct a convolutional layer\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    f: the length\n",
    "    input_n: Size of the features used in the convention.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    layer_index: the index of layer\n",
    "    Returns:\n",
    "    hidden units: The unit output for the next layer.\n",
    "    weights: the weights in the current hidden layer\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden'+str(layer_index)) as scope:     # name scope may be ignored first\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, channel, hidden_units], math.sqrt(f))\n",
    "\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(f))\n",
    "    hidden = relu(conv1d(features, weights) + biases, 0.01)\n",
    "    shape = [-1,1,f,hidden_units]\n",
    "    h_pool1 = max_pool_1x1(hidden,shape)\n",
    "    return h_pool1, weights\n",
    "\n",
    "# fully connected layer, here the data are two dimension, [batch, length]\n",
    "def densely_connect(features, input_n, hidden_units):\n",
    "    \"\"\"Construct a fully (densely) connected layer.\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    input_n: Size of units in the previous layer.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    Returns:\n",
    "    logits: The estimated output in last layer.\n",
    "    weights: the weights in the hidden layer\n",
    "    \"\"\"\n",
    "    with tf.name_scope('softmax_linear') as dense:\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    logits = relu(tf.matmul(features, weights) + biases, 0.01)      # the matrix product operation\n",
    "    return logits, weights\n",
    "\n",
    "# dropout layer (it is not necessary)\n",
    "# randomly set (1-keep_prob) percentage of units to be zero\n",
    "def dropout(features, input_n, hidden_units, keep_prob):\n",
    "    with tf.name_scope('dropout'):\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    h_fc1_drop = tf.nn.dropout(features, keep_prob)\n",
    "    drop_out = relu(tf.matmul(features, weights) + biases, 0.01)\n",
    "    return drop_out\n",
    "\n",
    "# calculate the loss in the neural network\n",
    "def loss(logits, labels):\n",
    "    \"\"\"Calculates the loss from the logits and the labels.\n",
    "    Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size, NUM_CLASSES].\n",
    "    Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        labels = tf.to_int64(labels)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=labels, logits=logits, name='xentropy')\n",
    "    # tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "def next_batch(data, label, num):\n",
    "    \"\"\"Generate the next batch randomly\n",
    "    Args:\n",
    "    data: training data.\n",
    "    label: training label.\n",
    "    num: the size in a batch\n",
    "    Returns:\n",
    "    next batch's training features and labels.\n",
    "    \"\"\"\n",
    "    index = np.arange(len(data))\n",
    "    np.random.shuffle(index)\n",
    "#     train_feature = data[np.array(index)[0:num]]\n",
    "#     train_label = label[np.array(index)[0:num]]\n",
    "#     return train_feature, train_label\n",
    "    train_feature_batch = [data[b] for b in index[0:num]]\n",
    "    train_feature_batch_vector = getVectorFromModel(train_feature_batch, conceptLabelDict, vector_model)\n",
    "    train_feature_batch_vector = np.asarray(train_feature_batch_vector)\n",
    "    train_label_batch = [label[b] for b in index[0:num]]\n",
    "    train_label_batch = np.asarray(train_label_batch)    \n",
    "    return train_feature_batch_vector, train_label_batch\n",
    "\n",
    "def relu(x, alpha=0., max_value=None):\n",
    "    '''ReLU.\n",
    "    alpha: slope of negative section.\n",
    "    '''\n",
    "    negative_part = tf.nn.relu(-x)\n",
    "    x = tf.nn.relu(x)\n",
    "    if max_value is not None:\n",
    "        x = tf.clip_by_value(x, tf.cast(0., dtype=tf.float32),\n",
    "                             tf.cast(max_value, dtype=tf.float32))\n",
    "    x -= tf.constant(alpha, dtype=tf.float32) * negative_part\n",
    "    return x\n",
    "\n",
    "#define a session to run the model\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#place holders for training features and label\n",
    "#None means the value is variable\n",
    "x = tf.placeholder(tf.float32, shape=[None, FEATURE_NUM], name =\"input_vector\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, CLASS_NUM], name = \"class_label\")\n",
    "\n",
    "# decide whether it is training or testing, it is not used in our model, but it may be used\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "#from [-1, 512, 1] -> [-1, 256, 32] -> [-1, 128, 64] -> [-1, 64, 64] -> [-1, 32, 64] -> [-1, 16, 64] -> [-1, 8, 64] -> [-1, 200]\n",
    "\n",
    "#6 hidden layers\n",
    "x_1 = tf.reshape(x, [-1,FEATURE_NUM,1])\n",
    "h_pool0, w0 = layer(x_1, FEATURE_NUM, 15, 1, 32, 0)\n",
    "h_pool0 = tf.reshape(h_pool0, [-1,512,32])\n",
    "h_pool1, w1 = layer(h_pool0, 512, 10, 32, 64, 1)\n",
    "h_pool1 = tf.reshape(h_pool1, [-1,256,64])\n",
    "h_pool2, w2 = layer(h_pool1, 256, 10, 64, 64, 2)\n",
    "h_pool2 = tf.reshape(h_pool2, [-1,128,64])\n",
    "h_pool3, w3 = layer(h_pool2, 128, 10, 64, 64, 3)\n",
    "h_pool3 = tf.reshape(h_pool3, [-1,64,64])\n",
    "h_pool4, w4 = layer(h_pool3, 64, 5, 64, 64, 4)\n",
    "h_pool4 = tf.reshape(h_pool4, [-1,32,64])\n",
    "h_pool5, w5 = layer(h_pool4, 32, 5, 64, 64, 5)\n",
    "h_pool5 = tf.reshape(h_pool5, [-1,16,64])\n",
    "h_pool6, w6 = layer(h_pool5, 16, 5, 64, 64, 6)\n",
    "h_pool6 = tf.reshape(h_pool6, [-1,8,64])\n",
    "\n",
    "#densely connected: 200 units\n",
    "h_pool_flat = tf.reshape(h_pool6, [-1, 8*64])\n",
    "h_dc, w_d = densely_connect(h_pool_flat, 8*64, 200)\n",
    "\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y_conv=dropout(h_dc, (int)(h_dc.get_shape()[1]), CLASS_NUM, keep_prob)\n",
    "\n",
    "\n",
    "beta = 0.001\n",
    "cross_entropy = loss(y_conv, y_)\n",
    "loss = cross_entropy +beta*(tf.nn.l2_loss(w0) + tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2)+tf.nn.l2_loss(w3)+tf.nn.l2_loss(w4)+tf.nn.l2_loss(w5)+tf.nn.l2_loss(w6)+tf.nn.l2_loss(w_d))  #L2 regularization\n",
    "epsilon = 1e-5      # learning rate\n",
    "train_step = tf.train.AdamOptimizer(epsilon).minimize(loss)     #optimization function, our goal is to minimize the loss\n",
    "\n",
    "predict = tf.argmax(y_conv,1, name =\"predict\")   #the predicted class\n",
    "probability = tf.nn.softmax(y_conv, name=\"probability\")  #the predicted probability\n",
    "\n",
    "# calculate the accuray, the corrected classified divided by the total size\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1), name=\"correct_prediction\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name=\"accuracy\")\n",
    "\n",
    "#saver to save the training check point\n",
    "# variables can be restored in a new model by 'saver.restore(sess, save_path)'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())  #initialize the variables\n",
    "\n",
    "\n",
    "for i in range(1,TRAIN_ITER):       #training iterations\n",
    "    d, l = next_batch(train_feature, y_m, train_batch_size)      # get batch_size samples in one batch\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d), len(l)))\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d[0]), len(l[0])))\n",
    "    _, ls=sess.run([train_step,cross_entropy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:True})     #run the train step (optimization function), the second one is just to show the loss in this iteration.   THE FEED dictionary is to feed the place holders which are needed in the optimization function.\n",
    "    \n",
    "    if i%display_step==0:\n",
    "        print(_, i)\n",
    "        acc = sess.run([accuracy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:False})\n",
    "        print(\"Train Loss:\", ls, \"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model results\n",
    "save_path = saver.save(sess, cnn_model_path + \"model-noleaky.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature len  200984\n",
      "label len  200984\n",
      "196 iterations in total\n",
      "[0.5103878  0.5141214  0.5204867  ... 0.50858593 0.521889   0.5111781 ]\n",
      "[0.5203107  0.5220207  0.5119409  ... 0.51485014 0.5158626  0.5250098 ]\n",
      "[0.50877905 0.51450837 0.5171261  ... 0.51070404 0.52264875 0.51385844]\n",
      "[0.5165692  0.5218987  0.5176706  ... 0.5062199  0.51866484 0.5273182 ]\n",
      "[0.53162223 0.51026887 0.51535106 ... 0.5125212  0.52240753 0.5112776 ]\n",
      "[0.51803654 0.5202344  0.5144007  ... 0.5121566  0.50945395 0.5148882 ]\n",
      "[0.50961566 0.5078465  0.50770885 ... 0.5225208  0.51761687 0.5135645 ]\n",
      "[0.5166318  0.5234635  0.51271474 ... 0.52218294 0.5307162  0.5237374 ]\n",
      "[0.511525   0.51094633 0.50610095 ... 0.516732   0.51440984 0.5253612 ]\n",
      "[0.5248963  0.51630235 0.52068454 ... 0.5145146  0.5133762  0.51320285]\n",
      "[0.5180866  0.5086271  0.50933146 ... 0.5156291  0.51588523 0.51736057]\n",
      "[0.5186412  0.51912904 0.51579535 ... 0.5214133  0.5315218  0.52012336]\n",
      "[0.50973016 0.51946336 0.5081233  ... 0.51567686 0.5269794  0.5179987 ]\n",
      "[0.5254525  0.5135905  0.51733434 ... 0.5125945  0.51346064 0.5260144 ]\n",
      "[0.51705444 0.5088912  0.52789026 ... 0.50991935 0.5191312  0.52204585]\n",
      "[0.5241752 0.5214965 0.5172255 ... 0.5171758 0.5136986 0.5138036]\n",
      "[0.5151616  0.51719934 0.5178789  ... 0.5237391  0.5180168  0.5261527 ]\n",
      "[0.51506436 0.5121331  0.5186745  ... 0.5201872  0.5118971  0.5129551 ]\n",
      "[0.50936836 0.5235592  0.51496667 ... 0.509065   0.5200426  0.53133965]\n",
      "[0.52504885 0.5169492  0.5137622  ... 0.51874584 0.52726555 0.5205954 ]\n",
      "[0.5199069 0.5161301 0.5113381 ... 0.5184974 0.5278439 0.5166438]\n",
      "[0.5182668  0.5100385  0.51887053 ... 0.51978624 0.5123412  0.51409096]\n",
      "[0.52685523 0.5119794  0.5077304  ... 0.5219559  0.52413803 0.5137731 ]\n",
      "[0.514414   0.52045006 0.52002853 ... 0.52288204 0.517848   0.51962304]\n",
      "[0.5146518  0.51432663 0.5353824  ... 0.525928   0.52212805 0.5168634 ]\n",
      "[0.5165486  0.5123398  0.5151593  ... 0.5191299  0.5102389  0.52321965]\n",
      "[0.5128886  0.5126816  0.51325274 ... 0.5116495  0.51836586 0.50974417]\n",
      "[0.5074932  0.51627004 0.5265993  ... 0.51429594 0.5152327  0.51618034]\n",
      "[0.5100176  0.5263544  0.5174969  ... 0.52157426 0.5265557  0.5103703 ]\n",
      "[0.51107985 0.5076484  0.51725465 ... 0.5214075  0.5143277  0.5220505 ]\n",
      "[0.5079974  0.5155096  0.5115547  ... 0.5312795  0.51562196 0.5125394 ]\n",
      "[0.5118747  0.51278937 0.50772285 ... 0.5218692  0.52333724 0.5161658 ]\n",
      "[0.51701504 0.51069593 0.5278656  ... 0.51339847 0.5186647  0.52104485]\n",
      "[0.5087323  0.5172879  0.53123754 ... 0.5173946  0.5097236  0.511187  ]\n",
      "[0.5236083  0.5136792  0.5246733  ... 0.51260585 0.5218042  0.51934665]\n",
      "[0.51823723 0.51407254 0.5093703  ... 0.51140606 0.50636905 0.5210701 ]\n",
      "[0.51666313 0.5159052  0.5160494  ... 0.51866597 0.5172563  0.51726127]\n",
      "[0.52518624 0.51273644 0.5233112  ... 0.52489907 0.5173413  0.52584505]\n",
      "[0.50997144 0.5140218  0.5072158  ... 0.51042277 0.5163626  0.5032032 ]\n",
      "[0.5090872  0.51809746 0.51000535 ... 0.5145713  0.52084255 0.51317495]\n",
      "[0.51412785 0.5143226  0.5107795  ... 0.51537335 0.5218863  0.5235897 ]\n",
      "[0.5126796  0.5187308  0.5182506  ... 0.5233341  0.5107029  0.52052194]\n",
      "[0.51562047 0.5192255  0.50602144 ... 0.5132067  0.5132865  0.5174845 ]\n",
      "[0.5146466  0.5110304  0.51160884 ... 0.5191025  0.5007597  0.5139943 ]\n",
      "[0.5148289  0.5180172  0.52482945 ... 0.5178693  0.5131929  0.51556194]\n",
      "[0.5162425  0.5178826  0.50519085 ... 0.5211054  0.52596986 0.51932746]\n",
      "[0.51475674 0.5152548  0.5123732  ... 0.5109844  0.509113   0.53135496]\n",
      "[0.52365273 0.5157309  0.52015144 ... 0.5217283  0.5167375  0.51595753]\n",
      "[0.5182041  0.512907   0.5121964  ... 0.5274688  0.5160466  0.51122236]\n",
      "[0.5188736  0.52579695 0.519563   ... 0.52284634 0.53495216 0.518482  ]\n",
      "[0.52803737 0.51979667 0.53131074 ... 0.52491623 0.51718557 0.5139894 ]\n",
      "[0.52077556 0.5223971  0.5112928  ... 0.51273334 0.5207765  0.5161884 ]\n",
      "[0.5179938  0.5099169  0.5153675  ... 0.51015335 0.5152189  0.5191989 ]\n",
      "[0.5298244  0.5154147  0.5158741  ... 0.51059437 0.51594824 0.51195073]\n",
      "[0.5205355  0.5213118  0.51189846 ... 0.5166241  0.515845   0.5156661 ]\n",
      "[0.5130652  0.5129347  0.52664286 ... 0.5129452  0.5140944  0.5173888 ]\n",
      "[0.51345295 0.5290144  0.5145111  ... 0.5091612  0.5168655  0.5015707 ]\n",
      "[0.5262096  0.51219857 0.51247746 ... 0.52739424 0.52197653 0.52519745]\n",
      "[0.50998026 0.5143846  0.5087917  ... 0.5197994  0.5089569  0.5094645 ]\n",
      "[0.5245621  0.515369   0.51995784 ... 0.5158175  0.5155709  0.50739664]\n",
      "[0.51596    0.5110296  0.5202231  ... 0.51080656 0.5135629  0.51601815]\n",
      "[0.51732135 0.5133388  0.5193535  ... 0.51412845 0.5115172  0.51366585]\n",
      "[0.5158607  0.5224266  0.51086104 ... 0.5165687  0.5043394  0.5222459 ]\n",
      "[0.5052747  0.5216134  0.5161647  ... 0.50818    0.5132909  0.50751555]\n",
      "[0.5201331  0.51848674 0.5143184  ... 0.5161348  0.5234135  0.51642364]\n",
      "[0.51356304 0.51440734 0.51014274 ... 0.50552493 0.5201961  0.5190467 ]\n",
      "[0.52396977 0.5183519  0.52221394 ... 0.5155562  0.5114087  0.52633625]\n",
      "[0.51764286 0.51613104 0.52239776 ... 0.5149034  0.5141711  0.52225256]\n",
      "[0.51743114 0.52196276 0.51082283 ... 0.5156204  0.52252907 0.5078358 ]\n",
      "[0.5231461  0.5102106  0.51333684 ... 0.5162965  0.51205283 0.5135077 ]\n",
      "[0.5286493  0.5245162  0.51108474 ... 0.51682526 0.52305734 0.519749  ]\n",
      "[0.52400136 0.5236187  0.5155738  ... 0.5151273  0.53070587 0.5186888 ]\n",
      "[0.5230069  0.52353925 0.52613634 ... 0.5237385  0.51762587 0.5125764 ]\n",
      "[0.5165481  0.5119312  0.51326317 ... 0.5126611  0.51116675 0.5145671 ]\n",
      "[0.5319825  0.5163565  0.5159686  ... 0.5288981  0.5185053  0.52427286]\n",
      "[0.5193398  0.5077597  0.51271623 ... 0.51494074 0.5136096  0.52544427]\n",
      "[0.5120713 0.5233659 0.5078994 ... 0.5240467 0.5122042 0.5220637]\n",
      "[0.51618004 0.5143985  0.51872396 ... 0.5128998  0.51948595 0.51735294]\n",
      "[0.5145324  0.5135534  0.51676995 ... 0.5184883  0.5219269  0.5079041 ]\n",
      "[0.5231299  0.514159   0.515163   ... 0.51836425 0.51374155 0.51741105]\n",
      "[0.51306444 0.5184855  0.5158805  ... 0.50875527 0.5151342  0.5192869 ]\n",
      "[0.51001894 0.51849174 0.5227877  ... 0.5118407  0.5161398  0.5180405 ]\n",
      "[0.52816683 0.5087888  0.5249525  ... 0.5147443  0.5103875  0.5212803 ]\n",
      "[0.50920904 0.5125038  0.5110147  ... 0.5212087  0.5083708  0.52615905]\n",
      "[0.5165995  0.51576376 0.5213575  ... 0.5059093  0.5102676  0.51455915]\n",
      "[0.51947945 0.5228279  0.5113229  ... 0.5208896  0.52485365 0.5235713 ]\n",
      "[0.5187602  0.51947045 0.5133654  ... 0.5196422  0.5219486  0.5095226 ]\n",
      "[0.5117753  0.51522005 0.50945157 ... 0.52881575 0.5108938  0.5241018 ]\n",
      "[0.50302756 0.5107379  0.5274862  ... 0.51935995 0.5162097  0.51378495]\n",
      "[0.5105833  0.5175981  0.5195844  ... 0.52316475 0.5278945  0.5125601 ]\n",
      "[0.5199866  0.5113542  0.5107058  ... 0.5139354  0.52038753 0.5033484 ]\n",
      "[0.51771325 0.5084933  0.5210324  ... 0.51046294 0.51732737 0.51216114]\n",
      "[0.52440244 0.523866   0.5151257  ... 0.51706034 0.51354635 0.51233697]\n",
      "[0.5098597  0.5222578  0.524471   ... 0.50191253 0.5186709  0.5124817 ]\n",
      "[0.5119053  0.5165443  0.51166123 ... 0.5097948  0.51570326 0.51273763]\n",
      "[0.513697   0.5081538  0.51833767 ... 0.5176627  0.51922375 0.5180695 ]\n",
      "[0.52424926 0.5117439  0.52639174 ... 0.51358956 0.51713824 0.5191704 ]\n",
      "[0.52262753 0.51450676 0.52823484 ... 0.5150361  0.5116963  0.52100694]\n",
      "[0.5123493  0.52259034 0.51436913 ... 0.5117127  0.5142814  0.5220642 ]\n",
      "[0.51134056 0.5145407  0.51579905 ... 0.5171264  0.52277106 0.52014065]\n",
      "[0.5342332  0.5210369  0.5108256  ... 0.51239115 0.5126913  0.5072473 ]\n",
      "[0.51640517 0.5250939  0.52221596 ... 0.52006817 0.5212421  0.51146984]\n",
      "[0.5254873  0.5150668  0.51715946 ... 0.5133221  0.5222146  0.52879006]\n",
      "[0.50988877 0.52725655 0.51246125 ... 0.5100862  0.51666856 0.5139889 ]\n",
      "[0.51410204 0.52512187 0.52510667 ... 0.52925617 0.5172596  0.5114973 ]\n",
      "[0.5130015  0.5292015  0.52334416 ... 0.5131643  0.5198209  0.5181112 ]\n",
      "[0.51891994 0.5155926  0.5124614  ... 0.53443366 0.5190648  0.51255846]\n",
      "[0.5204727  0.51558113 0.50869066 ... 0.5306478  0.52621347 0.5101888 ]\n",
      "[0.5219987  0.51210403 0.5142716  ... 0.51808786 0.5248716  0.51808196]\n",
      "[0.5183451  0.5157621  0.52316564 ... 0.52213347 0.5220442  0.5094666 ]\n",
      "[0.51539147 0.51853335 0.5181684  ... 0.5213927  0.5037052  0.51922023]\n",
      "[0.51350313 0.5202454  0.5153913  ... 0.5173893  0.5248189  0.5146217 ]\n",
      "[0.521448   0.51902276 0.51468855 ... 0.5205349  0.522054   0.5117429 ]\n",
      "[0.5300501  0.50838345 0.51182795 ... 0.5192007  0.51683307 0.5098726 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5366185  0.5243073  0.51350665 ... 0.51457065 0.51005125 0.525147  ]\n",
      "[0.50922984 0.52830744 0.52019876 ... 0.5060312  0.5167573  0.51022303]\n",
      "[0.50929064 0.5202549  0.50570405 ... 0.5159045  0.52591455 0.5171846 ]\n",
      "[0.51892626 0.5152769  0.51698714 ... 0.5209384  0.5124091  0.5186718 ]\n",
      "[0.510887   0.51399136 0.5203602  ... 0.5110727  0.51602906 0.51458067]\n",
      "[0.5124719  0.51224244 0.5133909  ... 0.5121318  0.5215159  0.52800137]\n",
      "[0.5102104  0.5199852  0.5231355  ... 0.5178927  0.5271122  0.52591664]\n",
      "[0.51063323 0.512758   0.50892866 ... 0.51240915 0.52094346 0.5174463 ]\n",
      "[0.50974905 0.5108271  0.517727   ... 0.51351327 0.51231575 0.5158364 ]\n",
      "[0.5155911  0.5161324  0.5121857  ... 0.51268893 0.5100771  0.5066504 ]\n",
      "[0.5159114  0.5103886  0.5272077  ... 0.5132807  0.5212684  0.50516206]\n",
      "[0.5181301 0.5128428 0.5185116 ... 0.5171459 0.5275129 0.5202013]\n",
      "[0.51388526 0.51954824 0.51504475 ... 0.5163311  0.5134277  0.5281081 ]\n",
      "[0.518155   0.5166326  0.5157494  ... 0.51897544 0.52175456 0.5176948 ]\n",
      "[0.51923865 0.5183125  0.51266646 ... 0.5162459  0.523397   0.5134672 ]\n",
      "[0.52193975 0.51318103 0.5149083  ... 0.5100299  0.511821   0.517642  ]\n",
      "[0.5087523 0.5198835 0.5326495 ... 0.5210281 0.5106385 0.5157069]\n",
      "[0.5071398  0.5131661  0.52168953 ... 0.51484704 0.5144314  0.5158431 ]\n",
      "[0.5182081  0.5285998  0.51900196 ... 0.5137507  0.51862925 0.5180891 ]\n",
      "[0.51393735 0.51317656 0.51159775 ... 0.5154059  0.5114304  0.5231089 ]\n",
      "[0.5180377  0.51094913 0.5341305  ... 0.5073235  0.5173259  0.51272684]\n",
      "[0.5222655  0.50168127 0.52635026 ... 0.51718706 0.51999813 0.51766557]\n",
      "[0.521176   0.5217683  0.5253108  ... 0.52583444 0.505462   0.52173597]\n",
      "[0.51974976 0.5102009  0.5124926  ... 0.52572954 0.5179222  0.5154505 ]\n",
      "[0.5216407  0.5179543  0.5322296  ... 0.52145994 0.51918966 0.52444774]\n",
      "[0.5137833  0.5230871  0.5151836  ... 0.51239073 0.51660275 0.5064627 ]\n",
      "[0.5191945  0.5162532  0.5057299  ... 0.50936407 0.5161379  0.513306  ]\n",
      "[0.51255995 0.5169802  0.51898724 ... 0.5122048  0.51620036 0.5119234 ]\n",
      "[0.51090854 0.51009625 0.51738405 ... 0.51296854 0.5210053  0.517807  ]\n",
      "[0.5167821 0.5107568 0.5186336 ... 0.5100435 0.5150641 0.521478 ]\n",
      "[0.5136595  0.52778715 0.5143094  ... 0.51231265 0.50830406 0.51293087]\n",
      "[0.51787543 0.52054936 0.50442106 ... 0.5169944  0.50485677 0.5095868 ]\n",
      "[0.51747644 0.50963277 0.51603276 ... 0.5137504  0.5185027  0.50809985]\n",
      "[0.51835334 0.5169222  0.521775   ... 0.5254697  0.514074   0.5215669 ]\n",
      "[0.5178716  0.52706707 0.54084074 ... 0.5197034  0.5208939  0.53774023]\n",
      "[0.5158093  0.51772565 0.5308973  ... 0.50494224 0.50433326 0.5120344 ]\n",
      "[0.5256883  0.5188811  0.5139359  ... 0.51303965 0.521638   0.520682  ]\n",
      "[0.5146296  0.5175596  0.51995194 ... 0.5255083  0.5122642  0.5371574 ]\n",
      "[0.5122514  0.50927705 0.51630396 ... 0.52336085 0.5215632  0.51501334]\n",
      "[0.51909024 0.5127439  0.5104084  ... 0.5219436  0.5153129  0.5183477 ]\n",
      "[0.5117536  0.5147992  0.51308924 ... 0.52840775 0.5168597  0.51255333]\n",
      "[0.5103901  0.5204489  0.5174168  ... 0.5237244  0.5116616  0.51547545]\n",
      "[0.5101898  0.5161533  0.5169098  ... 0.50744873 0.5171775  0.5116226 ]\n",
      "[0.5185047  0.5115517  0.5286501  ... 0.53080493 0.51758397 0.5155802 ]\n",
      "[0.5074246  0.5304681  0.521721   ... 0.5294101  0.51504606 0.5199955 ]\n",
      "[0.5133229  0.5267364  0.51729935 ... 0.52253467 0.520198   0.5195785 ]\n",
      "[0.52017516 0.5129285  0.5152913  ... 0.51218593 0.5231618  0.5151554 ]\n",
      "[0.51474696 0.522127   0.52551156 ... 0.51989096 0.5302246  0.51532614]\n",
      "[0.5173577  0.51842725 0.5280011  ... 0.5188906  0.5183232  0.5156983 ]\n",
      "[0.5144008  0.51018345 0.51436335 ... 0.540143   0.5195856  0.513011  ]\n",
      "[0.51528186 0.5193431  0.51924485 ... 0.51952714 0.518626   0.53706276]\n",
      "[0.51487315 0.5174128  0.51157427 ... 0.5184501  0.5071326  0.50263494]\n",
      "[0.5203274  0.5075531  0.5154216  ... 0.5161693  0.5161925  0.50525296]\n",
      "[0.5130063 0.5178787 0.5178147 ... 0.5148747 0.5176288 0.5176049]\n",
      "[0.5247789  0.5160799  0.5153426  ... 0.5188078  0.5178738  0.51572317]\n",
      "[0.5123777  0.5081917  0.5199022  ... 0.51545125 0.51489055 0.5140318 ]\n",
      "[0.5136977  0.51772606 0.5144471  ... 0.526779   0.5184432  0.52765894]\n",
      "[0.5116924  0.5143147  0.50899935 ... 0.5134117  0.51197475 0.5311724 ]\n",
      "[0.5152177  0.51633686 0.5177825  ... 0.51841116 0.5123015  0.5082935 ]\n",
      "[0.5162558  0.5201402  0.52118635 ... 0.52830005 0.5124935  0.5122598 ]\n",
      "[0.5099665  0.52024084 0.52022076 ... 0.52477616 0.5100816  0.51844174]\n",
      "[0.5141133  0.50842625 0.5193416  ... 0.51324093 0.51342136 0.5190219 ]\n",
      "[0.5132359  0.50877184 0.5178851  ... 0.5237377  0.5193758  0.52186227]\n",
      "[0.51505566 0.5126955  0.5122404  ... 0.5240778  0.5148073  0.50665706]\n",
      "[0.5143417  0.5193057  0.5210352  ... 0.5227719  0.5061834  0.51305866]\n",
      "[0.51666886 0.51286626 0.5233289  ... 0.5243687  0.5207317  0.51745665]\n",
      "[0.51090676 0.51796675 0.5070293  ... 0.51819944 0.5210768  0.51627153]\n",
      "[0.51329464 0.52425164 0.51326126 ... 0.5031627  0.531892   0.50943214]\n",
      "[0.5145663  0.5205068  0.5209007  ... 0.51404715 0.5096208  0.5175205 ]\n",
      "[0.5154373  0.5242872  0.52497554 ... 0.52774554 0.5153716  0.51535606]\n",
      "[0.5129156  0.5188367  0.51620096 ... 0.5208087  0.51592416 0.5200693 ]\n",
      "[0.5051852  0.5210505  0.5156929  ... 0.5110699  0.51975286 0.5133816 ]\n",
      "[0.5100542  0.51904523 0.51986253 ... 0.5172691  0.51317316 0.51603293]\n",
      "[0.5082013  0.51293695 0.51722383 ... 0.5182175  0.51786613 0.5116978 ]\n",
      "[0.5198758  0.51506245 0.52844584 ... 0.5166419  0.5185423  0.5120106 ]\n",
      "[0.50918466 0.51445585 0.5160373  ... 0.53099215 0.5297113  0.5233026 ]\n",
      "[0.51559925 0.51149875 0.5248049  ... 0.5316858  0.52131605 0.51199645]\n",
      "[0.51273316 0.51738    0.51389617 ... 0.5285251  0.5166239  0.52233875]\n",
      "[0.5228396  0.5185446  0.5105809  ... 0.5180488  0.5205261  0.51431865]\n",
      "[0.5205734  0.51685363 0.5114376  ... 0.5135088  0.5152071  0.51024836]\n",
      "[0.5194944  0.5096743  0.5141097  ... 0.5157221  0.5246402  0.51539767]\n",
      "[0.5209425  0.51752764 0.526186   ... 0.51308984 0.5142261  0.51529825]\n"
     ]
    }
   ],
   "source": [
    "# sess.run  or tensor.eval are two ways\n",
    "# get the accuracy in the testing data\n",
    "# need to cut down the size of testing data into batches\n",
    "\n",
    "print(\"feature len \", len(test_feature))\n",
    "print(\"label len \", len(test_y_m))\n",
    "test_batch_size = 1024\n",
    "print(\"%s iterations in total\" % (len(test_feature)//test_batch_size))\n",
    "for i in range(len(test_feature)//test_batch_size):\n",
    "    test_feature_batch = test_feature[ i*test_batch_size : min(i*test_batch_size +test_batch_size, len(test_feature))]\n",
    "    test_feature_batch_vector = getVectorFromModel(test_feature_batch, conceptLabelDict, vector_model)\n",
    "    test_y_m_batch = test_y_m[ i*test_batch_size : min(i*test_batch_size +test_batch_size,len(test_feature))]\n",
    "    y_prob = sess.run(probability, feed_dict={x:test_feature_batch_vector, keep_prob:1, is_training:False})\n",
    "    print(y_prob[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.run  or tensor.eval are two ways\n",
    "# get the accuracy in the testing data\n",
    "# need to cut down the size of testing data into batches\n",
    "\n",
    "print(\"feature len \", len(test_feature))\n",
    "print(\"label len \", len(test_y_m))\n",
    "test_batch_size = 1024\n",
    "print(\"%s iterations in total\" % (len(test_feature)//test_batch_size))\n",
    "for i in range(len(test_feature)//test_batch_size):\n",
    "    test_feature_batch = test_feature[ i*test_batch_size : min(i*test_batch_size +test_batch_size, len(test_feature))]\n",
    "    test_feature_batch_vector = getVectorFromModel(test_feature_batch, conceptLabelDict, vector_model)\n",
    "    test_y_m_batch = test_y_m[ i*test_batch_size : min(i*test_batch_size +test_batch_size,len(test_feature))]\n",
    "    print(\"%d iteration accurarcy: %s\" % (i, accuracy.eval(session=sess, feed_dict={x:test_feature_batch_vector, y_:test_y_m_batch,keep_prob: 1, is_training:False})))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    y_pred = sess.run(predict, feed_dict={x:test_feature_batch, keep_prob:1, is_training:False})\n",
    "    err_ids=np.flatnonzero(np.eye(2)[y_pred] != test_y_m_batch)\n",
    "    for err_id in err_ids:\n",
    "        print(\"index %d predicted label %s, but true label is %s\" % (err_id, y_pred[err_id], test_y_m_batch[err_id]))\n",
    "        idpair = ids_test[err_id] \n",
    "        concept1 = conceptLabelDict[idpair[0]]\n",
    "        concept2 = conceptLabelDict[idpair[1]]\n",
    "        print(\"%s Concept Pairs: (%s --- %s)\" % (idpair, concept1, concept2 ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = y_pred\n",
    "test_label_list = test_y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(accuracy_score(result, test_label_list))\n",
    "print(average_precision_score(result, test_label_list))\n",
    "\n",
    "print(f1_score(result, test_label_list, average='macro') ) \n",
    "\n",
    "print(f1_score(result, test_label_list, average='micro')  )\n",
    "\n",
    "print(f1_score(result, test_label_list, average='weighted') )\n",
    "\n",
    "print(f1_score(result, test_label_list, average=None))\n",
    "\n",
    "print(precision_score(result, test_label_list, average=None))\n",
    "print(recall_score(result, test_label_list, average=None))\n",
    "\n",
    "print(roc_auc_score(result, test_label_list, average=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
