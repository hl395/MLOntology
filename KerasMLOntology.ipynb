{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/anaconda3/envs/MLOntology/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variabls\n",
    "\n",
    "directory_path = \"/home/hao/AnacondaProjects/MLOntology/\"\n",
    "data_path = directory_path + \"data/\"\n",
    "vector_model_path = directory_path +\"vectorModel/\"\n",
    "cnn_model_path = directory_path +\"cnnModel/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptLabelDict={}\n",
    "errors=[]\n",
    "\n",
    "def read_label(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptLabelDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "label_file = data_path + \"ontClassLabels_july2017.txt\"\n",
    "read_label(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['37225000', '52860004', 1], ['159386001', '159385002', 1], ['233836002', '233835003', 1], ['233836002', '304914007', 1], ['224923003', '224717003', 1]]\n",
      "502459\n"
     ]
    }
   ],
   "source": [
    "conceptPairDict={}\n",
    "errors=[]\n",
    "conceptPairList=[]\n",
    "\n",
    "def read_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptPairList.append([splitted[1], splitted[2].replace(\"\\r\\n\", \"\"), 1])\n",
    "#                 conceptPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "pair_file = data_path + \"ontHierarchy_july2017.txt\"\n",
    "read_pair(pair_file)\n",
    "\n",
    "first2pairs = conceptPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['273187009', '272765000', 0], ['272877001', '272765000', 0], ['273216002', '272765000', 0], ['273125004', '272765000', 0], ['272973003', '272765000', 0]]\n",
      "6167243\n",
      "502459\n"
     ]
    }
   ],
   "source": [
    "conceptNotPairDict={}\n",
    "conceptNotPairList=[]\n",
    "\n",
    "def read_not_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                conceptNotPairList.append([splitted[0], splitted[1].replace(\"\\r\\n\", \"\"), 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "notPair_file = data_path + \"taxNotPairs_july2017.txt\"\n",
    "read_not_pair(notPair_file)\n",
    "\n",
    "# first2pairs = {k: conceptNotPairDict[k] for k in list(conceptNotPairDict)[10:15]}\n",
    "first2pairs =conceptNotPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptNotPairList))\n",
    "\n",
    "# In-place shuffle\n",
    "random.shuffle(conceptNotPairList)\n",
    "conceptNotPairList = conceptNotPairList[:len(conceptPairList)]\n",
    "\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('237267007', 0.6096370220184326),\n",
      " ('722913002', 0.5996204018592834),\n",
      " ('722912007', 0.5989267230033875),\n",
      " ('267262008', 0.5890039205551147),\n",
      " ('277485007', 0.5662010312080383),\n",
      " ('446466006', 0.558641791343689),\n",
      " ('177131001', 0.5541282892227173),\n",
      " ('177130000', 0.5492905974388123),\n",
      " ('199672002', 0.5478367805480957),\n",
      " ('10759611000119105', 0.5416226983070374)]\n"
     ]
    }
   ],
   "source": [
    "vector_model_file = vector_model_path + \"model0\"\n",
    "\n",
    "vector_model = gensim.models.Doc2Vec.load(vector_model_file)\n",
    "\n",
    "inferred_vector = vector_model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(vector_model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def readFromPairList(id_pair_list, id_notPair_list):\n",
    "    pair_list = id_pair_list + id_notPair_list\n",
    "    random.shuffle(pair_list)\n",
    "    idpairs_list =[]\n",
    "    label_list =[]\n",
    "    for i, line in enumerate(pair_list):      \n",
    "        idpairs_list.append([line[0], line[1]])\n",
    "        label_list.append(line[2])\n",
    "    return idpairs_list, label_list\n",
    "\n",
    "idpairs_list, label_list= readFromPairList(conceptPairList, conceptNotPairList)\n",
    "\n",
    "print(label_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['395150007', '418711005'], ['105241000', '7557006'], ['715705000', '8696009'], ['2649007', '61789006'], ['18307000', '272520006'], ['66742008', '438980004'], ['298350001', '24163000'], ['371430001', '417625002'], ['386033004', '276730002'], ['161043008', '88284004'], ['81876003', '67241009'], ['243872007', '170626004'], ['364567007', '364572003'], ['223697002', '223369002'], ['46743008', '74628008'], ['52807003', '107069004'], ['409109009', '353987005'], ['69527006', '104975003'], ['350345004', '347597002'], ['700774005', '465405006']]\n",
      "[['1090009', '106234000'], ['253917003', '93406002'], ['90317004', '314468002'], ['241774007', '291954007'], ['405789005', '363905002'], ['715141007', '64770007'], ['468289004', '463072004'], ['698101006', '701219001'], ['87118001', '706912001'], ['53973008', '67017004'], ['371572003', '702683000'], ['26154000', '443778003'], ['402102001', '468288007'], ['462733000', '466303007'], ['41167007', '106412004'], ['237000000', '177128002'], ['167947004', '428418001'], ['209195005', '312788000'], ['699646004', '117136008'], ['86003009', '371160000']]\n",
      "[1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
      "[1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(idpairs_list, label_list, test_size = 0.2, shuffle= True)\n",
    "print(X_train[:20])\n",
    "print(X_test[:20])\n",
    "print(y_train[:20])\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_classes = 2\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "\n",
    "\n",
    "# Y_train = np.eye(nb_classes)[y_train]\n",
    "# Y_test = np.eye(nb_classes)[y_test]\n",
    "\n",
    "# Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "# Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 1024\n",
    "\n",
    "def getVectorFromModel(concept_id, conceptLabelDict, model):\n",
    "    if concept_id in model.docvecs:\n",
    "        concept_vector= model.docvecs[concept_id]\n",
    "    else:\n",
    "        print(\"%s not found, get inferred vector \"%(concept_id))\n",
    "        concept_label = conceptLabelDict[concept_id]\n",
    "        concept_vector= model.infer_vector(concept_label.split())\n",
    "    return concept_vector\n",
    "\n",
    "def getVector(line, conceptLabelDict, model):        \n",
    "    a = getVectorFromModel(line[0], conceptLabelDict, model)\n",
    "    b = getVectorFromModel(line[1], conceptLabelDict, model)\n",
    "    c = np.array((a, b))\n",
    "    c = c.T \n",
    "    c = np.expand_dims(c, axis=2)\n",
    "    print(c.shape)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes=2 \n",
    "\n",
    "def generator(x_samples, y_samples, train_flag, batch_size=64):\n",
    "    samples = list(zip(x_samples, y_samples))\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            X_samples = []\n",
    "            Y_samples= []\n",
    "            for batch_sample in batch_samples:\n",
    "                pair_list = batch_sample[0]\n",
    "                data_vector = getVector(pair_list, conceptLabelDict, vector_model)\n",
    "                X_samples.append(data_vector)\n",
    "                class_label = batch_sample[1] \n",
    "                Y_samples.append(class_label)\n",
    "                \n",
    "            X_samples = np.array(X_samples).astype('float32')\n",
    "            Y_samples = np.eye(nb_classes)[Y_samples]\n",
    "            print('one batch ready')\n",
    "            yield shuffle(X_samples, Y_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_batch_size = 64\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(X_train, y_train, train_flag=True, batch_size=set_batch_size)\n",
    "validation_generator = generator(X_test, y_test, train_flag=False, batch_size=set_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = applications.resnet50.ResNet50(weights=None, include_top = True, classes=2)\n",
    "print('Model loaded')\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
    "csv_logger = CSVLogger('resnet18_cifar10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 2, 1)\n",
      "(512, 2, 1)Epoch 1/10\n",
      "\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "one batch ready\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "(512, 2, 1)\n",
      "one batch ready\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_10 to have shape (224, 224, 3) but got array with shape (512, 2, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-c863a12fabe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mset_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     callbacks=[lr_reducer, early_stopper, csv_logger])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1878\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_10 to have shape (224, 224, 3) but got array with shape (512, 2, 1)"
     ]
    }
   ],
   "source": [
    "myepochs = 10\n",
    "\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=len(X_train)//set_batch_size, \n",
    "                    epochs=myepochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=len(X_test)//set_batch_size,\n",
    "                    callbacks=[lr_reducer, early_stopper, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, Y_train,\n",
    "#               batch_size=batch_size,\n",
    "#               nb_epoch=nb_epoch,\n",
    "#               validation_data=(X_test, Y_test),\n",
    "#               shuffle=True,\n",
    "#               callbacks=[lr_reducer, early_stopper, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = '/home/hao/Pictures/elephant.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "(512, 2)\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "(4, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "a = np.zeros((512,2))\n",
    "print(a)\n",
    "print(a.shape)\n",
    "b = a.reshape(4,4,-1)\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
