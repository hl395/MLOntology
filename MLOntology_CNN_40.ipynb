{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['37225000', '52860004', 1], ['159386001', '159385002', 1], ['233836002', '233835003', 1], ['233836002', '304914007', 1], ['224923003', '224717003', 1]]\n",
      "502206\n"
     ]
    }
   ],
   "source": [
    "conceptPairDict={}\n",
    "errors=[]\n",
    "conceptPairList=[]\n",
    "\n",
    "def read_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptPairList.append([splitted[1], splitted[2].replace(\"\\r\\n\", \"\"), 1])\n",
    "#                 conceptPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "pair_file = \"/home/hao/AnacondaProjects/MLOntology/ontHierarchy.txt\"\n",
    "read_pair(pair_file)\n",
    "\n",
    "first2pairs = conceptPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['273187009', '272765000', 0], ['272877001', '272765000', 0], ['273216002', '272765000', 0], ['273125004', '272765000', 0], ['272973003', '272765000', 0]]\n",
      "6166563\n"
     ]
    }
   ],
   "source": [
    "conceptNotPairDict={}\n",
    "conceptNotPairList=[]\n",
    "\n",
    "def read_not_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                conceptNotPairList.append([splitted[0], splitted[1].replace(\"\\r\\n\", \"\"), 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "notPair_file = \"/home/hao/AnacondaProjects/MLOntology/taxNotPairs.txt\"\n",
    "read_not_pair(notPair_file)\n",
    "\n",
    "# first2pairs = {k: conceptNotPairDict[k] for k in list(conceptNotPairDict)[10:15]}\n",
    "first2pairs =conceptNotPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('237267007', 0.6277402639389038),\n",
      " ('722912007', 0.6036040782928467),\n",
      " ('446466006', 0.5923614501953125),\n",
      " ('722913002', 0.5906291007995605),\n",
      " ('67798003', 0.5715656280517578),\n",
      " ('253745002', 0.5681233406066895),\n",
      " ('10759661000119108', 0.5674417614936829),\n",
      " ('277485007', 0.5654561519622803),\n",
      " ('177130000', 0.5642964839935303),\n",
      " ('312974005', 0.5596712231636047)]\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/hao/AnacondaProjects/MLOntology/model0\"\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/MLOntology/model1\"\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[240050, 240072, 240069, 240023, 240032, 240044, 240056, 240035, 240051, 240097]\n",
      "[240160, 240198, 240158, 240189, 240135, 240180, 240113, 240162, 240115, 240182]\n",
      "[240275, 240292, 240214, 240255, 240291, 240216, 240253, 240230, 240202, 240218]\n",
      "[240394, 240377, 240351, 240389, 240386, 240326, 240347, 240387, 240374, 240357]\n",
      "[240482, 240421, 240495, 240431, 240466, 240493, 240402, 240432, 240434, 240498]\n",
      "[240589, 240505, 240570, 240597, 240532, 240526, 240582, 240533, 240551, 240519]\n",
      "[240634, 240609, 240663, 240635, 240680, 240620, 240640, 240691, 240661, 240695]\n",
      "[240792, 240721, 240709, 240704, 240739, 240742, 240748, 240705, 240701, 240768]\n",
      "[240821, 240844, 240807, 240804, 240836, 240835, 240851, 240873, 240809, 240899]\n",
      "[240996, 240903, 240982, 240963, 240929, 240904, 240930, 240917, 240923, 240902]\n",
      "[241065, 241036, 241041, 241067, 241014, 241088, 241048, 241015, 241055, 241042]\n",
      "[241177, 241133, 241156, 241162, 241189, 241183, 241114, 241140, 241141, 241194]\n",
      "[241224, 241242, 241222, 241284, 241274, 241207, 241293, 241208, 241214, 241220]\n",
      "[241326, 241385, 241310, 241340, 241305, 241349, 241331, 241320, 241336, 241311]\n",
      "[241438, 241463, 241494, 241479, 241468, 241401, 241457, 241418, 241429, 241435]\n",
      "[241589, 241539, 241510, 241587, 241538, 241531, 241526, 241599, 241518, 241592]\n",
      "[241630, 241644, 241655, 241668, 241603, 241609, 241650, 241677, 241622, 241605]\n",
      "[241720, 241725, 241784, 241744, 241745, 241753, 241704, 241741, 241706, 241780]\n",
      "[241851, 241837, 241819, 241885, 241861, 241830, 241850, 241878, 241848, 241835]\n",
      "[241971, 241923, 241922, 241934, 241981, 241940, 241946, 241949, 241952, 241935]\n",
      "[242010, 242015, 242063, 242087, 242082, 242089, 242085, 242056, 242086, 242004]\n",
      "[242139, 242114, 242146, 242196, 242185, 242164, 242125, 242166, 242104, 242158]\n",
      "[242204, 242237, 242269, 242274, 242240, 242229, 242220, 242268, 242253, 242290]\n",
      "[242384, 242367, 242398, 242383, 242325, 242363, 242379, 242348, 242312, 242344]\n",
      "[242457, 242459, 242412, 242488, 242435, 242485, 242496, 242458, 242489, 242467]\n",
      "[242500, 242537, 242597, 242547, 242552, 242570, 242557, 242523, 242583, 242572]\n",
      "[242660, 242606, 242635, 242642, 242655, 242647, 242623, 242610, 242624, 242685]\n",
      "[242710, 242739, 242717, 242735, 242771, 242786, 242754, 242787, 242740, 242750]\n",
      "[242889, 242874, 242849, 242811, 242861, 242807, 242875, 242825, 242800, 242876]\n",
      "[242990, 242905, 242961, 242969, 242938, 242941, 242927, 242922, 242917, 242923]\n",
      "[243098, 243040, 243038, 243000, 243092, 243084, 243023, 243054, 243050, 243081]\n",
      "[243164, 243117, 243144, 243135, 243109, 243190, 243138, 243136, 243151, 243163]\n",
      "[243251, 243283, 243202, 243223, 243282, 243228, 243208, 243227, 243224, 243218]\n",
      "[243329, 243370, 243392, 243367, 243321, 243376, 243348, 243358, 243374, 243306]\n",
      "[243449, 243433, 243450, 243427, 243446, 243476, 243478, 243409, 243451, 243413]\n",
      "[243556, 243521, 243546, 243562, 243569, 243513, 243525, 243588, 243533, 243557]\n",
      "[243611, 243644, 243616, 243603, 243665, 243649, 243609, 243632, 243617, 243615]\n",
      "[243796, 243712, 243747, 243765, 243729, 243788, 243786, 243717, 243746, 243760]\n",
      "[243835, 243896, 243849, 243880, 243846, 243869, 243892, 243852, 243877, 243815]\n",
      "[243951, 243944, 243939, 243903, 243900, 243911, 243966, 243963, 243995, 243956]\n",
      "[244068, 244064, 244058, 244049, 244052, 244030, 244079, 244028, 244073, 244035]\n",
      "[244125, 244133, 244169, 244123, 244160, 244197, 244190, 244174, 244176, 244124]\n",
      "[244237, 244239, 244259, 244274, 244262, 244258, 244298, 244219, 244235, 244280]\n",
      "[244315, 244304, 244376, 244335, 244345, 244316, 244344, 244398, 244367, 244302]\n",
      "[244427, 244471, 244430, 244412, 244459, 244448, 244434, 244494, 244472, 244436]\n",
      "[244596, 244570, 244551, 244520, 244583, 244534, 244552, 244548, 244558, 244579]\n",
      "[244613, 244698, 244608, 244600, 244654, 244648, 244621, 244674, 244610, 244637]\n",
      "[244708, 244753, 244796, 244785, 244787, 244776, 244777, 244790, 244768, 244733]\n",
      "[244810, 244817, 244842, 244826, 244815, 244850, 244888, 244882, 244816, 244885]\n",
      "[244932, 244963, 244970, 244906, 244987, 244949, 244920, 244930, 244938, 244940]\n",
      "[245099, 245075, 245037, 245098, 245044, 245086, 245016, 245089, 245056, 245061]\n",
      "[245125, 245160, 245154, 245112, 245185, 245140, 245147, 245134, 245193, 245127]\n",
      "[245227, 245285, 245284, 245275, 245250, 245224, 245258, 245217, 245249, 245231]\n",
      "[245331, 245378, 245319, 245314, 245341, 245359, 245349, 245363, 245318, 245334]\n",
      "[245471, 245488, 245454, 245450, 245420, 245499, 245425, 245442, 245406, 245441]\n",
      "[245571, 245549, 245572, 245585, 245500, 245544, 245594, 245596, 245574, 245543]\n",
      "[245606, 245625, 245691, 245647, 245672, 245615, 245626, 245653, 245618, 245663]\n",
      "[245764, 245761, 245748, 245786, 245714, 245757, 245737, 245754, 245768, 245766]\n",
      "[245806, 245854, 245835, 245867, 245837, 245839, 245893, 245830, 245887, 245836]\n",
      "[245978, 245985, 245905, 245917, 245921, 245900, 245983, 245911, 245951, 245958]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "feature_number = 1024\n",
    "\n",
    "train_list_before=[]\n",
    "train_label_list=[]\n",
    "test_list_before=[]\n",
    "test_label_list =[]\n",
    "\n",
    "offset = 6000*40\n",
    "\n",
    "for i in range(60):\n",
    "    index = np.arange(100)     #generate numbers from 0 to 100\n",
    "    np.random.shuffle(index)        #shuffle the 100 values\n",
    "    index = [int(100*i+j + offset) for j in index]\n",
    "    print(index[10:20])\n",
    "    train_list_before.extend([conceptPairList[b] for b in index[0:40]]) \n",
    "    train_label_list.extend([1]*40)\n",
    "    train_list_before.extend([conceptNotPairList[b] for b in index[40:80]])\n",
    "    train_label_list.extend([0]*40)\n",
    "    test_list_before.extend([conceptPairList[b] for b in index[80:90]])\n",
    "    test_label_list.extend([1]*10)\n",
    "    test_list_before.extend([conceptNotPairList[b] for b in index[90:100]])\n",
    "    test_label_list.extend([0]*10)\n",
    "\n",
    "\n",
    "    \n",
    "train_list =[]\n",
    "test_list = []\n",
    "\n",
    "for line in train_list_before:\n",
    "    if line[0] in model.docvecs and line[1] in model.docvecs:\n",
    "        a= model.docvecs[line[0]]\n",
    "        b= model.docvecs[line[1]]\n",
    "        c = np.array((a, b))\n",
    "#         train_list.append(np.reshape(c, feature_number)) \n",
    "        train_list.append(np.reshape(c, feature_number, order='F'))\n",
    "\n",
    "\n",
    "test_list_ids={}\n",
    "for i, line in enumerate(test_list_before):\n",
    "    if line[0] in model.docvecs and line[1] in model.docvecs:\n",
    "        a= model.docvecs[line[0]]\n",
    "        b= model.docvecs[line[1]]\n",
    "        c = np.array((a, b))\n",
    "        test_list_ids[i] = (line[0], line[1])\n",
    "#         test_list.append(np.reshape(c, feature_number))\n",
    "        test_list.append(np.reshape(c, feature_number, order='F'))\n",
    "\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.01630028,  0.10945631,  0.05380569, ...,  0.09613861,\n",
      "       -0.15353568, -0.24838926], dtype=float32), array([-0.01630028,  0.24935724,  0.05380569, ...,  0.32835263,\n",
      "       -0.15353568, -0.12039734], dtype=float32), array([ 0.01616716,  0.17246012, -0.01593279, ...,  0.31216961,\n",
      "       -0.02464111, -0.11221938], dtype=float32), array([ 0.19516075,  0.09534057, -0.06179632, ...,  0.06781578,\n",
      "       -0.13354021, -0.12042288], dtype=float32), array([ 0.24523383,  0.31121209, -0.06924546, ...,  0.23237613,\n",
      "       -0.03084917, -0.13484414], dtype=float32), array([ 0.06852512,  0.10180858,  0.06192448, ...,  0.92308307,\n",
      "        0.07017414, -0.03109001], dtype=float32), array([ 0.04436052,  0.13514425,  0.05496745, ...,  0.01479864,\n",
      "       -0.02943745,  0.01080734], dtype=float32), array([-0.01764492,  0.04205993, -0.02241248, ..., -0.13453349,\n",
      "        0.05162781, -0.06823476], dtype=float32), array([-0.01764492,  0.23519309, -0.02241248, ...,  0.32748488,\n",
      "        0.05162781, -0.24008305], dtype=float32), array([-0.01764492, -0.01903786, -0.02241248, ..., -0.01534603,\n",
      "        0.05162781, -0.31019911], dtype=float32), array([ 0.23379375,  0.18457079, -0.22017929, ...,  0.26817071,\n",
      "        0.08563729, -0.24617553], dtype=float32), array([ 0.07253507,  0.18457079, -0.32539439, ...,  0.26817071,\n",
      "        0.08723184, -0.24617553], dtype=float32), array([ 0.21973093,  0.18457079, -0.45162493, ...,  0.26817071,\n",
      "        0.23527452, -0.24617553], dtype=float32), array([ 0.07214104,  0.18457079, -0.42860419, ...,  0.26817071,\n",
      "       -0.02901208, -0.24617553], dtype=float32), array([ 0.0236714 ,  0.18457079, -0.53040493, ...,  0.26817071,\n",
      "        0.07937631, -0.24617553], dtype=float32), array([ 0.10953753,  0.18457079, -0.33292603, ...,  0.26817071,\n",
      "        0.01275476, -0.24617553], dtype=float32), array([-0.02443689,  0.13170105, -0.19159789, ..., -0.13342904,\n",
      "       -0.06543454,  0.06701519], dtype=float32), array([ 0.02235555,  0.13170105,  0.01674586, ..., -0.13342904,\n",
      "       -0.03550367,  0.06701519], dtype=float32), array([ 0.05723412,  0.18457079, -0.42279252, ...,  0.26817071,\n",
      "        0.0983312 , -0.24617553], dtype=float32), array([-0.03456545,  0.13170105, -0.03294364, ..., -0.13342904,\n",
      "       -0.02821245,  0.06701519], dtype=float32)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4800\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "print(train_list[30:50])\n",
    "print(train_label_list[30:50])\n",
    "print(len(train_list))\n",
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "[0.96395833333333336]\n"
     ]
    }
   ],
   "source": [
    "#SVM \n",
    "\n",
    "\n",
    "print(len(train_list))\n",
    "\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(train_list, train_label_list)\n",
    "\n",
    "train_errors=[]\n",
    "train_errors.append(clf.score(train_list, train_label_list))\n",
    "print(train_errors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptLabelDict={}\n",
    "errors=[]\n",
    "\n",
    "def read_label(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptLabelDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "label_file = \"/home/hao/AnacondaProjects/MLOntology/ontClassLabels.txt\"\n",
    "read_label(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 1 predicted label [0], but true label is 1\n",
      "('86234004', '64715009') Concept Pairs: (hypertensive heart and renal disease --- hypertensive heart disease)\n",
      "index 43 predicted label [0], but true label is 1\n",
      "('410243008', '392134007') Concept Pairs: (ear wax removal management --- care regimes management)\n",
      "index 81 predicted label [0], but true label is 1\n",
      "('370921009', '370917002') Concept Pairs: (patient death associated with a fall while being cared for in a healthcare facility --- serious reportable event associated with environment)\n",
      "index 93 predicted label [1], but true label is 0\n",
      "('13180005', '36999000') Concept Pairs: (retrograde menstruation --- helcomenia)\n",
      "index 95 predicted label [1], but true label is 0\n",
      "('103018003', '103304001') Concept Pairs: (exertional dizziness --- vertigo associated with recent change in eyeglasses)\n",
      "index 97 predicted label [1], but true label is 0\n",
      "('315018008', '103304001') Concept Pairs: (dizzy spells --- vertigo associated with recent change in eyeglasses)\n",
      "index 187 predicted label [0], but true label is 1\n",
      "('306432001', '306431008') Concept Pairs: (discharge by clinical geneticist --- discharge by geneticist)\n",
      "index 212 predicted label [1], but true label is 0\n",
      "('105512005', '105516008') Concept Pairs: (discord with landlord --- discord with teachers)\n",
      "index 235 predicted label [1], but true label is 0\n",
      "('95899007', '95914002') Concept Pairs: (drug action increased --- reversal of opiate activity)\n",
      "index 250 predicted label [1], but true label is 0\n",
      "('21243004', '4886009') Concept Pairs: (term birth of newborn --- premature birth of newborn male)\n",
      "index 269 predicted label [0], but true label is 1\n",
      "('124509009', '129456006') Concept Pairs: (deficiency of amidase --- specific enzyme deficiency)\n",
      "index 278 predicted label [1], but true label is 0\n",
      "('103411003', '72631000119101') Concept Pairs: (human immunodeficiency virus centers for disease control and prevention category b3 (acquired immunodeficiency syndrome) --- human immunodeficiency virus (hiv) ii infection category b2)\n",
      "index 382 predicted label [0], but true label is 1\n",
      "('722204007', '394733009') Concept Pairs: (legal medicine --- medical specialty)\n",
      "index 395 predicted label [1], but true label is 0\n",
      "('79524000', '58855008') Concept Pairs: (ecouteurism --- symptomatic exhibitionism)\n",
      "index 399 predicted label [1], but true label is 0\n",
      "('724745001', '58855008') Concept Pairs: (paraphilia involving solitary behavior or consenting individuals --- symptomatic exhibitionism)\n",
      "index 432 predicted label [1], but true label is 0\n",
      "('413330005', '44552000') Concept Pairs: (victim of oppression in country of origin --- victim of aerial warfare)\n",
      "index 449 predicted label [0], but true label is 1\n",
      "('410241005', '386053000') Concept Pairs: (ear wax removal assessment --- evaluation procedure)\n",
      "index 450 predicted label [1], but true label is 0\n",
      "('95839005', '359717002') Concept Pairs: (disorder involving the fibrinolytic system --- hereditary von willebrand disease type 2b)\n",
      "index 452 predicted label [1], but true label is 0\n",
      "('286923006', '286924000') Concept Pairs: (iron, copper, magnesium metabolism disorder --- phosphorus and calcium disorders)\n",
      "index 457 predicted label [1], but true label is 0\n",
      "('105604006', '359717002') Concept Pairs: (deficiency of naturally occurring coagulation factor inhibitor --- hereditary von willebrand disease type 2b)\n",
      "index 474 predicted label [1], but true label is 0\n",
      "('118234003', '398056004') Concept Pairs: (finding by site --- transient abnormality with full recovery)\n",
      "index 475 predicted label [1], but true label is 0\n",
      "('426655001', '5181007') Concept Pairs: (disorder of aromatic amino acid metabolism --- disorder of tryptophan metabolism)\n",
      "index 651 predicted label [1], but true label is 0\n",
      "('88939009', '79842004') Concept Pairs: (severe mood disorder without psychotic features --- stuporous depression)\n",
      "index 658 predicted label [1], but true label is 0\n",
      "('715924009', '79842004') Concept Pairs: (disruptive mood dysregulation disorder --- stuporous depression)\n",
      "index 670 predicted label [1], but true label is 0\n",
      "('187921002', '229721007') Concept Pairs: (developmental receptive language disorder --- speech delay)\n",
      "index 677 predicted label [1], but true label is 0\n",
      "('25766007', '229721007') Concept Pairs: (mixed receptive-expressive language disorder --- speech delay)\n",
      "index 763 predicted label [0], but true label is 1\n",
      "('305919008', '305918000') Concept Pairs: (referral by bereavement counselor --- referral by counselor)\n",
      "index 768 predicted label [0], but true label is 1\n",
      "('699650006', '310081005') Concept Pairs: (community based physiotherapy service --- professional allied to medicine service)\n",
      "index 830 predicted label [1], but true label is 0\n",
      "('82423001', '425908001') Concept Pairs: (chronic pain --- pain radiating to thoracic region right side)\n",
      "index 831 predicted label [1], but true label is 0\n",
      "('112104007', '425908001') Concept Pairs: (localized pain --- pain radiating to thoracic region right side)\n",
      "index 833 predicted label [1], but true label is 0\n",
      "('52598005', '425908001') Concept Pairs: (rest pain --- pain radiating to thoracic region right side)\n",
      "index 839 predicted label [1], but true label is 0\n",
      "('36163009', '425908001') Concept Pairs: (night pain --- pain radiating to thoracic region right side)\n",
      "index 882 predicted label [0], but true label is 1\n",
      "('53210006', '237981000') Concept Pairs: (inborn error of pyruvate metabolism --- disorder of pyruvate metabolism and mitochondrial respiratory chain)\n",
      "index 974 predicted label [1], but true label is 0\n",
      "('367368009', '360373000') Concept Pairs: (sulfite oxidase deficiency --- homocystinuria vitamin b12-responsive type iii)\n",
      "index 1015 predicted label [1], but true label is 0\n",
      "('84002002', '248103001') Concept Pairs: (pedophilia --- flashing)\n",
      "index 1030 predicted label [1], but true label is 0\n",
      "('1085191000119109', '10749641000119106') Concept Pairs: (complication due to chronic ulcerative proctitis --- blood coagulation disorder complicating pregnancy)\n",
      "index 1032 predicted label [1], but true label is 0\n",
      "('811004', '394697003') Concept Pairs: (flail motion --- extrapyramidal movements)\n",
      "index 1039 predicted label [1], but true label is 0\n",
      "('364807002', '394697003') Concept Pairs: (finding of control of movement --- extrapyramidal movements)\n",
      "index 1108 predicted label [0], but true label is 1\n",
      "('444332001', '250171008') Concept Pairs: (aware of prognosis --- clinical history and observation findings)\n",
      "index 1177 predicted label [1], but true label is 0\n",
      "('229751001', '229639009') Concept Pairs: (semantic-pragmatic disorder --- hypofunctional aphonia)\n",
      "index 1178 predicted label [1], but true label is 0\n",
      "('241831000', '241828001') Concept Pairs: (poisoning from sting of jelly fishes --- poisoning by cone shell venom)\n"
     ]
    }
   ],
   "source": [
    "for i, (item, label) in enumerate(zip(test_list, test_label_list)):\n",
    "    result = clf.predict([item])\n",
    "    if result != label:\n",
    "        print(\"index %d predicted label %s, but true label is %s\" % (i, result, label))\n",
    "        idpair = test_list_ids[i] \n",
    "        concept1 = conceptLabelDict[idpair[0]]\n",
    "        concept2 = conceptLabelDict[idpair[1]]\n",
    "        print(\"%s Concept Pairs: (%s --- %s)\" % (idpair, concept1, concept2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "1200\n",
      "[1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# m3 = np.array((a,d))\n",
    "# m3 = np.reshape(m3, 400, order='F')\n",
    "print(len(test_list))\n",
    "result = clf.predict(test_list)\n",
    "\n",
    "print(result.size)\n",
    "\n",
    "\n",
    "print(result[:29])\n",
    "print(np.array(test_label_list[:29]))\n",
    "\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   43   81   93   95   97  187  212  235  250  269  278  382  395  399\n",
      "  432  449  450  452  457  474  475  651  658  670  677  763  768  830  831\n",
      "  833  839  882  974 1015 1030 1032 1039 1108 1177 1178]\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "\n",
    "err_ids=np.flatnonzero(result != test_label_list)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965833333333\n",
      "0.959089929995\n",
      "0.965824765792\n",
      "0.965833333333\n",
      "0.965841900875\n",
      "[ 0.96528366  0.96636587]\n",
      "[ 0.95        0.98166667]\n",
      "[ 0.98106713  0.95153473]\n",
      "0.966300929543\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(accuracy_score(result, test_label_list))\n",
    "print(average_precision_score(result, test_label_list))\n",
    "\n",
    "print(f1_score(result, test_label_list, average='macro') ) \n",
    "\n",
    "print(f1_score(result, test_label_list, average='micro')  )\n",
    "\n",
    "print(f1_score(result, test_label_list, average='weighted') )\n",
    "\n",
    "print(f1_score(result, test_label_list, average=None))\n",
    "\n",
    "print(precision_score(result, test_label_list, average=None))\n",
    "print(recall_score(result, test_label_list, average=None))\n",
    "\n",
    "print(roc_auc_score(result, test_label_list, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label_list_m = np.eye(2)[train_label_list]\n",
    "# test_label_list_m = np.eye(2)[test_label_list]\n",
    "# print(test_label_list_m[10:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 100\n",
      "Train Loss: 0.680596 Acc: [0.51999998]\n",
      "None 200\n",
      "Train Loss: 0.654098 Acc: [0.55000001]\n",
      "None 300\n",
      "Train Loss: 0.597768 Acc: [0.54000002]\n",
      "None 400\n",
      "Train Loss: 0.606598 Acc: [0.43000001]\n",
      "None 500\n",
      "Train Loss: 0.524338 Acc: [0.63]\n",
      "None 600\n",
      "Train Loss: 0.472338 Acc: [0.72000003]\n",
      "None 700\n",
      "Train Loss: 0.404206 Acc: [0.85000002]\n",
      "None 800\n",
      "Train Loss: 0.552528 Acc: [0.81999999]\n",
      "None 900\n",
      "Train Loss: 0.449208 Acc: [0.87]\n",
      "None 1000\n",
      "Train Loss: 0.204811 Acc: [0.93000001]\n",
      "None 1100\n",
      "Train Loss: 0.145706 Acc: [0.94]\n",
      "None 1200\n",
      "Train Loss: 0.155893 Acc: [0.92000002]\n",
      "None 1300\n",
      "Train Loss: 0.0727803 Acc: [0.99000001]\n",
      "None 1400\n",
      "Train Loss: 0.0628381 Acc: [0.99000001]\n",
      "0.965\n",
      "Model saved in file: ./model-noleaky.ckpt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "In the data, there are 2 classes and every class has 3000 samples and every sample has 512 features\n",
    "The first 3000 samples are from class 0, second 3000 are from class 1\n",
    "'''\n",
    "# DATA_DIR = ''\n",
    "CLASS_NUM = 2       #there are 2 classes\n",
    "SPLIT_PERCENT = 0.8     #split the data into 80% for training and 20% for testing\n",
    "FEATURE_NUM = 1024   \n",
    "TRAIN_ITER = 1500    #the number of iterations for training\n",
    "display_step = 100        #how many iterations to display the results\n",
    "\n",
    "\n",
    "train_num = int(3000*SPLIT_PERCENT)     #the number of samples for training\n",
    "\n",
    "train_feature = train_list      #training features (list of list)\n",
    "train_y = train_label_list        #training lables    (list)\n",
    "test_feature = test_list       #test features  (list of list)\n",
    "test_y = test_label_list         #test labels    (list)\n",
    "\n",
    "\n",
    "y_m = np.eye(2)[train_y]\n",
    "test_y_m = np.eye(2)[test_y]\n",
    "\n",
    "'''\n",
    "y = wx+b        (vectors)\n",
    "'''\n",
    "#function to get variables 'w'\n",
    "def weight_variable(shape, num):\n",
    "    initial = tf.truncated_normal(shape, stddev=1/num)\n",
    "    return tf.Variable(initial, name='weight')\n",
    "\n",
    "#the bias 'b' in the equations\n",
    "def bias_variable(shape, num):\n",
    "    initial = tf.constant(0.0001, shape=shape)\n",
    "    return tf.Variable(initial, name='bias')\n",
    "\n",
    "#convolutional process\n",
    "def conv1d(x, W):\n",
    "    return tf.nn.conv1d(x, W, stride=1, padding='SAME')     #x: variable, w: weight, stride and padding (padding can be ignored currently) \n",
    "\n",
    "#pooling process\n",
    "def max_pool_1x1(x, shape):\n",
    "    x=tf.reshape(x,shape)       #it is transfered into four dimensions, but the other three are 1\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 4, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "'''\n",
    "The feature is 3 dimensional data.  [batch, length, channel] \n",
    "batch is usually ignored (for example there are 100 samples in a batch, so samples should not be modified mutually), length and channel are shown in the paper.\n",
    "At first, the length is 512, and channel is 1.\n",
    "Because our data are time series data, so length is enough, but for images, it may be [batch, length, width, channel]\n",
    "'''\n",
    "# the convolutional layer\n",
    "def layer(features, f, input_n, channel, hidden_units, layer_index):\n",
    "    \"\"\"Construct a convolutional layer\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    f: the length\n",
    "    input_n: Size of the features used in the convention.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    layer_index: the index of layer\n",
    "    Returns:\n",
    "    hidden units: The unit output for the next layer.\n",
    "    weights: the weights in the current hidden layer\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden'+str(layer_index)) as scope:     # name scope may be ignored first\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, channel, hidden_units], math.sqrt(f))\n",
    "\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(f))\n",
    "    hidden = relu(conv1d(features, weights) + biases, 0.01)\n",
    "    shape = [-1,1,f,hidden_units]\n",
    "    h_pool1 = max_pool_1x1(hidden,shape)\n",
    "    return h_pool1, weights\n",
    "\n",
    "# fully connected layer, here the data are two dimension, [batch, length]\n",
    "def densely_connect(features, input_n, hidden_units):\n",
    "    \"\"\"Construct a fully (densely) connected layer.\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    input_n: Size of units in the previous layer.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    Returns:\n",
    "    logits: The estimated output in last layer.\n",
    "    weights: the weights in the hidden layer\n",
    "    \"\"\"\n",
    "    with tf.name_scope('softmax_linear') as dense:\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    logits = relu(tf.matmul(features, weights) + biases, 0.01)      # the matrix product operation\n",
    "    return logits, weights\n",
    "\n",
    "# dropout layer (it is not necessary)\n",
    "# randomly set (1-keep_prob) percentage of units to be zero\n",
    "def dropout(features, input_n, hidden_units, keep_prob):\n",
    "    with tf.name_scope('dropout'):\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    h_fc1_drop = tf.nn.dropout(features, keep_prob)\n",
    "    drop_out = relu(tf.matmul(features, weights) + biases, 0.01)\n",
    "    return drop_out\n",
    "\n",
    "# calculate the loss in the neural network\n",
    "def loss(logits, labels):\n",
    "    \"\"\"Calculates the loss from the logits and the labels.\n",
    "    Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size, NUM_CLASSES].\n",
    "    Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        labels = tf.to_int64(labels)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=labels, logits=logits, name='xentropy')\n",
    "    # tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "def next_batch(data, label, num):\n",
    "    \"\"\"Generate the next batch randomly\n",
    "    Args:\n",
    "    data: training data.\n",
    "    label: training label.\n",
    "    num: the size in a batch\n",
    "    Returns:\n",
    "    next batch's training features and labels.\n",
    "    \"\"\"\n",
    "    index = np.arange(len(data))\n",
    "    np.random.shuffle(index)\n",
    "#     train_feature = data[np.array(index)[0:num]]\n",
    "#     train_label = label[np.array(index)[0:num]]\n",
    "    train_feature_batch = [data[b] for b in index[0:num]]\n",
    "    train_feature_batch = np.asarray(train_feature_batch)\n",
    "    train_label_batch = [label[b] for b in index[0:num]]\n",
    "    train_label_batch = np.asarray(train_label_batch)\n",
    "    return train_feature_batch, train_label_batch\n",
    "\n",
    "def relu(x, alpha=0., max_value=None):\n",
    "    '''ReLU.\n",
    "    alpha: slope of negative section.\n",
    "    '''\n",
    "    negative_part = tf.nn.relu(-x)\n",
    "    x = tf.nn.relu(x)\n",
    "    if max_value is not None:\n",
    "        x = tf.clip_by_value(x, tf.cast(0., dtype=tf.float32),\n",
    "                             tf.cast(max_value, dtype=tf.float32))\n",
    "    x -= tf.constant(alpha, dtype=tf.float32) * negative_part\n",
    "    return x\n",
    "\n",
    "#define a session to run the model\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#place holders for training features and label\n",
    "#None means the value is variable\n",
    "x = tf.placeholder(tf.float32, shape=[None, FEATURE_NUM])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, CLASS_NUM])\n",
    "\n",
    "# decide whether it is training or testing, it is not used in our model, but it may be used\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "#from [-1, 512, 1] -> [-1, 256, 32] -> [-1, 128, 64] -> [-1, 64, 64] -> [-1, 32, 64] -> [-1, 16, 64] -> [-1, 8, 64] -> [-1, 200]\n",
    "\n",
    "#6 hidden layers\n",
    "x_1 = tf.reshape(x, [-1,FEATURE_NUM,1])\n",
    "h_pool0, w0 = layer(x_1, FEATURE_NUM, 15, 1, 32, 0)\n",
    "h_pool0 = tf.reshape(h_pool0, [-1,512,32])\n",
    "h_pool1, w1 = layer(h_pool0, 512, 10, 32, 64, 1)\n",
    "\n",
    "h_pool1 = tf.reshape(h_pool1, [-1,256,64])\n",
    "h_pool2, w2 = layer(h_pool1, 256, 10, 64, 64, 2)\n",
    "h_pool2 = tf.reshape(h_pool2, [-1,128,64])\n",
    "h_pool3, w3 = layer(h_pool2, 128, 10, 64, 64, 3)\n",
    "h_pool3 = tf.reshape(h_pool3, [-1,64,64])\n",
    "h_pool4, w4 = layer(h_pool3, 64, 5, 64, 64, 4)\n",
    "h_pool4 = tf.reshape(h_pool4, [-1,32,64])\n",
    "h_pool5, w5 = layer(h_pool4, 32, 5, 64, 64, 5)\n",
    "h_pool5 = tf.reshape(h_pool5, [-1,16,64])\n",
    "h_pool6, w6 = layer(h_pool5, 16, 5, 64, 64, 6)\n",
    "h_pool6 = tf.reshape(h_pool6, [-1,8,64])\n",
    "\n",
    "#densely connected: 200 units\n",
    "h_pool_flat = tf.reshape(h_pool6, [-1, 8*64])\n",
    "h_dc, w_d = densely_connect(h_pool_flat, 8*64, 200)\n",
    "\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y_conv=dropout(h_dc, (int)(h_dc.get_shape()[1]), CLASS_NUM, keep_prob)\n",
    "\n",
    "\n",
    "beta = 0.001\n",
    "cross_entropy = loss(y_conv, y_)\n",
    "loss = cross_entropy +beta*(tf.nn.l2_loss(w0) + tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2)+tf.nn.l2_loss(w3)+tf.nn.l2_loss(w4)+tf.nn.l2_loss(w5)+tf.nn.l2_loss(w6)+tf.nn.l2_loss(w_d))  #L2 regularization\n",
    "epsilon = 1e-5      # learning rate\n",
    "train_step = tf.train.AdamOptimizer(epsilon).minimize(loss)     #optimization function, our goal is to minimize the loss\n",
    "\n",
    "predict = tf.argmax(y_conv,1)   #the predicted class\n",
    "\n",
    "# calculate the accuray, the corrected classified divided by the total size\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "#saver to save the training check point\n",
    "# variables can be restored in a new model by 'saver.restore(sess, save_path)'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())  #initialize the variables\n",
    "\n",
    "\n",
    "for i in range(1,TRAIN_ITER):       #training iterations\n",
    "    d, l = next_batch(train_feature, y_m, 100)      # get 100 samples in one batch\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d), len(l)))\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d[0]), len(l[0])))\n",
    "    _, ls=sess.run([train_step,cross_entropy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:True})     #run the train step (optimization function), the second one is just to show the loss in this iteration.   THE FEED dictionary is to feed the place holders which are needed in the optimization function.\n",
    "    \n",
    "    if i%display_step==0:\n",
    "        print(_, i)\n",
    "        acc = sess.run([accuracy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:False})\n",
    "        print(\"Train Loss:\", ls, \"Acc:\", acc)\n",
    "\n",
    "# sess.run  or tensor.eval are two ways\n",
    "# get the accuracy in the testing data\n",
    "print(accuracy.eval(session=sess, feed_dict={x:test_feature, y_:test_y_m, keep_prob: 1, is_training:False}))\n",
    "\n",
    "\n",
    "# save the model results\n",
    "save_path = saver.save(sess, \"./model-noleaky.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[  41   90   93   95   97  171  173  174  178  203  211  237  250  269  277\n",
      "  290  432  449  475  651  658  771  831  833  835  839  853  854  859  970\n",
      "  975  978  998 1037 1051 1057 1124 1171 1172 1173 1174 1178]\n",
      "42\n",
      "index 41 predicted label 0, but true label is 1\n",
      "('255056009', '363346000') Concept Pairs: (malignant tumor of head and/or neck --- malignant neoplastic disease)\n",
      "index 90 predicted label 1, but true label is 0\n",
      "('310577003', '170461002') Concept Pairs: (pneumococcal immunization status --- requires a course of gamma globulin)\n",
      "index 93 predicted label 1, but true label is 0\n",
      "('13180005', '36999000') Concept Pairs: (retrograde menstruation --- helcomenia)\n",
      "index 95 predicted label 1, but true label is 0\n",
      "('103018003', '103304001') Concept Pairs: (exertional dizziness --- vertigo associated with recent change in eyeglasses)\n",
      "index 97 predicted label 1, but true label is 0\n",
      "('315018008', '103304001') Concept Pairs: (dizzy spells --- vertigo associated with recent change in eyeglasses)\n",
      "index 171 predicted label 1, but true label is 0\n",
      "('445060000', '408557004') Concept Pairs: (left against medical advice --- primary health care team falls assessment defaulted)\n",
      "index 173 predicted label 1, but true label is 0\n",
      "('135821003', '305455009') Concept Pairs: (has support worker --- under care of general practitioner)\n",
      "index 174 predicted label 1, but true label is 0\n",
      "('305479006', '305455009') Concept Pairs: (under care of clinical pharmacologist --- under care of general practitioner)\n",
      "index 178 predicted label 1, but true label is 0\n",
      "('314430004', '408557004') Concept Pairs: (presence of interpreter --- primary health care team falls assessment defaulted)\n",
      "index 203 predicted label 0, but true label is 1\n",
      "('429114002', '254650008') Concept Pairs: (malignant basal cell neoplasm of skin --- malignant epithelial neoplasm of skin)\n",
      "index 211 predicted label 1, but true label is 0\n",
      "('716457009', '105729006') Concept Pairs: (able to tolerate changing incontinence pad --- finding of health perception, health management pattern)\n",
      "index 237 predicted label 1, but true label is 0\n",
      "('310455000', '310452002') Concept Pairs: (medical report requested --- disabled driver report status)\n",
      "index 250 predicted label 1, but true label is 0\n",
      "('21243004', '4886009') Concept Pairs: (term birth of newborn --- premature birth of newborn male)\n",
      "index 269 predicted label 0, but true label is 1\n",
      "('124509009', '129456006') Concept Pairs: (deficiency of amidase --- specific enzyme deficiency)\n",
      "index 277 predicted label 1, but true label is 0\n",
      "('416075005', '314843006') Concept Pairs: (on learning disability register --- immediately necessary treatment registration expired)\n",
      "index 290 predicted label 1, but true label is 0\n",
      "('308031009', '185272004') Concept Pairs: (seen in department --- seen in psychogeriatric clinic)\n",
      "index 432 predicted label 1, but true label is 0\n",
      "('413330005', '44552000') Concept Pairs: (victim of oppression in country of origin --- victim of aerial warfare)\n",
      "index 449 predicted label 0, but true label is 1\n",
      "('410241005', '386053000') Concept Pairs: (ear wax removal assessment --- evaluation procedure)\n",
      "index 475 predicted label 1, but true label is 0\n",
      "('426655001', '5181007') Concept Pairs: (disorder of aromatic amino acid metabolism --- disorder of tryptophan metabolism)\n",
      "index 651 predicted label 1, but true label is 0\n",
      "('88939009', '79842004') Concept Pairs: (severe mood disorder without psychotic features --- stuporous depression)\n",
      "index 658 predicted label 1, but true label is 0\n",
      "('715924009', '79842004') Concept Pairs: (disruptive mood dysregulation disorder --- stuporous depression)\n",
      "index 771 predicted label 1, but true label is 0\n",
      "('305756006', '305749000') Concept Pairs: (seen by liaison nurse --- seen by rheumatology nurse specialist)\n",
      "index 831 predicted label 1, but true label is 0\n",
      "('112104007', '425908001') Concept Pairs: (localized pain --- pain radiating to thoracic region right side)\n",
      "index 833 predicted label 1, but true label is 0\n",
      "('52598005', '425908001') Concept Pairs: (rest pain --- pain radiating to thoracic region right side)\n",
      "index 835 predicted label 1, but true label is 0\n",
      "('428209001', '425908001') Concept Pairs: (finding related to onset of pain --- pain radiating to thoracic region right side)\n",
      "index 839 predicted label 1, but true label is 0\n",
      "('36163009', '425908001') Concept Pairs: (night pain --- pain radiating to thoracic region right side)\n",
      "index 853 predicted label 1, but true label is 0\n",
      "('183737004', '183715006') Concept Pairs: (domiciliary visit received --- geriatric domiciliary visit requested)\n",
      "index 854 predicted label 1, but true label is 0\n",
      "('249857004', '105728003') Concept Pairs: (loss of midline awareness --- nursing observation and/or diagnosis)\n",
      "index 859 predicted label 1, but true label is 0\n",
      "('716461003', '105728003') Concept Pairs: (able to tolerate toileting routine --- nursing observation and/or diagnosis)\n",
      "index 970 predicted label 1, but true label is 0\n",
      "('305907000', '705062002') Concept Pairs: (seen in day hospital --- seen in bariatric surgery clinic)\n",
      "index 975 predicted label 1, but true label is 0\n",
      "('270420001', '705062002') Concept Pairs: (seen in own home --- seen in bariatric surgery clinic)\n",
      "index 978 predicted label 1, but true label is 0\n",
      "('313104000', '705062002') Concept Pairs: (seen in drug rehabilitation center --- seen in bariatric surgery clinic)\n",
      "index 998 predicted label 1, but true label is 0\n",
      "('21630007', '360619001') Concept Pairs: (glutathione s-transferase deficiency --- deficiency of non-specific cholinesterase)\n",
      "index 1037 predicted label 1, but true label is 0\n",
      "('401068004', '305700007') Concept Pairs: (seen by smoking cessation advisor --- seen by public health physician)\n",
      "index 1051 predicted label 1, but true label is 0\n",
      "('80231000119105', '124226002') Concept Pairs: (circulating enzyme deficiency --- deficiency of dehydrogenase)\n",
      "index 1057 predicted label 1, but true label is 0\n",
      "('3642008', '124226002') Concept Pairs: (disorder involving deficiency of steryl-sulfatase (ec 3.1.6.2) --- deficiency of dehydrogenase)\n",
      "index 1124 predicted label 0, but true label is 1\n",
      "('218358001', '95320005') Concept Pairs: (incontinentia pigmenti achromians syndrome --- disorder of skin)\n",
      "index 1171 predicted label 1, but true label is 0\n",
      "('183708003', '183764003') Concept Pairs: (domiciliary visit requested --- urological domiciliary visit done)\n",
      "index 1172 predicted label 1, but true label is 0\n",
      "('270420001', '185239004') Concept Pairs: (seen in own home --- seen in plastic surgery clinic)\n",
      "index 1173 predicted label 1, but true label is 0\n",
      "('305911006', '185239004') Concept Pairs: (seen in hospice --- seen in plastic surgery clinic)\n",
      "index 1174 predicted label 1, but true label is 0\n",
      "('305907000', '185239004') Concept Pairs: (seen in day hospital --- seen in plastic surgery clinic)\n",
      "index 1178 predicted label 1, but true label is 0\n",
      "('241831000', '241828001') Concept Pairs: (poisoning from sting of jelly fishes --- poisoning by cone shell venom)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = sess.run(predict, feed_dict={x:test_feature, keep_prob:1, is_training:False})\n",
    "print(y_pred[:20])\n",
    "print(test_y[:20])\n",
    "\n",
    "\n",
    "err_ids=np.flatnonzero(y_pred != test_y)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)\n",
    "for err_id in err_ids:\n",
    "    print(\"index %d predicted label %s, but true label is %s\" % (err_id, y_pred[err_id], test_y[err_id]))\n",
    "    idpair = test_list_ids[err_id] \n",
    "    concept1 = conceptLabelDict[idpair[0]]\n",
    "    concept2 = conceptLabelDict[idpair[1]]\n",
    "    print(\"%s Concept Pairs: (%s --- %s)\" % (idpair, concept1, concept2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965\n",
      "0.964443565401\n",
      "0.9649750934\n",
      "0.965\n",
      "0.9650249066\n",
      "[ 0.9640411   0.96590909]\n",
      "[ 0.93833333  0.99166667]\n",
      "[ 0.99119718  0.9414557 ]\n",
      "0.966326439651\n"
     ]
    }
   ],
   "source": [
    "result = y_pred\n",
    "test_label_list = test_y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(accuracy_score(result, test_label_list))\n",
    "print(average_precision_score(result, test_label_list))\n",
    "\n",
    "print(f1_score(result, test_label_list, average='macro') ) \n",
    "\n",
    "print(f1_score(result, test_label_list, average='micro')  )\n",
    "\n",
    "print(f1_score(result, test_label_list, average='weighted') )\n",
    "\n",
    "print(f1_score(result, test_label_list, average=None))\n",
    "\n",
    "print(precision_score(result, test_label_list, average=None))\n",
    "print(recall_score(result, test_label_list, average=None))\n",
    "\n",
    "print(roc_auc_score(result, test_label_list, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "err_ids=np.flatnonzero(result != test_label_list)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)\n",
    "for err_id in err_ids:\n",
    "    idpair = test_list_ids[err_id] \n",
    "    print(idpair)\n",
    "    concept1 = conceptLabelDict[idpair[0]]\n",
    "    concept2 = conceptLabelDict[idpair[1]]\n",
    "    print(\"(Concept 1 %s ---- Concept 2 %s)\" % (concept1, concept2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = [[1,2,3], [3,4,5], [3,4,5], [5,6,7],[2,3,7]]\n",
    "\n",
    "plist.extend([1]*4)\n",
    "print(plist)\n",
    "\n",
    "index = np.arange(5)\n",
    "\n",
    "c = [plist[b] for b in index[:2]]\n",
    "print(c)\n",
    "c.extend([plist[b] for b in index[2:]])\n",
    "print(c)\n",
    "\n",
    "for i in range(3):      # i is the class index, for example, i==0 for class 0, i==1 for class 1 ...\n",
    "    index = np.arange(30)     #generate numbers from 0 to 2999\n",
    "    np.random.shuffle(index)        #shuffle the 3000 values\n",
    "    index = [int(300*i+j) for j in index]\n",
    "    print(index)\n",
    "    print(np.array(index)[0:5])\n",
    "    print(np.array(index)[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([1,2,3])\n",
    "y=np.append(y, [1]*4)\n",
    "y= np.append(y, [0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/MLOntology/model1\"\n",
    "\n",
    "print(path)\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(inferred_vector)\n",
    "print(inferred_vector.size)\n",
    "# pprint(model.docvecs.most_similar([inferred_vector], topn=20))\n",
    "\n",
    "\n",
    "path = \"D:/MLOntology/model0\"\n",
    "\n",
    "print(path)\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(inferred_vector)\n",
    "print(inferred_vector.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs['SENT_5690']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "print(X)\n",
    "print(y)\n",
    "train_errors=[]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)  \n",
    "\n",
    "train_errors.append(clf.score(X, y))\n",
    "print(train_errors)\n",
    "X_test=[[2,2]]\n",
    "y_test = [1]\n",
    "test_errors=[]\n",
    "clf.predict(X_test)\n",
    "\n",
    "test_errors.append(clf.score(X_test, y_test))\n",
    "print(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "a= model.docvecs[0]\n",
    "b= model.docvecs[1]\n",
    "m1 = np.array((a, b))\n",
    "\n",
    "# print(np.reshape(m1, 1024))\n",
    "# print(np.reshape(m1, 400, order='F')) # two ways of reshape\n",
    "\n",
    "c= model.docvecs[2]\n",
    "d= model.docvecs[3]\n",
    "print(c.shape)\n",
    "print(d.shape)\n",
    "print(c.shape[0]+d.shape[0])\n",
    "m2 = np.array((c, d))\n",
    "\n",
    "m1 = np.reshape(m1, 1024)\n",
    "m2 = np.reshape(m2, 1024)\n",
    "# m1 = np.reshape(m1, 1024, order='F')\n",
    "# m2 = np.reshape(m2, 1024, order='F')\n",
    "\n",
    "print(m1)\n",
    "\n",
    "X = [m1, m2]\n",
    "print(X)\n",
    "\n",
    "XX = np.append(m1, m2)\n",
    "print(XX)\n",
    "\n",
    "y = [0, 1]\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X, y)\n",
    "\n",
    "m3 = np.array((a,d))\n",
    "m3 = np.reshape(m3, 1024, order='F')\n",
    "\n",
    "result = clf.predict([m3])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=  np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "m1 = np.array((a, b))\n",
    "print(m1)\n",
    "\n",
    "m2 = np.vstack((a, b)).T\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDict={}\n",
    "testDict[0] = (\"a\", \"b\")\n",
    "print(testDict[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,1,0,1,0,1,0])\n",
    "b = np.array([0,1,0,1,0,1,1])\n",
    "\n",
    "np.flatnonzero(a!=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
