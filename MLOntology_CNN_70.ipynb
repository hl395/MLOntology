{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['37225000', '52860004', 1], ['159386001', '159385002', 1], ['233836002', '233835003', 1], ['233836002', '304914007', 1], ['224923003', '224717003', 1]]\n",
      "502206\n"
     ]
    }
   ],
   "source": [
    "conceptPairDict={}\n",
    "errors=[]\n",
    "conceptPairList=[]\n",
    "\n",
    "def read_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptPairList.append([splitted[1], splitted[2].replace(\"\\r\\n\", \"\"), 1])\n",
    "#                 conceptPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "pair_file = \"/home/hao/AnacondaProjects/MLOntology/ontHierarchy.txt\"\n",
    "read_pair(pair_file)\n",
    "\n",
    "first2pairs = conceptPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['273187009', '272765000', 0], ['272877001', '272765000', 0], ['273216002', '272765000', 0], ['273125004', '272765000', 0], ['272973003', '272765000', 0]]\n",
      "6166563\n"
     ]
    }
   ],
   "source": [
    "conceptNotPairDict={}\n",
    "conceptNotPairList=[]\n",
    "\n",
    "def read_not_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                conceptNotPairList.append([splitted[0], splitted[1].replace(\"\\r\\n\", \"\"), 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "notPair_file = \"/home/hao/AnacondaProjects/MLOntology/taxNotPairs.txt\"\n",
    "read_not_pair(notPair_file)\n",
    "\n",
    "# first2pairs = {k: conceptNotPairDict[k] for k in list(conceptNotPairDict)[10:15]}\n",
    "first2pairs =conceptNotPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('237267007', 0.6276178359985352),\n",
      " ('722912007', 0.6031656265258789),\n",
      " ('446466006', 0.592012882232666),\n",
      " ('722913002', 0.5901555418968201),\n",
      " ('67798003', 0.5710010528564453),\n",
      " ('253745002', 0.567590594291687),\n",
      " ('10759661000119108', 0.5674116611480713),\n",
      " ('277485007', 0.5650626420974731),\n",
      " ('177130000', 0.5645525455474854),\n",
      " ('312974005', 0.5601972341537476)]\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/hao/AnacondaProjects/MLOntology/model0\"\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/MLOntology/model1\"\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420042, 420015, 420040, 420099, 420077, 420064, 420083, 420096, 420007, 420048]\n",
      "[420143, 420126, 420177, 420199, 420175, 420147, 420186, 420149, 420135, 420188]\n",
      "[420260, 420291, 420249, 420203, 420233, 420214, 420213, 420297, 420234, 420281]\n",
      "[420359, 420346, 420354, 420320, 420305, 420397, 420362, 420330, 420343, 420332]\n",
      "[420445, 420408, 420449, 420486, 420414, 420428, 420418, 420401, 420425, 420493]\n",
      "[420542, 420516, 420513, 420559, 420577, 420539, 420564, 420571, 420551, 420553]\n",
      "[420615, 420624, 420651, 420675, 420634, 420631, 420628, 420632, 420690, 420640]\n",
      "[420716, 420767, 420737, 420724, 420789, 420763, 420751, 420798, 420742, 420775]\n",
      "[420877, 420846, 420884, 420835, 420890, 420829, 420818, 420810, 420893, 420865]\n",
      "[420966, 420900, 420915, 420912, 420989, 420974, 420936, 420937, 420913, 420991]\n",
      "[421082, 421011, 421022, 421018, 421075, 421055, 421097, 421089, 421013, 421037]\n",
      "[421169, 421168, 421141, 421165, 421156, 421132, 421185, 421149, 421182, 421119]\n",
      "[421255, 421290, 421221, 421232, 421281, 421256, 421286, 421295, 421287, 421267]\n",
      "[421327, 421373, 421399, 421339, 421323, 421363, 421362, 421365, 421324, 421318]\n",
      "[421472, 421491, 421408, 421419, 421404, 421429, 421445, 421484, 421446, 421421]\n",
      "[421534, 421530, 421521, 421571, 421533, 421518, 421537, 421507, 421517, 421563]\n",
      "[421687, 421656, 421681, 421694, 421632, 421646, 421608, 421639, 421606, 421602]\n",
      "[421702, 421733, 421775, 421786, 421782, 421714, 421729, 421758, 421727, 421794]\n",
      "[421828, 421849, 421868, 421832, 421815, 421802, 421893, 421806, 421809, 421829]\n",
      "[421916, 421992, 421967, 421998, 421986, 421999, 421951, 421921, 421952, 421934]\n",
      "[422079, 422048, 422085, 422010, 422064, 422027, 422088, 422077, 422076, 422013]\n",
      "[422127, 422179, 422132, 422178, 422164, 422156, 422139, 422111, 422173, 422195]\n",
      "[422256, 422211, 422224, 422237, 422214, 422251, 422229, 422202, 422277, 422282]\n",
      "[422385, 422326, 422348, 422313, 422388, 422397, 422398, 422311, 422357, 422394]\n",
      "[422475, 422428, 422425, 422404, 422413, 422410, 422467, 422498, 422495, 422447]\n",
      "[422579, 422588, 422558, 422584, 422595, 422508, 422572, 422541, 422598, 422510]\n",
      "[422621, 422682, 422690, 422679, 422665, 422641, 422628, 422669, 422673, 422615]\n",
      "[422736, 422781, 422765, 422708, 422704, 422743, 422714, 422700, 422715, 422717]\n",
      "[422801, 422853, 422856, 422846, 422887, 422894, 422891, 422824, 422845, 422886]\n",
      "[422948, 422997, 422984, 422998, 422967, 422985, 422974, 422901, 422930, 422955]\n",
      "[423020, 423043, 423098, 423038, 423007, 423037, 423005, 423017, 423000, 423024]\n",
      "[423156, 423188, 423152, 423182, 423190, 423122, 423184, 423151, 423133, 423158]\n",
      "[423229, 423223, 423295, 423265, 423239, 423242, 423207, 423262, 423214, 423211]\n",
      "[423371, 423321, 423308, 423316, 423358, 423340, 423323, 423374, 423351, 423345]\n",
      "[423455, 423409, 423438, 423415, 423486, 423482, 423408, 423434, 423413, 423445]\n",
      "[423507, 423595, 423584, 423541, 423578, 423539, 423597, 423551, 423568, 423535]\n",
      "[423648, 423602, 423691, 423622, 423672, 423619, 423675, 423645, 423656, 423607]\n",
      "[423758, 423721, 423747, 423766, 423733, 423772, 423778, 423743, 423750, 423793]\n",
      "[423861, 423855, 423867, 423888, 423885, 423849, 423897, 423848, 423892, 423809]\n",
      "[423934, 423950, 423960, 423943, 423917, 423925, 423900, 423931, 423906, 423951]\n",
      "[424077, 424070, 424002, 424075, 424067, 424019, 424099, 424041, 424044, 424000]\n",
      "[424138, 424180, 424186, 424188, 424194, 424143, 424179, 424121, 424134, 424109]\n",
      "[424266, 424256, 424252, 424240, 424279, 424232, 424201, 424250, 424231, 424278]\n",
      "[424370, 424315, 424328, 424364, 424306, 424336, 424372, 424390, 424304, 424351]\n",
      "[424491, 424429, 424486, 424427, 424438, 424409, 424485, 424431, 424494, 424495]\n",
      "[424510, 424526, 424562, 424537, 424515, 424582, 424558, 424522, 424504, 424512]\n",
      "[424618, 424653, 424677, 424683, 424668, 424634, 424652, 424671, 424673, 424617]\n",
      "[424719, 424700, 424709, 424743, 424777, 424781, 424742, 424783, 424789, 424763]\n",
      "[424887, 424835, 424894, 424895, 424832, 424884, 424839, 424803, 424821, 424872]\n",
      "[424933, 424948, 424979, 424913, 424969, 424937, 424922, 424941, 424923, 424976]\n",
      "[425074, 425095, 425033, 425068, 425094, 425002, 425055, 425016, 425056, 425064]\n",
      "[425109, 425108, 425162, 425130, 425106, 425177, 425141, 425147, 425102, 425151]\n",
      "[425299, 425266, 425282, 425242, 425243, 425212, 425261, 425273, 425223, 425217]\n",
      "[425304, 425311, 425344, 425368, 425305, 425325, 425321, 425354, 425341, 425300]\n",
      "[425482, 425457, 425442, 425448, 425437, 425491, 425401, 425414, 425405, 425475]\n",
      "[425583, 425536, 425545, 425569, 425572, 425538, 425567, 425518, 425549, 425501]\n",
      "[425683, 425654, 425659, 425668, 425622, 425639, 425646, 425605, 425644, 425626]\n",
      "[425704, 425718, 425732, 425756, 425752, 425777, 425761, 425712, 425790, 425755]\n",
      "[425826, 425881, 425816, 425821, 425831, 425832, 425817, 425813, 425845, 425862]\n",
      "[425901, 425982, 425972, 425971, 425934, 425938, 425906, 425937, 425929, 425974]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "feature_number = 1024\n",
    "\n",
    "train_list_before=[]\n",
    "train_label_list=[]\n",
    "test_list_before=[]\n",
    "test_label_list =[]\n",
    "\n",
    "offset = 6000*70\n",
    "\n",
    "for i in range(60):\n",
    "    index = np.arange(100)     #generate numbers from 0 to 100\n",
    "    np.random.shuffle(index)        #shuffle the 100 values\n",
    "    index = [int(100*i+j + offset) for j in index]\n",
    "    print(index[10:20])\n",
    "    train_list_before.extend([conceptPairList[b] for b in index[0:40]]) \n",
    "    train_label_list.extend([1]*40)\n",
    "    train_list_before.extend([conceptNotPairList[b] for b in index[40:80]])\n",
    "    train_label_list.extend([0]*40)\n",
    "    test_list_before.extend([conceptPairList[b] for b in index[80:90]])\n",
    "    test_label_list.extend([1]*10)\n",
    "    test_list_before.extend([conceptNotPairList[b] for b in index[90:100]])\n",
    "    test_label_list.extend([0]*10)\n",
    "\n",
    "\n",
    "    \n",
    "train_list =[]\n",
    "test_list = []\n",
    "\n",
    "for line in train_list_before:\n",
    "    if line[0] in model.docvecs and line[1] in model.docvecs:\n",
    "        a= model.docvecs[line[0]]\n",
    "        b= model.docvecs[line[1]]\n",
    "        c = np.array((a, b))\n",
    "#         train_list.append(np.reshape(c, feature_number)) \n",
    "        train_list.append(np.reshape(c, feature_number, order='F'))\n",
    "\n",
    "\n",
    "test_list_ids={}\n",
    "for i, line in enumerate(test_list_before):\n",
    "    if line[0] in model.docvecs and line[1] in model.docvecs:\n",
    "        a= model.docvecs[line[0]]\n",
    "        b= model.docvecs[line[1]]\n",
    "        c = np.array((a, b))\n",
    "        test_list_ids[i] = (line[0], line[1])\n",
    "#         test_list.append(np.reshape(c, feature_number))\n",
    "        test_list.append(np.reshape(c, feature_number, order='F'))\n",
    "\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.09667846, -0.22888856, -0.11828602, ...,  0.17068371,\n",
      "       -0.07559696, -0.29764587], dtype=float32), array([  6.59864619e-02,  -1.51735768e-01,   1.15716271e-01, ...,\n",
      "         3.68521988e-01,  -2.39618123e-04,  -2.38876730e-01], dtype=float32), array([-0.09667846,  0.00117484, -0.11828602, ...,  0.12857193,\n",
      "       -0.07559696, -0.22679693], dtype=float32), array([-0.06303799, -0.02345722,  0.05710166, ...,  0.06410343,\n",
      "       -0.14062589, -0.16460237], dtype=float32), array([-0.10237666, -0.15087979,  0.08575669, ...,  0.24232902,\n",
      "       -0.09340938, -0.10237291], dtype=float32), array([-0.16328269, -0.2330901 , -0.01823091, ...,  0.14530303,\n",
      "        0.01692824, -0.07097324], dtype=float32), array([-0.0110109 , -0.09295212,  0.03703131, ...,  0.29461715,\n",
      "       -0.04684466,  0.0138872 ], dtype=float32), array([ 0.06598646,  0.14911251,  0.11571627, ...,  0.1633248 ,\n",
      "       -0.00023962, -0.12394836], dtype=float32), array([-0.05407899,  0.21701236,  0.07383446, ...,  0.00594241,\n",
      "       -0.12863521,  0.06928549], dtype=float32), array([-0.00243815, -0.0190145 , -0.1281904 , ..., -0.00589192,\n",
      "        0.02598577, -0.03734702], dtype=float32), array([ 0.05381349,  0.53877944, -0.25303808, ...,  0.2487746 ,\n",
      "        0.00929643,  0.38619459], dtype=float32), array([-0.07040787,  0.00131376, -0.16179682, ...,  0.3029801 ,\n",
      "        0.07697953, -0.06461965], dtype=float32), array([-0.22486062, -0.07641635,  0.06251881, ...,  0.12764972,\n",
      "       -0.02426124, -0.02291545], dtype=float32), array([-0.1303864 , -0.07641635, -0.1666864 , ...,  0.12764972,\n",
      "       -0.09061842, -0.02291545], dtype=float32), array([-0.01024445,  0.53877944, -0.09590059, ...,  0.2487746 ,\n",
      "        0.07660668,  0.38619459], dtype=float32), array([-0.0946484 ,  0.00978857, -0.03246938, ...,  0.07448416,\n",
      "        0.14004312,  0.07146799], dtype=float32), array([-0.16127214, -0.07641635,  0.00387149, ...,  0.12764972,\n",
      "        0.01585105, -0.02291545], dtype=float32), array([-0.09066872,  0.042509  , -0.01178297, ...,  0.19613692,\n",
      "       -0.04783517,  0.16956054], dtype=float32), array([ 0.00977417, -0.07641635, -0.17557016, ...,  0.12764972,\n",
      "        0.02485028, -0.02291545], dtype=float32), array([ 0.24474208,  0.53877944,  0.01189439, ...,  0.2487746 ,\n",
      "        0.3804422 ,  0.38619459], dtype=float32)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4800\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "print(train_list[30:50])\n",
    "print(train_label_list[30:50])\n",
    "print(len(train_list))\n",
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "[0.96416666666666662]\n"
     ]
    }
   ],
   "source": [
    "#SVM \n",
    "\n",
    "\n",
    "print(len(train_list))\n",
    "\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(train_list, train_label_list)\n",
    "\n",
    "train_errors=[]\n",
    "train_errors.append(clf.score(train_list, train_label_list))\n",
    "print(train_errors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptLabelDict={}\n",
    "errors=[]\n",
    "\n",
    "def read_label(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptLabelDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "label_file = \"/home/hao/AnacondaProjects/MLOntology/ontClassLabels.txt\"\n",
    "read_label(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 70 predicted label [1], but true label is 0\n",
      "('359704000', '87397002') Concept Pairs: (von willebrand disease, type 1^a^ --- von willebrand disease, type iia)\n",
      "index 100 predicted label [0], but true label is 1\n",
      "('449753006', '106076001') Concept Pairs: (pressure point of skin --- skin finding)\n",
      "index 246 predicted label [0], but true label is 1\n",
      "('299805003', '366353002') Concept Pairs: (jaw reflex brisk --- jaw reflex finding)\n",
      "index 270 predicted label [1], but true label is 0\n",
      "('301821009', '722915009') Concept Pairs: (drug-induced apnea --- apnea of newborn due to neurological injury)\n",
      "index 271 predicted label [1], but true label is 0\n",
      "('191997003', '288091000119109') Concept Pairs: (persistent insomnia --- behavioral insomnia of childhood, limit setting type)\n",
      "index 273 predicted label [1], but true label is 0\n",
      "('59050008', '288091000119109') Concept Pairs: (initial insomnia --- behavioral insomnia of childhood, limit setting type)\n",
      "index 274 predicted label [1], but true label is 0\n",
      "('54230003', '288091000119109') Concept Pairs: (mixed insomnia --- behavioral insomnia of childhood, limit setting type)\n",
      "index 275 predicted label [1], but true label is 0\n",
      "('446263001', '253879006') Concept Pairs: (loeys-dietz syndrome --- adult type polycystic kidney disease type 2)\n",
      "index 276 predicted label [1], but true label is 0\n",
      "('248259004', '288091000119109') Concept Pairs: (symptoms interfere with sleep --- behavioral insomnia of childhood, limit setting type)\n",
      "index 277 predicted label [1], but true label is 0\n",
      "('248258007', '288091000119109') Concept Pairs: (circumstances interfere with sleep --- behavioral insomnia of childhood, limit setting type)\n",
      "index 279 predicted label [1], but true label is 0\n",
      "('1821000', '722915009') Concept Pairs: (chemoreceptor apnea --- apnea of newborn due to neurological injury)\n",
      "index 305 predicted label [0], but true label is 1\n",
      "('171357000', '103736005') Concept Pairs: (shooting medical examination --- history and physical examination, sports participation)\n",
      "index 310 predicted label [1], but true label is 0\n",
      "('123031000119109', '40070004') Concept Pairs: (infection caused by yatapoxvirus --- molluscum contagiosum infection)\n",
      "index 313 predicted label [1], but true label is 0\n",
      "('414017008', '40070004') Concept Pairs: (disease caused by parapoxvirus --- molluscum contagiosum infection)\n",
      "index 318 predicted label [1], but true label is 0\n",
      "('407457004', '40070004') Concept Pairs: (disease caused by polyomaviridae --- molluscum contagiosum infection)\n",
      "index 330 predicted label [1], but true label is 0\n",
      "('446953001', '11618000') Concept Pairs: (dehiscence of anastomosis --- intra-amniotic infection of fetus)\n",
      "index 335 predicted label [1], but true label is 0\n",
      "('83223005', '40070004') Concept Pairs: (disease caused by parvoviridae --- molluscum contagiosum infection)\n",
      "index 468 predicted label [0], but true label is 1\n",
      "('299806002', '366353002') Concept Pairs: (jaw reflex reduced --- jaw reflex finding)\n",
      "index 473 predicted label [1], but true label is 0\n",
      "('238150007', '127341000119105') Concept Pairs: (sepsis syndrome --- severe sepsis with acute organ dysfunction caused by pneumococcus)\n",
      "index 475 predicted label [1], but true label is 0\n",
      "('251526004', '127351000119107') Concept Pairs: (single organ dysfunction --- severe sepsis with acute organ dysfunction caused by pseudomonas)\n",
      "index 476 predicted label [1], but true label is 0\n",
      "('14836003', '403722002') Concept Pairs: (radiation recall syndrome --- ulceration of skin caused by therapeutic ionizing radiation)\n",
      "index 477 predicted label [1], but true label is 0\n",
      "('193548006', '15993671000119108') Concept Pairs: (steroid-induced glaucoma glaucomatous stage --- open-angle glaucoma of right eye caused by steroid)\n",
      "index 478 predicted label [1], but true label is 0\n",
      "('193548006', '15993711000119107') Concept Pairs: (steroid-induced glaucoma glaucomatous stage --- open-angle glaucoma of left eye caused by steroid)\n",
      "index 492 predicted label [1], but true label is 0\n",
      "('111746009', '193067002') Concept Pairs: (mechanical complication of device --- nervous system complication from surgically implanted device)\n",
      "index 497 predicted label [1], but true label is 0\n",
      "('251526004', '1092601000119103') Concept Pairs: (single organ dysfunction --- severe sepsis with acute organ dysfunction caused by gonococcus)\n",
      "index 671 predicted label [1], but true label is 0\n",
      "('127209001', '164152003') Concept Pairs: (tibial lymphadenopathy --- on examination - popliteal lymphadenopathy)\n",
      "index 689 predicted label [0], but true label is 1\n",
      "('116312005', '302293008') Concept Pairs: (finding of lower limb --- finding of limb structure)\n",
      "index 884 predicted label [0], but true label is 1\n",
      "('118932009', '116316008') Concept Pairs: (disorder of foot --- finding of foot region)\n",
      "index 940 predicted label [0], but true label is 1\n",
      "('60737008', '238145001') Concept Pairs: (iron overload --- trace element excess)\n",
      "index 969 predicted label [0], but true label is 1\n",
      "('300341002', '249569004') Concept Pairs: (gallbladder absent --- gallbladder finding)\n",
      "index 1144 predicted label [0], but true label is 1\n",
      "('124702003', '129456006') Concept Pairs: (deficiency of glutarate-coa ligase --- specific enzyme deficiency)\n"
     ]
    }
   ],
   "source": [
    "for i, (item, label) in enumerate(zip(test_list, test_label_list)):\n",
    "    result = clf.predict([item])\n",
    "    if result != label:\n",
    "        print(\"index %d predicted label %s, but true label is %s\" % (i, result, label))\n",
    "        idpair = test_list_ids[i] \n",
    "        concept1 = conceptLabelDict[idpair[0]]\n",
    "        concept2 = conceptLabelDict[idpair[1]]\n",
    "        print(\"%s Concept Pairs: (%s --- %s)\" % (idpair, concept1, concept2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "1200\n",
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# m3 = np.array((a,d))\n",
    "# m3 = np.reshape(m3, 400, order='F')\n",
    "print(len(test_list))\n",
    "result = clf.predict(test_list)\n",
    "\n",
    "print(result.size)\n",
    "\n",
    "\n",
    "print(result[:29])\n",
    "print(np.array(test_label_list[:29]))\n",
    "\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  70  100  246  270  271  273  274  275  276  277  279  305  310  313  318\n",
      "  330  335  468  473  475  476  477  478  492  497  671  689  884  940  969\n",
      " 1144]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "err_ids=np.flatnonzero(result != test_label_list)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974166666667\n",
      "0.967982599239\n",
      "0.974163634482\n",
      "0.974166666667\n",
      "0.974169698851\n",
      "[ 0.97388374  0.97444353]\n",
      "[ 0.96333333  0.985     ]\n",
      "[ 0.9846678   0.96411093]\n",
      "0.974389366119\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(accuracy_score(result, test_label_list))\n",
    "print(average_precision_score(result, test_label_list))\n",
    "\n",
    "print(f1_score(result, test_label_list, average='macro') ) \n",
    "\n",
    "print(f1_score(result, test_label_list, average='micro')  )\n",
    "\n",
    "print(f1_score(result, test_label_list, average='weighted') )\n",
    "\n",
    "print(f1_score(result, test_label_list, average=None))\n",
    "\n",
    "print(precision_score(result, test_label_list, average=None))\n",
    "print(recall_score(result, test_label_list, average=None))\n",
    "\n",
    "print(roc_auc_score(result, test_label_list, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label_list_m = np.eye(2)[train_label_list]\n",
    "# test_label_list_m = np.eye(2)[test_label_list]\n",
    "# print(test_label_list_m[10:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 100\n",
      "Train Loss: 0.685398 Acc: [0.46000001]\n",
      "None 200\n",
      "Train Loss: 0.671084 Acc: [0.44999999]\n",
      "None 300\n",
      "Train Loss: 0.646757 Acc: [0.46000001]\n",
      "None 400\n",
      "Train Loss: 0.561262 Acc: [0.55000001]\n",
      "None 500\n",
      "Train Loss: 0.484874 Acc: [0.76999998]\n",
      "None 600\n",
      "Train Loss: 0.481232 Acc: [0.75999999]\n",
      "None 700\n",
      "Train Loss: 0.464998 Acc: [0.81999999]\n",
      "None 800\n",
      "Train Loss: 0.395949 Acc: [0.91000003]\n",
      "None 900\n",
      "Train Loss: 0.369665 Acc: [0.94]\n",
      "None 1000\n",
      "Train Loss: 0.421549 Acc: [0.93000001]\n",
      "None 1100\n",
      "Train Loss: 0.160405 Acc: [0.97000003]\n",
      "None 1200\n",
      "Train Loss: 0.124781 Acc: [0.98000002]\n",
      "None 1300\n",
      "Train Loss: 0.0826861 Acc: [0.98000002]\n",
      "None 1400\n",
      "Train Loss: 0.0518536 Acc: [0.98000002]\n",
      "0.989167\n",
      "Model saved in file: ./model-noleaky.ckpt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "In the data, there are 2 classes and every class has 3000 samples and every sample has 512 features\n",
    "The first 3000 samples are from class 0, second 3000 are from class 1\n",
    "'''\n",
    "# DATA_DIR = ''\n",
    "CLASS_NUM = 2       #there are 2 classes\n",
    "SPLIT_PERCENT = 0.8     #split the data into 80% for training and 20% for testing\n",
    "FEATURE_NUM = 1024   \n",
    "TRAIN_ITER = 1500    #the number of iterations for training\n",
    "display_step = 100        #how many iterations to display the results\n",
    "\n",
    "\n",
    "train_num = int(3000*SPLIT_PERCENT)     #the number of samples for training\n",
    "\n",
    "train_feature = train_list      #training features (list of list)\n",
    "train_y = train_label_list        #training lables    (list)\n",
    "test_feature = test_list       #test features  (list of list)\n",
    "test_y = test_label_list         #test labels    (list)\n",
    "\n",
    "\n",
    "y_m = np.eye(2)[train_y]\n",
    "test_y_m = np.eye(2)[test_y]\n",
    "\n",
    "'''\n",
    "y = wx+b        (vectors)\n",
    "'''\n",
    "#function to get variables 'w'\n",
    "def weight_variable(shape, num):\n",
    "    initial = tf.truncated_normal(shape, stddev=1/num)\n",
    "    return tf.Variable(initial, name='weight')\n",
    "\n",
    "#the bias 'b' in the equations\n",
    "def bias_variable(shape, num):\n",
    "    initial = tf.constant(0.0001, shape=shape)\n",
    "    return tf.Variable(initial, name='bias')\n",
    "\n",
    "#convolutional process\n",
    "def conv1d(x, W):\n",
    "    return tf.nn.conv1d(x, W, stride=1, padding='SAME')     #x: variable, w: weight, stride and padding (padding can be ignored currently) \n",
    "\n",
    "#pooling process\n",
    "def max_pool_1x1(x, shape):\n",
    "    x=tf.reshape(x,shape)       #it is transfered into four dimensions, but the other three are 1\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 4, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "'''\n",
    "The feature is 3 dimensional data.  [batch, length, channel] \n",
    "batch is usually ignored (for example there are 100 samples in a batch, so samples should not be modified mutually), length and channel are shown in the paper.\n",
    "At first, the length is 512, and channel is 1.\n",
    "Because our data are time series data, so length is enough, but for images, it may be [batch, length, width, channel]\n",
    "'''\n",
    "# the convolutional layer\n",
    "def layer(features, f, input_n, channel, hidden_units, layer_index):\n",
    "    \"\"\"Construct a convolutional layer\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    f: the length\n",
    "    input_n: Size of the features used in the convention.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    layer_index: the index of layer\n",
    "    Returns:\n",
    "    hidden units: The unit output for the next layer.\n",
    "    weights: the weights in the current hidden layer\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden'+str(layer_index)) as scope:     # name scope may be ignored first\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, channel, hidden_units], math.sqrt(f))\n",
    "\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(f))\n",
    "    hidden = relu(conv1d(features, weights) + biases, 0.01)\n",
    "    shape = [-1,1,f,hidden_units]\n",
    "    h_pool1 = max_pool_1x1(hidden,shape)\n",
    "    return h_pool1, weights\n",
    "\n",
    "# fully connected layer, here the data are two dimension, [batch, length]\n",
    "def densely_connect(features, input_n, hidden_units):\n",
    "    \"\"\"Construct a fully (densely) connected layer.\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    input_n: Size of units in the previous layer.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    Returns:\n",
    "    logits: The estimated output in last layer.\n",
    "    weights: the weights in the hidden layer\n",
    "    \"\"\"\n",
    "    with tf.name_scope('softmax_linear') as dense:\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    logits = relu(tf.matmul(features, weights) + biases, 0.01)      # the matrix product operation\n",
    "    return logits, weights\n",
    "\n",
    "# dropout layer (it is not necessary)\n",
    "# randomly set (1-keep_prob) percentage of units to be zero\n",
    "def dropout(features, input_n, hidden_units, keep_prob):\n",
    "    with tf.name_scope('dropout'):\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    h_fc1_drop = tf.nn.dropout(features, keep_prob)\n",
    "    drop_out = relu(tf.matmul(features, weights) + biases, 0.01)\n",
    "    return drop_out\n",
    "\n",
    "# calculate the loss in the neural network\n",
    "def loss(logits, labels):\n",
    "    \"\"\"Calculates the loss from the logits and the labels.\n",
    "    Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size, NUM_CLASSES].\n",
    "    Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        labels = tf.to_int64(labels)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=labels, logits=logits, name='xentropy')\n",
    "    # tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "def next_batch(data, label, num):\n",
    "    \"\"\"Generate the next batch randomly\n",
    "    Args:\n",
    "    data: training data.\n",
    "    label: training label.\n",
    "    num: the size in a batch\n",
    "    Returns:\n",
    "    next batch's training features and labels.\n",
    "    \"\"\"\n",
    "    index = np.arange(len(data))\n",
    "    np.random.shuffle(index)\n",
    "#     train_feature = data[np.array(index)[0:num]]\n",
    "#     train_label = label[np.array(index)[0:num]]\n",
    "    train_feature_batch = [data[b] for b in index[0:num]]\n",
    "    train_feature_batch = np.asarray(train_feature_batch)\n",
    "    train_label_batch = [label[b] for b in index[0:num]]\n",
    "    train_label_batch = np.asarray(train_label_batch)\n",
    "    return train_feature_batch, train_label_batch\n",
    "\n",
    "def relu(x, alpha=0., max_value=None):\n",
    "    '''ReLU.\n",
    "    alpha: slope of negative section.\n",
    "    '''\n",
    "    negative_part = tf.nn.relu(-x)\n",
    "    x = tf.nn.relu(x)\n",
    "    if max_value is not None:\n",
    "        x = tf.clip_by_value(x, tf.cast(0., dtype=tf.float32),\n",
    "                             tf.cast(max_value, dtype=tf.float32))\n",
    "    x -= tf.constant(alpha, dtype=tf.float32) * negative_part\n",
    "    return x\n",
    "\n",
    "#define a session to run the model\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#place holders for training features and label\n",
    "#None means the value is variable\n",
    "x = tf.placeholder(tf.float32, shape=[None, FEATURE_NUM])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, CLASS_NUM])\n",
    "\n",
    "# decide whether it is training or testing, it is not used in our model, but it may be used\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "#from [-1, 512, 1] -> [-1, 256, 32] -> [-1, 128, 64] -> [-1, 64, 64] -> [-1, 32, 64] -> [-1, 16, 64] -> [-1, 8, 64] -> [-1, 200]\n",
    "\n",
    "#6 hidden layers\n",
    "x_1 = tf.reshape(x, [-1,FEATURE_NUM,1])\n",
    "h_pool0, w0 = layer(x_1, FEATURE_NUM, 15, 1, 32, 0)\n",
    "h_pool0 = tf.reshape(h_pool0, [-1,512,32])\n",
    "h_pool1, w1 = layer(h_pool0, 512, 10, 32, 64, 1)\n",
    "\n",
    "h_pool1 = tf.reshape(h_pool1, [-1,256,64])\n",
    "h_pool2, w2 = layer(h_pool1, 256, 10, 64, 64, 2)\n",
    "h_pool2 = tf.reshape(h_pool2, [-1,128,64])\n",
    "h_pool3, w3 = layer(h_pool2, 128, 10, 64, 64, 3)\n",
    "h_pool3 = tf.reshape(h_pool3, [-1,64,64])\n",
    "h_pool4, w4 = layer(h_pool3, 64, 5, 64, 64, 4)\n",
    "h_pool4 = tf.reshape(h_pool4, [-1,32,64])\n",
    "h_pool5, w5 = layer(h_pool4, 32, 5, 64, 64, 5)\n",
    "h_pool5 = tf.reshape(h_pool5, [-1,16,64])\n",
    "h_pool6, w6 = layer(h_pool5, 16, 5, 64, 64, 6)\n",
    "h_pool6 = tf.reshape(h_pool6, [-1,8,64])\n",
    "\n",
    "#densely connected: 200 units\n",
    "h_pool_flat = tf.reshape(h_pool6, [-1, 8*64])\n",
    "h_dc, w_d = densely_connect(h_pool_flat, 8*64, 200)\n",
    "\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y_conv=dropout(h_dc, (int)(h_dc.get_shape()[1]), CLASS_NUM, keep_prob)\n",
    "\n",
    "\n",
    "beta = 0.001\n",
    "cross_entropy = loss(y_conv, y_)\n",
    "loss = cross_entropy +beta*(tf.nn.l2_loss(w0) + tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2)+tf.nn.l2_loss(w3)+tf.nn.l2_loss(w4)+tf.nn.l2_loss(w5)+tf.nn.l2_loss(w6)+tf.nn.l2_loss(w_d))  #L2 regularization\n",
    "epsilon = 1e-5      # learning rate\n",
    "train_step = tf.train.AdamOptimizer(epsilon).minimize(loss)     #optimization function, our goal is to minimize the loss\n",
    "\n",
    "predict = tf.argmax(y_conv,1)   #the predicted class\n",
    "\n",
    "# calculate the accuray, the corrected classified divided by the total size\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "#saver to save the training check point\n",
    "# variables can be restored in a new model by 'saver.restore(sess, save_path)'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())  #initialize the variables\n",
    "\n",
    "\n",
    "for i in range(1,TRAIN_ITER):       #training iterations\n",
    "    d, l = next_batch(train_feature, y_m, 100)      # get 100 samples in one batch\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d), len(l)))\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d[0]), len(l[0])))\n",
    "    _, ls=sess.run([train_step,cross_entropy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:True})     #run the train step (optimization function), the second one is just to show the loss in this iteration.   THE FEED dictionary is to feed the place holders which are needed in the optimization function.\n",
    "    \n",
    "    if i%display_step==0:\n",
    "        print(_, i)\n",
    "        acc = sess.run([accuracy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:False})\n",
    "        print(\"Train Loss:\", ls, \"Acc:\", acc)\n",
    "\n",
    "# sess.run  or tensor.eval are two ways\n",
    "# get the accuracy in the testing data\n",
    "print(accuracy.eval(session=sess, feed_dict={x:test_feature, y_:test_y_m, keep_prob: 1, is_training:False}))\n",
    "\n",
    "\n",
    "# save the model results\n",
    "save_path = saver.save(sess, \"./model-noleaky.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[  12   70   76  473  475  477  478  497  724  884  940  997 1114]\n",
      "13\n",
      "index 12 predicted label 1, but true label is 0\n",
      "('238037008', '10741005') Concept Pairs: (disorder of lipoprotein storage and metabolism --- lipid storage disease)\n",
      "index 70 predicted label 1, but true label is 0\n",
      "('359704000', '87397002') Concept Pairs: (von willebrand disease, type 1^a^ --- von willebrand disease, type iia)\n",
      "index 76 predicted label 1, but true label is 0\n",
      "('277893002', '718751000') Concept Pairs: (carbohydrate-deficient glycoprotein syndrome type i --- component of oligomeric golgi complex 4 congenital disorder of glycosylation)\n",
      "index 473 predicted label 1, but true label is 0\n",
      "('238150007', '127341000119105') Concept Pairs: (sepsis syndrome --- severe sepsis with acute organ dysfunction caused by pneumococcus)\n",
      "index 475 predicted label 1, but true label is 0\n",
      "('251526004', '127351000119107') Concept Pairs: (single organ dysfunction --- severe sepsis with acute organ dysfunction caused by pseudomonas)\n",
      "index 477 predicted label 1, but true label is 0\n",
      "('193548006', '15993671000119108') Concept Pairs: (steroid-induced glaucoma glaucomatous stage --- open-angle glaucoma of right eye caused by steroid)\n",
      "index 478 predicted label 1, but true label is 0\n",
      "('193548006', '15993711000119107') Concept Pairs: (steroid-induced glaucoma glaucomatous stage --- open-angle glaucoma of left eye caused by steroid)\n",
      "index 497 predicted label 1, but true label is 0\n",
      "('251526004', '1092601000119103') Concept Pairs: (single organ dysfunction --- severe sepsis with acute organ dysfunction caused by gonococcus)\n",
      "index 724 predicted label 0, but true label is 1\n",
      "('392091000', '386053000') Concept Pairs: (care regimes assessment --- evaluation procedure)\n",
      "index 884 predicted label 0, but true label is 1\n",
      "('118932009', '116316008') Concept Pairs: (disorder of foot --- finding of foot region)\n",
      "index 940 predicted label 0, but true label is 1\n",
      "('60737008', '238145001') Concept Pairs: (iron overload --- trace element excess)\n",
      "index 997 predicted label 1, but true label is 0\n",
      "('365839002', '309675002') Concept Pairs: (locomotor test finding --- on examination - joint)\n",
      "index 1114 predicted label 1, but true label is 0\n",
      "('164424003', '164607002') Concept Pairs: (on examination - skin red --- on examination - skin shiny over lesion)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = sess.run(predict, feed_dict={x:test_feature, keep_prob:1, is_training:False})\n",
    "print(y_pred[:20])\n",
    "print(test_y[:20])\n",
    "\n",
    "\n",
    "err_ids=np.flatnonzero(y_pred != test_y)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)\n",
    "for err_id in err_ids:\n",
    "    print(\"index %d predicted label %s, but true label is %s\" % (err_id, y_pred[err_id], test_y[err_id]))\n",
    "    idpair = test_list_ids[err_id] \n",
    "    concept1 = conceptLabelDict[idpair[0]]\n",
    "    concept2 = conceptLabelDict[idpair[1]]\n",
    "    print(\"%s Concept Pairs: (%s --- %s)\" % (idpair, concept1, concept2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989166666667\n",
      "0.986941241076\n",
      "0.98916629802\n",
      "0.989166666667\n",
      "0.989167035313\n",
      "[ 0.9891031   0.98922949]\n",
      "[ 0.98333333  0.995     ]\n",
      "[ 0.99494098  0.98352554]\n",
      "0.989233256749\n"
     ]
    }
   ],
   "source": [
    "result = y_pred\n",
    "test_label_list = test_y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(accuracy_score(result, test_label_list))\n",
    "print(average_precision_score(result, test_label_list))\n",
    "\n",
    "print(f1_score(result, test_label_list, average='macro') ) \n",
    "\n",
    "print(f1_score(result, test_label_list, average='micro')  )\n",
    "\n",
    "print(f1_score(result, test_label_list, average='weighted') )\n",
    "\n",
    "print(f1_score(result, test_label_list, average=None))\n",
    "\n",
    "print(precision_score(result, test_label_list, average=None))\n",
    "print(recall_score(result, test_label_list, average=None))\n",
    "\n",
    "print(roc_auc_score(result, test_label_list, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "err_ids=np.flatnonzero(result != test_label_list)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)\n",
    "for err_id in err_ids:\n",
    "    idpair = test_list_ids[err_id] \n",
    "    print(idpair)\n",
    "    concept1 = conceptLabelDict[idpair[0]]\n",
    "    concept2 = conceptLabelDict[idpair[1]]\n",
    "    print(\"(Concept 1 %s ---- Concept 2 %s)\" % (concept1, concept2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = [[1,2,3], [3,4,5], [3,4,5], [5,6,7],[2,3,7]]\n",
    "\n",
    "plist.extend([1]*4)\n",
    "print(plist)\n",
    "\n",
    "index = np.arange(5)\n",
    "\n",
    "c = [plist[b] for b in index[:2]]\n",
    "print(c)\n",
    "c.extend([plist[b] for b in index[2:]])\n",
    "print(c)\n",
    "\n",
    "for i in range(3):      # i is the class index, for example, i==0 for class 0, i==1 for class 1 ...\n",
    "    index = np.arange(30)     #generate numbers from 0 to 2999\n",
    "    np.random.shuffle(index)        #shuffle the 3000 values\n",
    "    index = [int(300*i+j) for j in index]\n",
    "    print(index)\n",
    "    print(np.array(index)[0:5])\n",
    "    print(np.array(index)[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([1,2,3])\n",
    "y=np.append(y, [1]*4)\n",
    "y= np.append(y, [0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/MLOntology/model1\"\n",
    "\n",
    "print(path)\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(inferred_vector)\n",
    "print(inferred_vector.size)\n",
    "# pprint(model.docvecs.most_similar([inferred_vector], topn=20))\n",
    "\n",
    "\n",
    "path = \"D:/MLOntology/model0\"\n",
    "\n",
    "print(path)\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(inferred_vector)\n",
    "print(inferred_vector.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs['SENT_5690']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "print(X)\n",
    "print(y)\n",
    "train_errors=[]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)  \n",
    "\n",
    "train_errors.append(clf.score(X, y))\n",
    "print(train_errors)\n",
    "X_test=[[2,2]]\n",
    "y_test = [1]\n",
    "test_errors=[]\n",
    "clf.predict(X_test)\n",
    "\n",
    "test_errors.append(clf.score(X_test, y_test))\n",
    "print(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "a= model.docvecs[0]\n",
    "b= model.docvecs[1]\n",
    "m1 = np.array((a, b))\n",
    "\n",
    "# print(np.reshape(m1, 1024))\n",
    "# print(np.reshape(m1, 400, order='F')) # two ways of reshape\n",
    "\n",
    "c= model.docvecs[2]\n",
    "d= model.docvecs[3]\n",
    "print(c.shape)\n",
    "print(d.shape)\n",
    "print(c.shape[0]+d.shape[0])\n",
    "m2 = np.array((c, d))\n",
    "\n",
    "m1 = np.reshape(m1, 1024)\n",
    "m2 = np.reshape(m2, 1024)\n",
    "# m1 = np.reshape(m1, 1024, order='F')\n",
    "# m2 = np.reshape(m2, 1024, order='F')\n",
    "\n",
    "print(m1)\n",
    "\n",
    "X = [m1, m2]\n",
    "print(X)\n",
    "\n",
    "XX = np.append(m1, m2)\n",
    "print(XX)\n",
    "\n",
    "y = [0, 1]\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X, y)\n",
    "\n",
    "m3 = np.array((a,d))\n",
    "m3 = np.reshape(m3, 1024, order='F')\n",
    "\n",
    "result = clf.predict([m3])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=  np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "m1 = np.array((a, b))\n",
    "print(m1)\n",
    "\n",
    "m2 = np.vstack((a, b)).T\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDict={}\n",
    "testDict[0] = (\"a\", \"b\")\n",
    "print(testDict[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,1,0,1,0,1,0])\n",
    "b = np.array([0,1,0,1,0,1,1])\n",
    "\n",
    "np.flatnonzero(a!=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
