{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['37225000', '52860004', 1], ['159386001', '159385002', 1], ['233836002', '233835003', 1], ['233836002', '304914007', 1], ['224923003', '224717003', 1]]\n",
      "502206\n"
     ]
    }
   ],
   "source": [
    "conceptPairDict={}\n",
    "errors=[]\n",
    "conceptPairList=[]\n",
    "\n",
    "def read_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptPairList.append([splitted[1], splitted[2].replace(\"\\r\\n\", \"\"), 1])\n",
    "#                 conceptPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "pair_file = \"/home/hao/AnacondaProjects/MLOntology/ontHierarchy.txt\"\n",
    "read_pair(pair_file)\n",
    "\n",
    "first2pairs = conceptPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['273187009', '272765000', 0], ['272877001', '272765000', 0], ['273216002', '272765000', 0], ['273125004', '272765000', 0], ['272973003', '272765000', 0]]\n",
      "6166563\n"
     ]
    }
   ],
   "source": [
    "conceptNotPairDict={}\n",
    "conceptNotPairList=[]\n",
    "\n",
    "def read_not_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                conceptNotPairList.append([splitted[0], splitted[1].replace(\"\\r\\n\", \"\"), 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "notPair_file = \"/home/hao/AnacondaProjects/MLOntology/taxNotPairs.txt\"\n",
    "read_not_pair(notPair_file)\n",
    "\n",
    "# first2pairs = {k: conceptNotPairDict[k] for k in list(conceptNotPairDict)[10:15]}\n",
    "first2pairs =conceptNotPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('237267007', 0.6275449395179749),\n",
      " ('722912007', 0.6037801504135132),\n",
      " ('446466006', 0.5921797752380371),\n",
      " ('722913002', 0.5907285213470459),\n",
      " ('67798003', 0.5716109275817871),\n",
      " ('253745002', 0.568181037902832),\n",
      " ('10759661000119108', 0.5672183036804199),\n",
      " ('277485007', 0.5658161640167236),\n",
      " ('177130000', 0.5643945932388306),\n",
      " ('312974005', 0.5597768425941467)]\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/hao/AnacondaProjects/MLOntology/model0\"\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/MLOntology/model1\"\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300062, 300094, 300024, 300008, 300070, 300036, 300085, 300080, 300044, 300065]\n",
      "[300164, 300132, 300196, 300138, 300180, 300124, 300137, 300133, 300151, 300127]\n",
      "[300287, 300225, 300285, 300299, 300219, 300217, 300222, 300273, 300243, 300294]\n",
      "[300343, 300307, 300394, 300370, 300340, 300365, 300377, 300326, 300395, 300344]\n",
      "[300473, 300441, 300459, 300410, 300465, 300428, 300457, 300423, 300443, 300422]\n",
      "[300569, 300503, 300530, 300567, 300586, 300534, 300541, 300525, 300581, 300589]\n",
      "[300612, 300633, 300663, 300687, 300626, 300653, 300661, 300686, 300654, 300610]\n",
      "[300791, 300792, 300747, 300760, 300761, 300779, 300756, 300782, 300720, 300736]\n",
      "[300819, 300858, 300868, 300840, 300827, 300888, 300830, 300801, 300859, 300805]\n",
      "[300983, 300925, 300941, 300950, 300980, 300970, 300988, 300929, 300903, 300952]\n",
      "[301007, 301096, 301009, 301010, 301033, 301086, 301015, 301001, 301054, 301095]\n",
      "[301163, 301151, 301175, 301112, 301150, 301155, 301120, 301168, 301164, 301170]\n",
      "[301278, 301222, 301204, 301254, 301244, 301201, 301255, 301235, 301276, 301214]\n",
      "[301309, 301389, 301395, 301319, 301384, 301304, 301370, 301387, 301305, 301350]\n",
      "[301444, 301433, 301457, 301469, 301450, 301438, 301480, 301402, 301465, 301448]\n",
      "[301506, 301513, 301583, 301531, 301528, 301565, 301597, 301551, 301547, 301591]\n",
      "[301647, 301694, 301602, 301616, 301626, 301668, 301692, 301680, 301663, 301634]\n",
      "[301702, 301745, 301731, 301791, 301764, 301751, 301797, 301761, 301740, 301769]\n",
      "[301813, 301849, 301800, 301855, 301841, 301875, 301833, 301894, 301821, 301808]\n",
      "[301924, 301938, 301908, 301986, 301903, 301940, 301961, 301941, 301980, 301978]\n",
      "[302000, 302011, 302027, 302072, 302051, 302071, 302049, 302016, 302032, 302086]\n",
      "[302180, 302141, 302172, 302183, 302182, 302154, 302158, 302169, 302199, 302184]\n",
      "[302201, 302241, 302222, 302213, 302212, 302214, 302264, 302260, 302277, 302250]\n",
      "[302384, 302372, 302329, 302341, 302360, 302337, 302389, 302307, 302351, 302398]\n",
      "[302486, 302454, 302489, 302476, 302412, 302449, 302495, 302403, 302458, 302472]\n",
      "[302539, 302543, 302592, 302587, 302534, 302599, 302566, 302541, 302583, 302505]\n",
      "[302650, 302668, 302678, 302632, 302653, 302631, 302636, 302689, 302683, 302638]\n",
      "[302798, 302700, 302773, 302757, 302790, 302774, 302775, 302731, 302732, 302760]\n",
      "[302869, 302840, 302824, 302800, 302893, 302873, 302871, 302820, 302844, 302885]\n",
      "[302963, 302955, 302970, 302900, 302940, 302907, 302903, 302946, 302928, 302965]\n",
      "[303083, 303070, 303041, 303071, 303034, 303038, 303018, 303004, 303010, 303086]\n",
      "[303177, 303199, 303113, 303157, 303194, 303115, 303141, 303178, 303162, 303185]\n",
      "[303210, 303212, 303255, 303227, 303208, 303294, 303213, 303236, 303264, 303244]\n",
      "[303342, 303386, 303359, 303323, 303314, 303328, 303373, 303300, 303390, 303387]\n",
      "[303466, 303411, 303424, 303472, 303400, 303485, 303454, 303484, 303470, 303462]\n",
      "[303577, 303571, 303567, 303500, 303535, 303544, 303531, 303546, 303556, 303581]\n",
      "[303625, 303614, 303602, 303619, 303665, 303605, 303691, 303640, 303653, 303644]\n",
      "[303788, 303737, 303714, 303798, 303771, 303700, 303759, 303761, 303727, 303770]\n",
      "[303861, 303896, 303821, 303812, 303851, 303840, 303803, 303865, 303857, 303828]\n",
      "[303979, 303938, 303961, 303955, 303988, 303922, 303985, 303950, 303933, 303944]\n",
      "[304055, 304091, 304050, 304045, 304078, 304020, 304019, 304031, 304029, 304002]\n",
      "[304130, 304121, 304113, 304112, 304181, 304151, 304195, 304106, 304152, 304138]\n",
      "[304231, 304269, 304228, 304241, 304201, 304202, 304230, 304222, 304227, 304264]\n",
      "[304329, 304356, 304313, 304368, 304348, 304306, 304363, 304369, 304318, 304336]\n",
      "[304415, 304424, 304402, 304414, 304434, 304452, 304466, 304405, 304497, 304408]\n",
      "[304530, 304539, 304565, 304531, 304549, 304541, 304590, 304559, 304581, 304525]\n",
      "[304641, 304691, 304671, 304653, 304610, 304677, 304683, 304654, 304608, 304696]\n",
      "[304766, 304784, 304709, 304796, 304761, 304765, 304703, 304768, 304734, 304701]\n",
      "[304888, 304860, 304818, 304889, 304832, 304884, 304869, 304836, 304800, 304859]\n",
      "[304914, 304942, 304944, 304994, 304996, 304976, 304934, 304918, 304978, 304985]\n",
      "[305030, 305032, 305015, 305010, 305083, 305077, 305037, 305068, 305044, 305039]\n",
      "[305162, 305106, 305153, 305144, 305125, 305107, 305123, 305165, 305113, 305129]\n",
      "[305272, 305263, 305235, 305221, 305232, 305202, 305283, 305285, 305294, 305204]\n",
      "[305392, 305320, 305310, 305398, 305306, 305391, 305367, 305355, 305343, 305385]\n",
      "[305446, 305484, 305401, 305412, 305476, 305470, 305458, 305496, 305403, 305467]\n",
      "[305562, 305542, 305592, 305501, 305586, 305591, 305528, 305533, 305544, 305582]\n",
      "[305673, 305666, 305630, 305617, 305627, 305645, 305616, 305600, 305604, 305678]\n",
      "[305703, 305792, 305760, 305719, 305790, 305771, 305775, 305770, 305795, 305705]\n",
      "[305875, 305845, 305897, 305899, 305825, 305803, 305873, 305853, 305844, 305884]\n",
      "[305965, 305908, 305919, 305924, 305944, 305964, 305972, 305959, 305912, 305936]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "feature_number = 1024\n",
    "\n",
    "train_list_before=[]\n",
    "train_label_list=[]\n",
    "test_list_before=[]\n",
    "test_label_list =[]\n",
    "\n",
    "offset = 6000*50\n",
    "\n",
    "for i in range(60):\n",
    "    index = np.arange(100)     #generate numbers from 0 to 100\n",
    "    np.random.shuffle(index)        #shuffle the 100 values\n",
    "    index = [int(100*i+j + offset) for j in index]\n",
    "    print(index[10:20])\n",
    "    train_list_before.extend([conceptPairList[b] for b in index[0:40]]) \n",
    "    train_label_list.extend([1]*40)\n",
    "    train_list_before.extend([conceptNotPairList[b] for b in index[40:80]])\n",
    "    train_label_list.extend([0]*40)\n",
    "    test_list_before.extend([conceptPairList[b] for b in index[80:90]])\n",
    "    test_label_list.extend([1]*10)\n",
    "    test_list_before.extend([conceptNotPairList[b] for b in index[90:100]])\n",
    "    test_label_list.extend([0]*10)\n",
    "\n",
    "\n",
    "    \n",
    "train_list =[]\n",
    "test_list = []\n",
    "\n",
    "for line in train_list_before:\n",
    "    if line[0] in model.docvecs and line[1] in model.docvecs:\n",
    "        a= model.docvecs[line[0]]\n",
    "        b= model.docvecs[line[1]]\n",
    "        c = np.array((a, b))\n",
    "#         train_list.append(np.reshape(c, feature_number)) \n",
    "        train_list.append(np.reshape(c, feature_number, order='F'))\n",
    "\n",
    "\n",
    "test_list_ids={}\n",
    "for i, line in enumerate(test_list_before):\n",
    "    if line[0] in model.docvecs and line[1] in model.docvecs:\n",
    "        a= model.docvecs[line[0]]\n",
    "        b= model.docvecs[line[1]]\n",
    "        c = np.array((a, b))\n",
    "        test_list_ids[i] = (line[0], line[1])\n",
    "#         test_list.append(np.reshape(c, feature_number))\n",
    "        test_list.append(np.reshape(c, feature_number, order='F'))\n",
    "\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.05526982,  0.03595751,  0.04793205, ...,  0.01763459,\n",
      "       -0.06277778, -0.11440518], dtype=float32), array([ 0.05908632,  0.11953794, -0.05293976, ...,  0.1858876 ,\n",
      "       -0.10361813, -0.14412729], dtype=float32), array([ 0.04241558, -0.01488476,  0.08403853, ...,  0.41364488,\n",
      "        0.04100596,  0.08464405], dtype=float32), array([ 0.12759799,  0.07505289,  0.02259964, ..., -0.00908431,\n",
      "        0.07731086, -0.01629093], dtype=float32), array([ 0.05908632,  0.04456509, -0.05293976, ...,  0.11063285,\n",
      "       -0.10361813, -0.11765458], dtype=float32), array([-0.05463852,  0.03970724, -0.05088031, ...,  0.24226899,\n",
      "       -0.100255  , -0.10292189], dtype=float32), array([ 0.11786103,  0.1882166 , -0.00029253, ..., -0.01044459,\n",
      "       -0.04936048, -0.04468998], dtype=float32), array([ 0.12795576,  0.15066677, -0.02919042, ...,  0.08682617,\n",
      "       -0.07839128, -0.07007205], dtype=float32), array([-0.12717773, -0.08106725, -0.01984232, ...,  0.17390174,\n",
      "       -0.23808259, -0.29141706], dtype=float32), array([ 0.10560421,  0.11295617,  0.04386383, ...,  0.06855298,\n",
      "       -0.08047203, -0.03852015], dtype=float32), array([ 0.18627375,  0.03437665, -0.459795  , ...,  0.19556142,\n",
      "       -0.2141725 , -0.00371151], dtype=float32), array([ 0.12305943,  0.03437665, -0.46177819, ...,  0.19556142,\n",
      "        0.05952481, -0.00371151], dtype=float32), array([ 0.27192256,  0.03437665, -0.38403204, ...,  0.19556142,\n",
      "        0.09365594, -0.00371151], dtype=float32), array([ 0.10953753,  0.03437665, -0.33292603, ...,  0.19556142,\n",
      "        0.01275476, -0.00371151], dtype=float32), array([-0.20941824,  0.03437665, -0.33341241, ...,  0.19556142,\n",
      "        0.04098545, -0.00371151], dtype=float32), array([ 0.02957128,  0.03437665, -0.36633679, ...,  0.19556142,\n",
      "        0.12623748, -0.00371151], dtype=float32), array([ 0.14789814,  0.03437665, -0.24077888, ...,  0.19556142,\n",
      "        0.17523043, -0.00371151], dtype=float32), array([ 0.27724564,  0.03437665, -0.48958808, ...,  0.19556142,\n",
      "        0.23475398, -0.00371151], dtype=float32), array([ 0.11037967,  0.03437665, -0.37268332, ...,  0.19556142,\n",
      "        0.07562885, -0.00371151], dtype=float32), array([ 0.14240399,  0.03437665, -0.49709123, ...,  0.19556142,\n",
      "        0.14704947, -0.00371151], dtype=float32)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4800\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "print(train_list[30:50])\n",
    "print(train_label_list[30:50])\n",
    "print(len(train_list))\n",
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "[0.96937499999999999]\n"
     ]
    }
   ],
   "source": [
    "#SVM \n",
    "\n",
    "\n",
    "print(len(train_list))\n",
    "\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(train_list, train_label_list)\n",
    "\n",
    "train_errors=[]\n",
    "train_errors.append(clf.score(train_list, train_label_list))\n",
    "print(train_errors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptLabelDict={}\n",
    "errors=[]\n",
    "\n",
    "def read_label(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptLabelDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "label_file = \"/home/hao/AnacondaProjects/MLOntology/ontClassLabels.txt\"\n",
    "read_label(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 44 predicted label [0], but true label is 1\n",
      "('394583002', '394733009') Concept Pairs: (endocrinology --- medical specialty)\n",
      "index 86 predicted label [0], but true label is 1\n",
      "('109001000119105', '281666001') Concept Pairs: (family history of protein c deficiency --- family history of disorder)\n",
      "index 141 predicted label [0], but true label is 1\n",
      "('170706008', '243857008') Concept Pairs: (epilepsy associated problems --- epilepsy monitoring status)\n",
      "index 153 predicted label [1], but true label is 0\n",
      "('38720006', '199306007') Concept Pairs: (septuplet pregnancy --- continuing pregnancy after abortion of one fetus or more)\n",
      "index 154 predicted label [1], but true label is 0\n",
      "('62098001', '249065002') Concept Pairs: (head not engaged --- oblique lie, breech in iliac fossa)\n",
      "index 167 predicted label [0], but true label is 1\n",
      "('306503001', '306494002') Concept Pairs: (discharge by nurse psychotherapist --- discharge by clinical nurse specialist)\n",
      "index 169 predicted label [0], but true label is 1\n",
      "('39112005', '78548001') Concept Pairs: (glutathione synthase deficiency with 5-oxoprolinuria --- enzymopathy)\n",
      "index 272 predicted label [1], but true label is 0\n",
      "('33627001', '199863008') Concept Pairs: (prolonged first stage of labor --- delayed delivery second twin with antenatal problem)\n",
      "index 278 predicted label [1], but true label is 0\n",
      "('77259008', '199863008') Concept Pairs: (prolonged second stage of labor --- delayed delivery second twin with antenatal problem)\n",
      "index 306 predicted label [0], but true label is 1\n",
      "('281330008', '281323002') Concept Pairs: (no test request given --- insufficient requesting detail)\n",
      "index 327 predicted label [0], but true label is 1\n",
      "('305447002', '305444009') Concept Pairs: (under care of homeopath --- under care of complementary therapist)\n",
      "index 350 predicted label [1], but true label is 0\n",
      "('229719002', '90662007') Concept Pairs: (phonological disorder --- dysprosody of 'pseudoforeign dialect')\n",
      "index 425 predicted label [0], but true label is 1\n",
      "('313313009', '129125009') Concept Pairs: (diaphragm lesion excised --- procedure with explicit context)\n",
      "index 437 predicted label [1], but true label is 0\n",
      "('42810003', '76441001') Concept Pairs: (major depression in remission --- severe major depression, single episode, without psychotic features)\n",
      "index 454 predicted label [1], but true label is 0\n",
      "('48022009', '250327000') Concept Pairs: (abnormal lymphocyte destruction --- blood coagulation pathway finding)\n",
      "index 470 predicted label [1], but true label is 0\n",
      "('198439001', '31351009') Concept Pairs: (menopausal concentration lack --- postartificial menopausal syndrome)\n",
      "index 471 predicted label [1], but true label is 0\n",
      "('267436001', '3744001') Concept Pairs: (lipoprotein deficiency disorder --- hyperlipoproteinemia)\n",
      "index 472 predicted label [1], but true label is 0\n",
      "('231537008', '47916000') Concept Pairs: (developmental agnosia --- developmental arithmetic disorder)\n",
      "index 473 predicted label [1], but true label is 0\n",
      "('21639008', '212971003') Concept Pairs: (hypervolemia --- deprivation of water)\n",
      "index 475 predicted label [1], but true label is 0\n",
      "('21630007', '124692001') Concept Pairs: (glutathione s-transferase deficiency --- deficiency of isoleucine-transfer ribonucleic acid ligase)\n",
      "index 476 predicted label [1], but true label is 0\n",
      "('287266002', '124692001') Concept Pairs: (muscle d-lactate dehydrogenase deficiency --- deficiency of isoleucine-transfer ribonucleic acid ligase)\n",
      "index 492 predicted label [1], but true label is 0\n",
      "('216840008', '214511003') Concept Pairs: (accidental poisoning by exhaust gas from stationary motor vehicle --- noncollision motor vehicle traffic accident involving accidental poisoning from exhaust gas generated by motor vehicle while in motion, pedal cyclist injured)\n",
      "index 517 predicted label [1], but true label is 0\n",
      "('439698008', '442121006') Concept Pairs: (hereditary thrombophilia --- thrombophilia due to vascular anomaly)\n",
      "index 526 predicted label [0], but true label is 1\n",
      "('143691000119102', '416471007') Concept Pairs: (family history of breast cancer gene mutation in first degree relative --- family history of clinical finding)\n",
      "index 583 predicted label [0], but true label is 1\n",
      "('306497009', '306494002') Concept Pairs: (discharge by contact tracing nurse --- discharge by clinical nurse specialist)\n",
      "index 603 predicted label [0], but true label is 1\n",
      "('390906007', '308335008') Concept Pairs: (follow-up encounter --- patient encounter procedure)\n",
      "index 635 predicted label [1], but true label is 0\n",
      "('205418005', '57484009') Concept Pairs: (goldenhar syndrome --- behcet's syndrome, complete type)\n",
      "index 637 predicted label [1], but true label is 0\n",
      "('254150007', '57484009') Concept Pairs: (francois syndrome --- behcet's syndrome, complete type)\n",
      "index 783 predicted label [0], but true label is 1\n",
      "('63234004', '365574009') Concept Pairs: (divorce, life event --- finding of life event)\n",
      "index 801 predicted label [0], but true label is 1\n",
      "('29679002', '105724001') Concept Pairs: (carrier of disorder --- disease related state)\n",
      "index 837 predicted label [1], but true label is 0\n",
      "('41267003', '705154007') Concept Pairs: (graft failure due to stenosis --- acute vascular graft rejection)\n",
      "index 848 predicted label [0], but true label is 1\n",
      "('26533002', '365508006') Concept Pairs: (apartment building living --- finding of residence and accommodation circumstances)\n",
      "index 850 predicted label [1], but true label is 0\n",
      "('216763006', '216821002') Concept Pairs: (accidental poisoning from shellfish --- accidental poisoning by liquefied petroleum gas distributed through pipes)\n",
      "index 851 predicted label [1], but true label is 0\n",
      "('216725003', '216821002') Concept Pairs: (accidental poisoning by mixtures of herbicides with plant foods and fertilizers --- accidental poisoning by liquefied petroleum gas distributed through pipes)\n",
      "index 853 predicted label [1], but true label is 0\n",
      "('442784009', '216821002') Concept Pairs: (accidental poisoning caused by gaseous substance --- accidental poisoning by liquefied petroleum gas distributed through pipes)\n",
      "index 872 predicted label [1], but true label is 0\n",
      "('36262007', '48309007') Concept Pairs: (pygmalionism --- pedophilia, nonexclusive type)\n",
      "index 876 predicted label [1], but true label is 0\n",
      "('724744002', '48309007') Concept Pairs: (paraphilia involving non-consenting individual --- pedophilia, nonexclusive type)\n",
      "index 880 predicted label [0], but true label is 1\n",
      "('281333005', '281323002') Concept Pairs: (no contact number given --- insufficient requesting detail)\n",
      "index 892 predicted label [1], but true label is 0\n",
      "('194439006', '31491000119109') Concept Pairs: (disorders of excessive somnolence --- disruptions of 24 hour sleep-wake cycle)\n",
      "index 894 predicted label [1], but true label is 0\n",
      "('271794005', '724749007') Concept Pairs: (disorder of sleep-wake cycle --- behaviorally induced hypersomnia)\n",
      "index 1019 predicted label [1], but true label is 0\n",
      "('429530004', '103294007') Concept Pairs: (dizziness of unknown cause --- vertigo produced by neck pressure)\n",
      "index 1039 predicted label [1], but true label is 0\n",
      "('15616006', '343570008') Concept Pairs: (disorder of trace mineral metabolism --- latent hemochromatosis)\n",
      "index 1088 predicted label [0], but true label is 1\n",
      "('182774007', '309370004') Concept Pairs: (consent to donate organs given --- consent status)\n",
      "index 1148 predicted label [0], but true label is 1\n",
      "('702860000', '257585005') Concept Pairs: (general surgery clinic --- clinic)\n",
      "index 1169 predicted label [0], but true label is 1\n",
      "('106227002', '362981000') Concept Pairs: (general information qualifier --- qualifier value)\n",
      "index 1179 predicted label [1], but true label is 0\n",
      "('366719003', '250064001') Concept Pairs: (heel-shin test finding --- finger-finger test abnormal)\n",
      "index 1183 predicted label [0], but true label is 1\n",
      "('185397002', '185391001') Concept Pairs: (patient comes 6 monthly --- frequency of encounter)\n",
      "index 1186 predicted label [0], but true label is 1\n",
      "('109897000', '109896009') Concept Pairs: (indication for modification of patient behavior status --- indication for modification of patient status)\n"
     ]
    }
   ],
   "source": [
    "for i, (item, label) in enumerate(zip(test_list, test_label_list)):\n",
    "    result = clf.predict([item])\n",
    "    if result != label:\n",
    "        print(\"index %d predicted label %s, but true label is %s\" % (i, result, label))\n",
    "        idpair = test_list_ids[i] \n",
    "        concept1 = conceptLabelDict[idpair[0]]\n",
    "        concept2 = conceptLabelDict[idpair[1]]\n",
    "        print(\"%s Concept Pairs: (%s --- %s)\" % (idpair, concept1, concept2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "1200\n",
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# m3 = np.array((a,d))\n",
    "# m3 = np.reshape(m3, 400, order='F')\n",
    "print(len(test_list))\n",
    "result = clf.predict(test_list)\n",
    "\n",
    "print(result.size)\n",
    "\n",
    "\n",
    "print(result[:29])\n",
    "print(np.array(test_label_list[:29]))\n",
    "\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  44   86  141  153  154  167  169  272  278  306  327  350  425  437  454\n",
      "  470  471  472  473  475  476  492  517  526  583  603  635  637  783  801\n",
      "  837  848  850  851  853  872  876  880  892  894 1019 1039 1088 1148 1169\n",
      " 1179 1183 1186]\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "err_ids=np.flatnonzero(result != test_label_list)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "0.94548245614\n",
      "0.959998222143\n",
      "0.96\n",
      "0.960001777857\n",
      "[ 0.95973154  0.9602649 ]\n",
      "[ 0.95333333  0.96666667]\n",
      "[ 0.96621622  0.95394737]\n",
      "0.960081792319\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(accuracy_score(result, test_label_list))\n",
    "print(average_precision_score(result, test_label_list))\n",
    "\n",
    "print(f1_score(result, test_label_list, average='macro') ) \n",
    "\n",
    "print(f1_score(result, test_label_list, average='micro')  )\n",
    "\n",
    "print(f1_score(result, test_label_list, average='weighted') )\n",
    "\n",
    "print(f1_score(result, test_label_list, average=None))\n",
    "\n",
    "print(precision_score(result, test_label_list, average=None))\n",
    "print(recall_score(result, test_label_list, average=None))\n",
    "\n",
    "print(roc_auc_score(result, test_label_list, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label_list_m = np.eye(2)[train_label_list]\n",
    "# test_label_list_m = np.eye(2)[test_label_list]\n",
    "# print(test_label_list_m[10:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 100\n",
      "Train Loss: 0.636943 Acc: [0.63]\n",
      "None 200\n",
      "Train Loss: 0.563843 Acc: [0.82999998]\n",
      "None 300\n",
      "Train Loss: 0.500321 Acc: [0.75999999]\n",
      "None 400\n",
      "Train Loss: 0.399372 Acc: [0.86000001]\n",
      "None 500\n",
      "Train Loss: 0.266366 Acc: [0.92000002]\n",
      "None 600\n",
      "Train Loss: 0.184362 Acc: [0.94]\n",
      "None 700\n",
      "Train Loss: 0.179474 Acc: [0.94]\n",
      "None 800\n",
      "Train Loss: 0.127734 Acc: [0.99000001]\n",
      "None 900\n",
      "Train Loss: 0.0902365 Acc: [0.97000003]\n",
      "None 1000\n",
      "Train Loss: 0.074926 Acc: [0.99000001]\n",
      "None 1100\n",
      "Train Loss: 0.0574856 Acc: [0.99000001]\n",
      "None 1200\n",
      "Train Loss: 0.0568659 Acc: [0.99000001]\n",
      "None 1300\n",
      "Train Loss: 0.0534269 Acc: [0.99000001]\n",
      "None 1400\n",
      "Train Loss: 0.0429387 Acc: [0.99000001]\n",
      "0.981667\n",
      "Model saved in file: ./model-noleaky.ckpt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "In the data, there are 2 classes and every class has 3000 samples and every sample has 512 features\n",
    "The first 3000 samples are from class 0, second 3000 are from class 1\n",
    "'''\n",
    "# DATA_DIR = ''\n",
    "CLASS_NUM = 2       #there are 2 classes\n",
    "SPLIT_PERCENT = 0.8     #split the data into 80% for training and 20% for testing\n",
    "FEATURE_NUM = 1024   \n",
    "TRAIN_ITER = 1500    #the number of iterations for training\n",
    "display_step = 100        #how many iterations to display the results\n",
    "\n",
    "\n",
    "train_num = int(3000*SPLIT_PERCENT)     #the number of samples for training\n",
    "\n",
    "train_feature = train_list      #training features (list of list)\n",
    "train_y = train_label_list        #training lables    (list)\n",
    "test_feature = test_list       #test features  (list of list)\n",
    "test_y = test_label_list         #test labels    (list)\n",
    "\n",
    "\n",
    "y_m = np.eye(2)[train_y]\n",
    "test_y_m = np.eye(2)[test_y]\n",
    "\n",
    "'''\n",
    "y = wx+b        (vectors)\n",
    "'''\n",
    "#function to get variables 'w'\n",
    "def weight_variable(shape, num):\n",
    "    initial = tf.truncated_normal(shape, stddev=1/num)\n",
    "    return tf.Variable(initial, name='weight')\n",
    "\n",
    "#the bias 'b' in the equations\n",
    "def bias_variable(shape, num):\n",
    "    initial = tf.constant(0.0001, shape=shape)\n",
    "    return tf.Variable(initial, name='bias')\n",
    "\n",
    "#convolutional process\n",
    "def conv1d(x, W):\n",
    "    return tf.nn.conv1d(x, W, stride=1, padding='SAME')     #x: variable, w: weight, stride and padding (padding can be ignored currently) \n",
    "\n",
    "#pooling process\n",
    "def max_pool_1x1(x, shape):\n",
    "    x=tf.reshape(x,shape)       #it is transfered into four dimensions, but the other three are 1\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 4, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "'''\n",
    "The feature is 3 dimensional data.  [batch, length, channel] \n",
    "batch is usually ignored (for example there are 100 samples in a batch, so samples should not be modified mutually), length and channel are shown in the paper.\n",
    "At first, the length is 512, and channel is 1.\n",
    "Because our data are time series data, so length is enough, but for images, it may be [batch, length, width, channel]\n",
    "'''\n",
    "# the convolutional layer\n",
    "def layer(features, f, input_n, channel, hidden_units, layer_index):\n",
    "    \"\"\"Construct a convolutional layer\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    f: the length\n",
    "    input_n: Size of the features used in the convention.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    layer_index: the index of layer\n",
    "    Returns:\n",
    "    hidden units: The unit output for the next layer.\n",
    "    weights: the weights in the current hidden layer\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden'+str(layer_index)) as scope:     # name scope may be ignored first\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, channel, hidden_units], math.sqrt(f))\n",
    "\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(f))\n",
    "    hidden = relu(conv1d(features, weights) + biases, 0.01)\n",
    "    shape = [-1,1,f,hidden_units]\n",
    "    h_pool1 = max_pool_1x1(hidden,shape)\n",
    "    return h_pool1, weights\n",
    "\n",
    "# fully connected layer, here the data are two dimension, [batch, length]\n",
    "def densely_connect(features, input_n, hidden_units):\n",
    "    \"\"\"Construct a fully (densely) connected layer.\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    input_n: Size of units in the previous layer.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    Returns:\n",
    "    logits: The estimated output in last layer.\n",
    "    weights: the weights in the hidden layer\n",
    "    \"\"\"\n",
    "    with tf.name_scope('softmax_linear') as dense:\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    logits = relu(tf.matmul(features, weights) + biases, 0.01)      # the matrix product operation\n",
    "    return logits, weights\n",
    "\n",
    "# dropout layer (it is not necessary)\n",
    "# randomly set (1-keep_prob) percentage of units to be zero\n",
    "def dropout(features, input_n, hidden_units, keep_prob):\n",
    "    with tf.name_scope('dropout'):\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    h_fc1_drop = tf.nn.dropout(features, keep_prob)\n",
    "    drop_out = relu(tf.matmul(features, weights) + biases, 0.01)\n",
    "    return drop_out\n",
    "\n",
    "# calculate the loss in the neural network\n",
    "def loss(logits, labels):\n",
    "    \"\"\"Calculates the loss from the logits and the labels.\n",
    "    Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size, NUM_CLASSES].\n",
    "    Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        labels = tf.to_int64(labels)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=labels, logits=logits, name='xentropy')\n",
    "    # tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "def next_batch(data, label, num):\n",
    "    \"\"\"Generate the next batch randomly\n",
    "    Args:\n",
    "    data: training data.\n",
    "    label: training label.\n",
    "    num: the size in a batch\n",
    "    Returns:\n",
    "    next batch's training features and labels.\n",
    "    \"\"\"\n",
    "    index = np.arange(len(data))\n",
    "    np.random.shuffle(index)\n",
    "#     train_feature = data[np.array(index)[0:num]]\n",
    "#     train_label = label[np.array(index)[0:num]]\n",
    "    train_feature_batch = [data[b] for b in index[0:num]]\n",
    "    train_feature_batch = np.asarray(train_feature_batch)\n",
    "    train_label_batch = [label[b] for b in index[0:num]]\n",
    "    train_label_batch = np.asarray(train_label_batch)\n",
    "    return train_feature_batch, train_label_batch\n",
    "\n",
    "def relu(x, alpha=0., max_value=None):\n",
    "    '''ReLU.\n",
    "    alpha: slope of negative section.\n",
    "    '''\n",
    "    negative_part = tf.nn.relu(-x)\n",
    "    x = tf.nn.relu(x)\n",
    "    if max_value is not None:\n",
    "        x = tf.clip_by_value(x, tf.cast(0., dtype=tf.float32),\n",
    "                             tf.cast(max_value, dtype=tf.float32))\n",
    "    x -= tf.constant(alpha, dtype=tf.float32) * negative_part\n",
    "    return x\n",
    "\n",
    "#define a session to run the model\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#place holders for training features and label\n",
    "#None means the value is variable\n",
    "x = tf.placeholder(tf.float32, shape=[None, FEATURE_NUM])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, CLASS_NUM])\n",
    "\n",
    "# decide whether it is training or testing, it is not used in our model, but it may be used\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "#from [-1, 512, 1] -> [-1, 256, 32] -> [-1, 128, 64] -> [-1, 64, 64] -> [-1, 32, 64] -> [-1, 16, 64] -> [-1, 8, 64] -> [-1, 200]\n",
    "\n",
    "#6 hidden layers\n",
    "x_1 = tf.reshape(x, [-1,FEATURE_NUM,1])\n",
    "h_pool0, w0 = layer(x_1, FEATURE_NUM, 15, 1, 32, 0)\n",
    "h_pool0 = tf.reshape(h_pool0, [-1,512,32])\n",
    "h_pool1, w1 = layer(h_pool0, 512, 10, 32, 64, 1)\n",
    "\n",
    "h_pool1 = tf.reshape(h_pool1, [-1,256,64])\n",
    "h_pool2, w2 = layer(h_pool1, 256, 10, 64, 64, 2)\n",
    "h_pool2 = tf.reshape(h_pool2, [-1,128,64])\n",
    "h_pool3, w3 = layer(h_pool2, 128, 10, 64, 64, 3)\n",
    "h_pool3 = tf.reshape(h_pool3, [-1,64,64])\n",
    "h_pool4, w4 = layer(h_pool3, 64, 5, 64, 64, 4)\n",
    "h_pool4 = tf.reshape(h_pool4, [-1,32,64])\n",
    "h_pool5, w5 = layer(h_pool4, 32, 5, 64, 64, 5)\n",
    "h_pool5 = tf.reshape(h_pool5, [-1,16,64])\n",
    "h_pool6, w6 = layer(h_pool5, 16, 5, 64, 64, 6)\n",
    "h_pool6 = tf.reshape(h_pool6, [-1,8,64])\n",
    "\n",
    "#densely connected: 200 units\n",
    "h_pool_flat = tf.reshape(h_pool6, [-1, 8*64])\n",
    "h_dc, w_d = densely_connect(h_pool_flat, 8*64, 200)\n",
    "\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y_conv=dropout(h_dc, (int)(h_dc.get_shape()[1]), CLASS_NUM, keep_prob)\n",
    "\n",
    "\n",
    "beta = 0.001\n",
    "cross_entropy = loss(y_conv, y_)\n",
    "loss = cross_entropy +beta*(tf.nn.l2_loss(w0) + tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2)+tf.nn.l2_loss(w3)+tf.nn.l2_loss(w4)+tf.nn.l2_loss(w5)+tf.nn.l2_loss(w6)+tf.nn.l2_loss(w_d))  #L2 regularization\n",
    "epsilon = 1e-5      # learning rate\n",
    "train_step = tf.train.AdamOptimizer(epsilon).minimize(loss)     #optimization function, our goal is to minimize the loss\n",
    "\n",
    "predict = tf.argmax(y_conv,1)   #the predicted class\n",
    "\n",
    "# calculate the accuray, the corrected classified divided by the total size\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "#saver to save the training check point\n",
    "# variables can be restored in a new model by 'saver.restore(sess, save_path)'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())  #initialize the variables\n",
    "\n",
    "\n",
    "for i in range(1,TRAIN_ITER):       #training iterations\n",
    "    d, l = next_batch(train_feature, y_m, 100)      # get 100 samples in one batch\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d), len(l)))\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d[0]), len(l[0])))\n",
    "    _, ls=sess.run([train_step,cross_entropy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:True})     #run the train step (optimization function), the second one is just to show the loss in this iteration.   THE FEED dictionary is to feed the place holders which are needed in the optimization function.\n",
    "    \n",
    "    if i%display_step==0:\n",
    "        print(_, i)\n",
    "        acc = sess.run([accuracy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:False})\n",
    "        print(\"Train Loss:\", ls, \"Acc:\", acc)\n",
    "\n",
    "# sess.run  or tensor.eval are two ways\n",
    "# get the accuracy in the testing data\n",
    "print(accuracy.eval(session=sess, feed_dict={x:test_feature, y_:test_y_m, keep_prob: 1, is_training:False}))\n",
    "\n",
    "\n",
    "# save the model results\n",
    "save_path = saver.save(sess, \"./model-noleaky.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[ 101  153  225  409  425  466  492  630  636  639  683  733  764  801  892\n",
      "  920  969 1019 1090 1163 1169 1182]\n",
      "22\n",
      "index 101 predicted label 0, but true label is 1\n",
      "('167036008', '430925007') Concept Pairs: (bone profile --- measurement of substance)\n",
      "index 153 predicted label 1, but true label is 0\n",
      "('38720006', '199306007') Concept Pairs: (septuplet pregnancy --- continuing pregnancy after abortion of one fetus or more)\n",
      "index 225 predicted label 0, but true label is 1\n",
      "('708115005', '49062001') Concept Pairs: (device component --- device)\n",
      "index 409 predicted label 0, but true label is 1\n",
      "('447007003', '364644000') Concept Pairs: (jamar hydraulic hand dynamometer score --- functional observable)\n",
      "index 425 predicted label 0, but true label is 1\n",
      "('313313009', '129125009') Concept Pairs: (diaphragm lesion excised --- procedure with explicit context)\n",
      "index 466 predicted label 0, but true label is 1\n",
      "('467980008', '303634002') Concept Pairs: (corneal resection holder --- ophthalmological device)\n",
      "index 492 predicted label 1, but true label is 0\n",
      "('216840008', '214511003') Concept Pairs: (accidental poisoning by exhaust gas from stationary motor vehicle --- noncollision motor vehicle traffic accident involving accidental poisoning from exhaust gas generated by motor vehicle while in motion, pedal cyclist injured)\n",
      "index 630 predicted label 1, but true label is 0\n",
      "('307779006', '185232008') Concept Pairs: (seen in supervised accommodation --- seen in psychiatry clinic)\n",
      "index 636 predicted label 1, but true label is 0\n",
      "('270420001', '185232008') Concept Pairs: (seen in own home --- seen in psychiatry clinic)\n",
      "index 639 predicted label 1, but true label is 0\n",
      "('185209009', '185232008') Concept Pairs: (seen in recreation place --- seen in psychiatry clinic)\n",
      "index 683 predicted label 0, but true label is 1\n",
      "('2415007', '239953001') Concept Pairs: (lumbosacral radiculopathy --- soft tissue lesion)\n",
      "index 733 predicted label 1, but true label is 0\n",
      "('77402005', '46556004') Concept Pairs: (disorder of lysine and/or hydroxylysine metabolism --- aminoacidemia)\n",
      "index 764 predicted label 0, but true label is 1\n",
      "('444908001', '275823006') Concept Pairs: (isolation nursing in negative pressure isolation environment --- isolation nursing)\n",
      "index 801 predicted label 0, but true label is 1\n",
      "('29679002', '105724001') Concept Pairs: (carrier of disorder --- disease related state)\n",
      "index 892 predicted label 1, but true label is 0\n",
      "('194439006', '31491000119109') Concept Pairs: (disorders of excessive somnolence --- disruptions of 24 hour sleep-wake cycle)\n",
      "index 920 predicted label 0, but true label is 1\n",
      "('231003007', '42839003') Concept Pairs: (repair of spinal myelomeningocele using distant flap --- repair of myelomeningocele)\n",
      "index 969 predicted label 0, but true label is 1\n",
      "('721745002', '307425001') Concept Pairs: (childhood cellulitis of perianal region caused by beta-hemolytic streptococcus group a --- perianal cellulitis)\n",
      "index 1019 predicted label 1, but true label is 0\n",
      "('429530004', '103294007') Concept Pairs: (dizziness of unknown cause --- vertigo produced by neck pressure)\n",
      "index 1090 predicted label 1, but true label is 0\n",
      "('273700000', '360921008') Concept Pairs: (hyper-beta-carnosinemia --- deficiency of peptidase a)\n",
      "index 1163 predicted label 0, but true label is 1\n",
      "('365754004', '118245000') Concept Pairs: (finding of presence of poison --- measurement finding)\n",
      "index 1169 predicted label 0, but true label is 1\n",
      "('106227002', '362981000') Concept Pairs: (general information qualifier --- qualifier value)\n",
      "index 1182 predicted label 0, but true label is 1\n",
      "('281867008', '64572001') Concept Pairs: (multisystem disorder --- disease)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = sess.run(predict, feed_dict={x:test_feature, keep_prob:1, is_training:False})\n",
    "print(y_pred[:20])\n",
    "print(test_y[:20])\n",
    "\n",
    "\n",
    "err_ids=np.flatnonzero(y_pred != test_y)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)\n",
    "for err_id in err_ids:\n",
    "    print(\"index %d predicted label %s, but true label is %s\" % (err_id, y_pred[err_id], test_y[err_id]))\n",
    "    idpair = test_list_ids[err_id] \n",
    "    concept1 = conceptLabelDict[idpair[0]]\n",
    "    concept2 = conceptLabelDict[idpair[1]]\n",
    "    print(\"%s Concept Pairs: (%s --- %s)\" % (idpair, concept1, concept2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981666666667\n",
      "0.9710598434\n",
      "0.981666462961\n",
      "0.981666666667\n",
      "0.981666870373\n",
      "[ 0.98172757  0.98160535]\n",
      "[ 0.985       0.97833333]\n",
      "[ 0.97847682  0.98489933]\n",
      "0.981688075026\n"
     ]
    }
   ],
   "source": [
    "result = y_pred\n",
    "test_label_list = test_y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(accuracy_score(result, test_label_list))\n",
    "print(average_precision_score(result, test_label_list))\n",
    "\n",
    "print(f1_score(result, test_label_list, average='macro') ) \n",
    "\n",
    "print(f1_score(result, test_label_list, average='micro')  )\n",
    "\n",
    "print(f1_score(result, test_label_list, average='weighted') )\n",
    "\n",
    "print(f1_score(result, test_label_list, average=None))\n",
    "\n",
    "print(precision_score(result, test_label_list, average=None))\n",
    "print(recall_score(result, test_label_list, average=None))\n",
    "\n",
    "print(roc_auc_score(result, test_label_list, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "err_ids=np.flatnonzero(result != test_label_list)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)\n",
    "for err_id in err_ids:\n",
    "    idpair = test_list_ids[err_id] \n",
    "    print(idpair)\n",
    "    concept1 = conceptLabelDict[idpair[0]]\n",
    "    concept2 = conceptLabelDict[idpair[1]]\n",
    "    print(\"(Concept 1 %s ---- Concept 2 %s)\" % (concept1, concept2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = [[1,2,3], [3,4,5], [3,4,5], [5,6,7],[2,3,7]]\n",
    "\n",
    "plist.extend([1]*4)\n",
    "print(plist)\n",
    "\n",
    "index = np.arange(5)\n",
    "\n",
    "c = [plist[b] for b in index[:2]]\n",
    "print(c)\n",
    "c.extend([plist[b] for b in index[2:]])\n",
    "print(c)\n",
    "\n",
    "for i in range(3):      # i is the class index, for example, i==0 for class 0, i==1 for class 1 ...\n",
    "    index = np.arange(30)     #generate numbers from 0 to 2999\n",
    "    np.random.shuffle(index)        #shuffle the 3000 values\n",
    "    index = [int(300*i+j) for j in index]\n",
    "    print(index)\n",
    "    print(np.array(index)[0:5])\n",
    "    print(np.array(index)[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([1,2,3])\n",
    "y=np.append(y, [1]*4)\n",
    "y= np.append(y, [0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/MLOntology/model1\"\n",
    "\n",
    "print(path)\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(inferred_vector)\n",
    "print(inferred_vector.size)\n",
    "# pprint(model.docvecs.most_similar([inferred_vector], topn=20))\n",
    "\n",
    "\n",
    "path = \"D:/MLOntology/model0\"\n",
    "\n",
    "print(path)\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(inferred_vector)\n",
    "print(inferred_vector.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs['SENT_5690']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "print(X)\n",
    "print(y)\n",
    "train_errors=[]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)  \n",
    "\n",
    "train_errors.append(clf.score(X, y))\n",
    "print(train_errors)\n",
    "X_test=[[2,2]]\n",
    "y_test = [1]\n",
    "test_errors=[]\n",
    "clf.predict(X_test)\n",
    "\n",
    "test_errors.append(clf.score(X_test, y_test))\n",
    "print(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "a= model.docvecs[0]\n",
    "b= model.docvecs[1]\n",
    "m1 = np.array((a, b))\n",
    "\n",
    "# print(np.reshape(m1, 1024))\n",
    "# print(np.reshape(m1, 400, order='F')) # two ways of reshape\n",
    "\n",
    "c= model.docvecs[2]\n",
    "d= model.docvecs[3]\n",
    "print(c.shape)\n",
    "print(d.shape)\n",
    "print(c.shape[0]+d.shape[0])\n",
    "m2 = np.array((c, d))\n",
    "\n",
    "m1 = np.reshape(m1, 1024)\n",
    "m2 = np.reshape(m2, 1024)\n",
    "# m1 = np.reshape(m1, 1024, order='F')\n",
    "# m2 = np.reshape(m2, 1024, order='F')\n",
    "\n",
    "print(m1)\n",
    "\n",
    "X = [m1, m2]\n",
    "print(X)\n",
    "\n",
    "XX = np.append(m1, m2)\n",
    "print(XX)\n",
    "\n",
    "y = [0, 1]\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X, y)\n",
    "\n",
    "m3 = np.array((a,d))\n",
    "m3 = np.reshape(m3, 1024, order='F')\n",
    "\n",
    "result = clf.predict([m3])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=  np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "m1 = np.array((a, b))\n",
    "print(m1)\n",
    "\n",
    "m2 = np.vstack((a, b)).T\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDict={}\n",
    "testDict[0] = (\"a\", \"b\")\n",
    "print(testDict[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,1,0,1,0,1,0])\n",
    "b = np.array([0,1,0,1,0,1,1])\n",
    "\n",
    "np.flatnonzero(a!=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
