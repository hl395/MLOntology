{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\MLOntology\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "d:\\Anaconda3\\envs\\MLOntology\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "# Imports\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variabls\n",
    "\n",
    "directory_path =  \"D:/MLOntology/NCIt/\"\n",
    "data_path = directory_path + \"data/\"\n",
    "vector_model_path = directory_path +\"vectorModel/\"\n",
    "cnn_model_path = directory_path +\"cnnModel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trailing_number(s):\n",
    "    m = re.search(r'\\d+$', s)\n",
    "    return m.group() if m else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prostate carcinoma\n",
      "stage ia esophageal cancer ajcc v7\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#read class label file\n",
    "#create mapping from id to labels  \n",
    "#iso-8859-1 , encoding=\"iso-8859-1\"\n",
    "conceptLabelDict={}\n",
    "errors=[]\n",
    "\n",
    "def read_label(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptID = get_trailing_number(splitted[1])\n",
    "                conceptLabelDict[conceptID] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "label_file = data_path + \"ontClassLabels_owl_ncit.txt\"\n",
    "read_label(label_file)\n",
    "print(conceptLabelDict[\"4863\"])\n",
    "print(conceptLabelDict[\"115117\"])\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3801', 0.576964795589447),\n",
      " ('7387', 0.534248948097229),\n",
      " ('7158', 0.5248719453811646),\n",
      " ('7087', 0.5207959413528442),\n",
      " ('66803', 0.516688883304596),\n",
      " ('6409', 0.5089408159255981),\n",
      " ('8502', 0.5051150321960449),\n",
      " ('136619', 0.4978005290031433),\n",
      " ('3191', 0.49357178807258606),\n",
      " ('3087', 0.4708564281463623)]\n"
     ]
    }
   ],
   "source": [
    "# PV-DM seems better??\n",
    "vector_model_file = vector_model_path + \"model1\"\n",
    "\n",
    "vector_model = gensim.models.Doc2Vec.load(vector_model_file)\n",
    "\n",
    "inferred_vector = vector_model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(vector_model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectorFromModel(concept_id, conceptLabelDict, model):\n",
    "    if concept_id in model.docvecs:\n",
    "        concept_vector= model.docvecs[concept_id]\n",
    "    else:\n",
    "        print(\"%s not found, get inferred vector \"%(concept_id))\n",
    "        concept_label = conceptLabelDict[concept_id]\n",
    "        concept_vector= model.infer_vector(concept_label.split())\n",
    "    return concept_vector\n",
    "\n",
    "def getVector(line, conceptLabelDict, model):        \n",
    "    a = getVectorFromModel(line[0], conceptLabelDict, model)\n",
    "    b = getVectorFromModel(line[1], conceptLabelDict, model)\n",
    "    c = np.array((a, b))\n",
    "    c = c.T \n",
    "#     c = np.expand_dims(c, axis=2)\n",
    "#     print(c.shape)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels=2 \n",
    "\n",
    "def get_batches(x_samples, y_samples, batch_size=64):\n",
    "    samples = list(zip(x_samples, y_samples))\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    shuffle(samples)\n",
    "    for offset in range(0, num_samples, batch_size):\n",
    "        batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "        X_samples = []\n",
    "        Y_samples= []\n",
    "        for batch_sample in batch_samples:\n",
    "            pair_list = batch_sample[0]\n",
    "            data_vector = getVector(pair_list, conceptLabelDict, vector_model)\n",
    "#                 data_vector = stackVector(data_vector)\n",
    "#                 print(data_vector.shape)\n",
    "            X_samples.append(data_vector)\n",
    "            class_label = batch_sample[1] \n",
    "            Y_samples.append(class_label)\n",
    "\n",
    "        X_samples = np.array(X_samples).astype('float32')\n",
    "        Y_samples = np.eye(n_channels)[Y_samples]\n",
    "#             print('one batch ready')\n",
    "        yield shuffle(X_samples, Y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['4861', '7318', 1], ['87152', '87150', 1], ['87153', '140032', 1], ['87154', '87153', 1], ['87155', '87153', 1]]\n",
      "16533\n"
     ]
    }
   ],
   "source": [
    "conceptPairDict={}\n",
    "errors=[]\n",
    "conceptPairList=[]\n",
    "\n",
    "def read_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                childID = get_trailing_number(splitted[1])\n",
    "                parentID = get_trailing_number(splitted[2].replace(\"\\r\\n\", \"\"))\n",
    "                conceptPairList.append([childID, parentID , 1])\n",
    "#                 conceptPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "pair_file = data_path + \"ontHierarchy_owl_ncit.txt\"\n",
    "read_pair(pair_file)\n",
    "\n",
    "checkpairs = conceptPairList[10:15]\n",
    "print(checkpairs)\n",
    "print(len(conceptPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['7918', '9151', 0], ['7918', '48612', 0], ['7918', '48613', 0], ['7918', '91231', 0], ['7918', '66753', 0]]\n",
      "37147\n"
     ]
    }
   ],
   "source": [
    "conceptNotPairDict={}\n",
    "conceptNotPairList=[]\n",
    "\n",
    "def read_not_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                childID = get_trailing_number(splitted[0])\n",
    "                notparentID = get_trailing_number(splitted[1].replace(\"\\r\\n\", \"\"))\n",
    "                conceptNotPairList.append([childID, notparentID, 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "notPair_file = data_path + \"taxNotPairs_owl_ncit.txt\"\n",
    "read_not_pair(notPair_file)\n",
    "\n",
    "\n",
    "first2pairs =conceptNotPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8784', '86026', 0], ['8784', '7917', 0], ['8784', '86034', 0], ['8784', '86033', 0], ['8784', '86053', 0]]\n",
      "737409\n"
     ]
    }
   ],
   "source": [
    "testingPairList=[]\n",
    "\n",
    "def read_test_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                childID = get_trailing_number(splitted[0])\n",
    "                notparentID = get_trailing_number(splitted[1].replace(\"\\r\\n\", \"\"))\n",
    "                assert childID in vector_model.docvecs, \"%s not in vector model\"%(childID)\n",
    "                assert notparentID in vector_model.docvecs, \"%s not in vector model\"%(notparentID)\n",
    "                testingPairList.append([childID, notparentID, 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "testingPair_file = data_path + \"testing_owl_ncit.txt\"\n",
    "read_test_pair(testingPair_file)\n",
    "\n",
    "\n",
    "first2pairs =testingPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(testingPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def readFromPairList(id_pair_list, id_notPair_list):\n",
    "    pair_list = id_pair_list + id_notPair_list\n",
    "    random.shuffle(pair_list)\n",
    "    idpairs_list =[]\n",
    "    label_list =[]\n",
    "    for i, line in enumerate(pair_list):      \n",
    "        idpairs_list.append([line[0], line[1]])\n",
    "        label_list.append(line[2])\n",
    "    return idpairs_list, label_list\n",
    "\n",
    "idpairs_list, label_list= readFromPairList([], testingPairList)\n",
    "\n",
    "print(label_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/MLOntology/checkpoints-cnn/har.ckpt\n",
      "Test accuracy: 0.685525\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #First let's load meta graph and restore weights\n",
    "    saver = tf.train.import_meta_graph('D:/MLOntology/checkpoints-cnn/har.ckpt.meta')\n",
    "    # Restore\n",
    "    saver.restore(sess, 'D:/MLOntology/checkpoints-cnn/har.ckpt')\n",
    "    # Access the graph\n",
    "    graph = tf.get_default_graph()\n",
    "    #How to access saved variable/Tensor/placeholders \n",
    "    inputs_ = graph.get_tensor_by_name(\"inputs:0\")\n",
    "    labels_ = graph.get_tensor_by_name(\"labels:0\")    \n",
    "    keep_prob_ = graph.get_tensor_by_name(\"keep:0\")\n",
    "    # How to access saved operation\n",
    "    accuracy = graph.get_tensor_by_name(\"accuracy:0\")\n",
    "\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('D:/MLOntology/checkpoints-cnn/'))\n",
    "\n",
    "    \n",
    "    for x_t, y_t in get_batches(idpairs_list, label_list, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1}\n",
    "        \n",
    "        batch_acc = sess.run(accuracy, feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/MLOntology/NCIt/cnnModel/area/har.ckpt\n",
      "Test accuracy: 0.126865\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #First let's load meta graph and restore weights\n",
    "    saver = tf.train.import_meta_graph(cnn_model_path+'area/har.ckpt.meta')\n",
    "    # Restore\n",
    "    saver.restore(sess, cnn_model_path+'area/har.ckpt')\n",
    "    # Access the graph\n",
    "    graph = tf.get_default_graph()\n",
    "    #How to access saved variable/Tensor/placeholders \n",
    "    inputs_ = graph.get_tensor_by_name(\"inputs:0\")\n",
    "    labels_ = graph.get_tensor_by_name(\"labels:0\")    \n",
    "    keep_prob_ = graph.get_tensor_by_name(\"keep:0\")\n",
    "    # How to access saved operation\n",
    "    accuracy = graph.get_tensor_by_name(\"accuracy:0\")\n",
    "\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('D:/MLOntology/checkpoints-cnn/'))\n",
    "\n",
    "    \n",
    "    for x_t, y_t in get_batches(idpairs_list, label_list, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1}\n",
    "        \n",
    "        batch_acc = sess.run(accuracy, feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/MLOntology/NCIt/cnnModel/hier/har.ckpt\n",
      "Test accuracy: 0.172135\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #First let's load meta graph and restore weights\n",
    "    saver = tf.train.import_meta_graph(cnn_model_path+'hier/har.ckpt.meta')\n",
    "    # Restore\n",
    "    saver.restore(sess, cnn_model_path+'hier/har.ckpt')\n",
    "    # Access the graph\n",
    "    graph = tf.get_default_graph()\n",
    "    #How to access saved variable/Tensor/placeholders \n",
    "    inputs_ = graph.get_tensor_by_name(\"inputs:0\")\n",
    "    labels_ = graph.get_tensor_by_name(\"labels:0\")    \n",
    "    keep_prob_ = graph.get_tensor_by_name(\"keep:0\")\n",
    "    # How to access saved operation\n",
    "    accuracy = graph.get_tensor_by_name(\"accuracy:0\")\n",
    "\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('D:/MLOntology/checkpoints-cnn/'))\n",
    "\n",
    "    \n",
    "    for x_t, y_t in get_batches(idpairs_list, label_list, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1}\n",
    "        \n",
    "        batch_acc = sess.run(accuracy, feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((128,2))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 2)\n"
     ]
    }
   ],
   "source": [
    "b= np.ones((128,2))\n",
    "print(b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4)\n"
     ]
    }
   ],
   "source": [
    "c= np.concatenate((a,b),axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
