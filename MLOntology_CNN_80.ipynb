{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['37225000', '52860004', 1], ['159386001', '159385002', 1], ['233836002', '233835003', 1], ['233836002', '304914007', 1], ['224923003', '224717003', 1]]\n",
      "502206\n"
     ]
    }
   ],
   "source": [
    "conceptPairDict={}\n",
    "errors=[]\n",
    "conceptPairList=[]\n",
    "\n",
    "def read_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptPairList.append([splitted[1], splitted[2].replace(\"\\r\\n\", \"\"), 1])\n",
    "#                 conceptPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "pair_file = \"/home/hao/AnacondaProjects/MLOntology/ontHierarchy.txt\"\n",
    "read_pair(pair_file)\n",
    "\n",
    "first2pairs = conceptPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['273187009', '272765000', 0], ['272877001', '272765000', 0], ['273216002', '272765000', 0], ['273125004', '272765000', 0], ['272973003', '272765000', 0]]\n",
      "6166563\n"
     ]
    }
   ],
   "source": [
    "conceptNotPairDict={}\n",
    "conceptNotPairList=[]\n",
    "\n",
    "def read_not_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                conceptNotPairList.append([splitted[0], splitted[1].replace(\"\\r\\n\", \"\"), 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "notPair_file = \"/home/hao/AnacondaProjects/MLOntology/taxNotPairs.txt\"\n",
    "read_not_pair(notPair_file)\n",
    "\n",
    "# first2pairs = {k: conceptNotPairDict[k] for k in list(conceptNotPairDict)[10:15]}\n",
    "first2pairs =conceptNotPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('237267007', 0.6276803612709045),\n",
      " ('722912007', 0.6034080982208252),\n",
      " ('446466006', 0.5920654535293579),\n",
      " ('722913002', 0.5904892683029175),\n",
      " ('67798003', 0.5712212324142456),\n",
      " ('253745002', 0.5680616497993469),\n",
      " ('10759661000119108', 0.5673926472663879),\n",
      " ('277485007', 0.5651165246963501),\n",
      " ('177130000', 0.5642103552818298),\n",
      " ('312974005', 0.5603421926498413)]\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/hao/AnacondaProjects/MLOntology/model0\"\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/MLOntology/model1\"\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[480039, 480021, 480087, 480096, 480071, 480062, 480054, 480061, 480097, 480063]\n",
      "[480180, 480176, 480153, 480127, 480132, 480173, 480126, 480189, 480193, 480182]\n",
      "[480279, 480222, 480221, 480277, 480290, 480264, 480255, 480280, 480212, 480253]\n",
      "[480336, 480390, 480393, 480396, 480394, 480354, 480361, 480340, 480327, 480334]\n",
      "[480492, 480464, 480482, 480424, 480439, 480445, 480452, 480499, 480474, 480496]\n",
      "[480544, 480512, 480552, 480587, 480563, 480531, 480520, 480596, 480506, 480536]\n",
      "[480654, 480684, 480689, 480611, 480673, 480679, 480697, 480646, 480612, 480682]\n",
      "[480785, 480732, 480798, 480797, 480701, 480751, 480707, 480750, 480767, 480716]\n",
      "[480871, 480899, 480816, 480823, 480802, 480831, 480875, 480838, 480801, 480841]\n",
      "[480953, 480933, 480913, 480985, 480904, 480961, 480978, 480902, 480962, 480903]\n",
      "[481022, 481013, 481018, 481039, 481058, 481031, 481059, 481005, 481032, 481025]\n",
      "[481156, 481107, 481150, 481160, 481120, 481188, 481109, 481115, 481137, 481163]\n",
      "[481208, 481259, 481234, 481289, 481222, 481290, 481276, 481291, 481281, 481223]\n",
      "[481385, 481367, 481349, 481334, 481343, 481311, 481352, 481333, 481375, 481354]\n",
      "[481401, 481424, 481460, 481439, 481440, 481457, 481459, 481410, 481494, 481402]\n",
      "[481573, 481506, 481566, 481595, 481564, 481556, 481562, 481577, 481509, 481530]\n",
      "[481619, 481649, 481614, 481621, 481671, 481643, 481617, 481607, 481639, 481674]\n",
      "[481716, 481771, 481782, 481773, 481725, 481706, 481768, 481755, 481751, 481736]\n",
      "[481892, 481850, 481886, 481803, 481848, 481822, 481801, 481860, 481810, 481845]\n",
      "[481965, 481917, 481976, 481959, 481900, 481916, 481934, 481958, 481918, 481927]\n",
      "[482045, 482014, 482004, 482012, 482051, 482067, 482036, 482062, 482043, 482075]\n",
      "[482116, 482111, 482142, 482157, 482178, 482192, 482152, 482159, 482134, 482183]\n",
      "[482253, 482228, 482295, 482202, 482235, 482245, 482286, 482279, 482230, 482220]\n",
      "[482330, 482354, 482311, 482312, 482388, 482374, 482358, 482302, 482362, 482317]\n",
      "[482410, 482462, 482416, 482420, 482457, 482469, 482450, 482436, 482461, 482423]\n",
      "[482596, 482592, 482523, 482516, 482584, 482521, 482501, 482503, 482568, 482557]\n",
      "[482694, 482627, 482636, 482614, 482604, 482655, 482646, 482635, 482644, 482674]\n",
      "[482707, 482750, 482763, 482734, 482719, 482773, 482709, 482736, 482720, 482743]\n",
      "[482825, 482899, 482874, 482800, 482858, 482863, 482829, 482847, 482804, 482892]\n",
      "[482920, 482911, 482970, 482949, 482945, 482971, 482909, 482936, 482948, 482932]\n",
      "[483007, 483035, 483065, 483021, 483066, 483046, 483078, 483052, 483085, 483096]\n",
      "[483161, 483144, 483192, 483187, 483186, 483138, 483123, 483172, 483129, 483174]\n",
      "[483248, 483268, 483225, 483202, 483231, 483224, 483242, 483244, 483217, 483274]\n",
      "[483380, 483322, 483386, 483377, 483364, 483354, 483316, 483382, 483332, 483336]\n",
      "[483486, 483442, 483492, 483496, 483495, 483476, 483477, 483449, 483482, 483434]\n",
      "[483527, 483540, 483583, 483576, 483530, 483524, 483518, 483510, 483512, 483569]\n",
      "[483616, 483617, 483664, 483646, 483665, 483679, 483683, 483620, 483628, 483627]\n",
      "[483737, 483715, 483732, 483759, 483792, 483703, 483789, 483781, 483702, 483713]\n",
      "[483828, 483882, 483886, 483800, 483880, 483891, 483814, 483807, 483888, 483899]\n",
      "[483993, 483989, 483976, 483957, 483932, 483945, 483956, 483939, 483925, 483978]\n",
      "[484099, 484020, 484034, 484014, 484063, 484004, 484015, 484026, 484055, 484048]\n",
      "[484123, 484122, 484174, 484165, 484175, 484187, 484173, 484140, 484129, 484169]\n",
      "[484288, 484230, 484238, 484245, 484283, 484281, 484224, 484242, 484232, 484279]\n",
      "[484332, 484372, 484345, 484315, 484324, 484366, 484335, 484327, 484314, 484375]\n",
      "[484415, 484459, 484480, 484416, 484440, 484427, 484499, 484431, 484445, 484489]\n",
      "[484560, 484569, 484557, 484539, 484515, 484568, 484532, 484552, 484525, 484520]\n",
      "[484631, 484663, 484648, 484695, 484661, 484647, 484680, 484637, 484692, 484616]\n",
      "[484753, 484708, 484718, 484711, 484770, 484721, 484767, 484719, 484746, 484736]\n",
      "[484896, 484823, 484837, 484800, 484825, 484841, 484820, 484842, 484899, 484898]\n",
      "[484977, 484952, 484972, 484941, 484944, 484989, 484936, 484963, 484939, 484971]\n",
      "[485044, 485072, 485066, 485035, 485001, 485036, 485033, 485051, 485022, 485097]\n",
      "[485170, 485185, 485190, 485199, 485124, 485193, 485151, 485155, 485144, 485186]\n",
      "[485253, 485295, 485246, 485245, 485201, 485214, 485240, 485268, 485271, 485228]\n",
      "[485336, 485396, 485374, 485304, 485381, 485366, 485390, 485301, 485310, 485346]\n",
      "[485400, 485469, 485439, 485488, 485421, 485452, 485482, 485435, 485462, 485402]\n",
      "[485571, 485544, 485546, 485585, 485596, 485565, 485527, 485509, 485514, 485575]\n",
      "[485639, 485678, 485621, 485658, 485676, 485633, 485607, 485632, 485656, 485637]\n",
      "[485717, 485759, 485738, 485722, 485733, 485765, 485768, 485706, 485793, 485709]\n",
      "[485873, 485866, 485871, 485827, 485821, 485825, 485851, 485886, 485885, 485892]\n",
      "[485963, 485921, 485997, 485983, 485926, 485900, 485904, 485993, 485991, 485946]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "feature_number = 1024\n",
    "\n",
    "train_list_before=[]\n",
    "train_label_list=[]\n",
    "test_list_before=[]\n",
    "test_label_list =[]\n",
    "\n",
    "offset = 6000*80\n",
    "\n",
    "for i in range(60):\n",
    "    index = np.arange(100)     #generate numbers from 0 to 100\n",
    "    np.random.shuffle(index)        #shuffle the 100 values\n",
    "    index = [int(100*i+j + offset) for j in index]\n",
    "    print(index[10:20])\n",
    "    train_list_before.extend([conceptPairList[b] for b in index[0:40]]) \n",
    "    train_label_list.extend([1]*40)\n",
    "    train_list_before.extend([conceptNotPairList[b] for b in index[40:80]])\n",
    "    train_label_list.extend([0]*40)\n",
    "    test_list_before.extend([conceptPairList[b] for b in index[80:90]])\n",
    "    test_label_list.extend([1]*10)\n",
    "    test_list_before.extend([conceptNotPairList[b] for b in index[90:100]])\n",
    "    test_label_list.extend([0]*10)\n",
    "\n",
    "\n",
    "    \n",
    "train_list =[]\n",
    "test_list = []\n",
    "\n",
    "for line in train_list_before:\n",
    "    if line[0] in model.docvecs and line[1] in model.docvecs:\n",
    "        a= model.docvecs[line[0]]\n",
    "        b= model.docvecs[line[1]]\n",
    "        c = np.array((a, b))\n",
    "#         train_list.append(np.reshape(c, feature_number)) \n",
    "        train_list.append(np.reshape(c, feature_number, order='F'))\n",
    "\n",
    "\n",
    "test_list_ids={}\n",
    "for i, line in enumerate(test_list_before):\n",
    "    if line[0] in model.docvecs and line[1] in model.docvecs:\n",
    "        a= model.docvecs[line[0]]\n",
    "        b= model.docvecs[line[1]]\n",
    "        c = np.array((a, b))\n",
    "        test_list_ids[i] = (line[0], line[1])\n",
    "#         test_list.append(np.reshape(c, feature_number))\n",
    "        test_list.append(np.reshape(c, feature_number, order='F'))\n",
    "\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.08479115,  0.0663422 ,  0.05275482, ...,  0.12619175,\n",
      "       -0.12809703, -0.17363998], dtype=float32), array([ 0.18267164,  0.11323926,  0.17254877, ...,  0.38802034,\n",
      "       -0.02322777, -0.18750308], dtype=float32), array([ 0.10050754,  0.11395817,  0.05497552, ...,  0.11559462,\n",
      "        0.07492956,  0.04896749], dtype=float32), array([ 0.09933073,  0.2667613 , -0.11126914, ...,  0.12529561,\n",
      "       -0.0748793 , -0.09309307], dtype=float32), array([-0.15109827, -0.29729855,  0.01252204, ...,  0.04958273,\n",
      "       -0.06732967, -0.07634298], dtype=float32), array([-0.02135331, -0.04509294,  0.05080427, ..., -0.03882974,\n",
      "       -0.06732488, -0.13254765], dtype=float32), array([ 0.02048296,  0.09543272,  0.04198248, ...,  0.39602429,\n",
      "       -0.01850535,  0.12173899], dtype=float32), array([-0.06650621,  0.10761148, -0.41892618, ..., -0.35549027,\n",
      "        0.02083908,  0.26113132], dtype=float32), array([ 0.17041151,  0.02062284, -0.07458932, ..., -0.05251388,\n",
      "       -0.03330891,  0.17465736], dtype=float32), array([-0.07406281,  0.01339929, -0.02099244, ...,  0.28631487,\n",
      "       -0.06090889, -0.05264195], dtype=float32), array([ 0.14944069,  0.00844833,  0.08820705, ...,  0.15588856,\n",
      "        0.06903277,  0.04082579], dtype=float32), array([ 0.12990399,  0.04825258, -0.1145922 , ...,  0.08668827,\n",
      "        0.21536374, -0.06833659], dtype=float32), array([ 0.09980828,  0.02773909,  0.21816085, ...,  0.08349491,\n",
      "       -0.07676569,  0.04511245], dtype=float32), array([ 0.48778597,  0.04825258,  0.25874674, ...,  0.08668827,\n",
      "        0.34074962, -0.06833659], dtype=float32), array([ 0.21071906,  0.21317461,  0.21897404, ...,  0.50246388,\n",
      "        0.12493704,  0.00348273], dtype=float32), array([-0.01631606,  0.04825258,  0.10576498, ...,  0.08668827,\n",
      "        0.08390829, -0.06833659], dtype=float32), array([ 0.08392967,  0.07797378,  0.04749823, ...,  0.30750844,\n",
      "        0.0867404 , -0.04560573], dtype=float32), array([ 0.04419223,  0.21317461,  0.17194562, ...,  0.50246388,\n",
      "        0.1409615 ,  0.00348273], dtype=float32), array([ 0.10572182,  0.15442044,  0.06259046, ...,  0.16554968,\n",
      "       -0.00343887, -0.05764891], dtype=float32), array([ 0.30164629,  0.02773909,  0.16146721, ...,  0.08349491,\n",
      "        0.02619874,  0.04511245], dtype=float32)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4800\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "print(train_list[30:50])\n",
    "print(train_label_list[30:50])\n",
    "print(len(train_list))\n",
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "[0.96729166666666666]\n"
     ]
    }
   ],
   "source": [
    "#SVM \n",
    "\n",
    "\n",
    "print(len(train_list))\n",
    "\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(train_list, train_label_list)\n",
    "\n",
    "train_errors=[]\n",
    "train_errors.append(clf.score(train_list, train_label_list))\n",
    "print(train_errors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptLabelDict={}\n",
    "errors=[]\n",
    "\n",
    "def read_label(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptLabelDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "label_file = \"/home/hao/AnacondaProjects/MLOntology/ontClassLabels.txt\"\n",
    "read_label(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 1 predicted label [0], but true label is 1\n",
      "('94872003', '126835002') Concept Pairs: (neoplasm of uncertain behavior of ileum --- neoplasm of ileum)\n",
      "index 30 predicted label [1], but true label is 0\n",
      "('188729005', '426248008') Concept Pairs: (adult t-cell leukemia --- aleukemic lymphoid leukemia in remission)\n",
      "index 48 predicted label [0], but true label is 1\n",
      "('445104009', '396238001') Concept Pairs: (sum of proportion of positive staining neoplastic cells score and average intensity of staining score for hormone receptors using immunohistochemistry --- tumor measureable)\n",
      "index 89 predicted label [0], but true label is 1\n",
      "('276805005', '146801000119103') Concept Pairs: (leiomyoma of esophagus --- leiomyoma)\n",
      "index 157 predicted label [1], but true label is 0\n",
      "('262938004', '262902006') Concept Pairs: (secondary hemorrhage --- hematoma of urinary conduit)\n",
      "index 166 predicted label [0], but true label is 1\n",
      "('90678009', '41188003') Concept Pairs: (disorder of lip --- disorder of oral soft tissues)\n",
      "index 176 predicted label [1], but true label is 0\n",
      "('366545009', '16687001') Concept Pairs: (finding of size of hand --- knuckle pads)\n",
      "index 205 predicted label [0], but true label is 1\n",
      "('449295001', '423812005') Concept Pairs: (sarcoma of skull --- sarcoma of head and neck)\n",
      "index 212 predicted label [1], but true label is 0\n",
      "('262966007', '278024000') Concept Pairs: (rupture of muscle --- rhabdomyosarcoma of bladder)\n",
      "index 302 predicted label [0], but true label is 1\n",
      "('396869009', '396871009') Concept Pairs: (pancreatic intraepithelial neoplasia-2: flat or papillary mucinous epithelium with mild to moderate dysplasia --- pancreatic intraepithelial neoplasia)\n",
      "index 364 predicted label [0], but true label is 1\n",
      "('109555003', '400178008') Concept Pairs: (neonatal alveolar lymphangioma --- lymphangioma)\n",
      "index 366 predicted label [0], but true label is 1\n",
      "('109555003', '92196005') Concept Pairs: (neonatal alveolar lymphangioma --- benign neoplasm of lung)\n",
      "index 372 predicted label [1], but true label is 0\n",
      "('402371000', '238677004') Concept Pairs: (verrucous sarcoidosis --- maculopapular sarcoidosis)\n",
      "index 379 predicted label [1], but true label is 0\n",
      "('72470008', '238677004') Concept Pairs: (sarcoidosis, lupus pernio type --- maculopapular sarcoidosis)\n",
      "index 442 predicted label [0], but true label is 1\n",
      "('269467007', '363503004') Concept Pairs: (malignant neoplasm of hand bones --- malignant tumor of upper limb)\n",
      "index 505 predicted label [0], but true label is 1\n",
      "('93826009', '94105000') Concept Pairs: (primary malignant neoplasm of hepatic flexure of colon --- primary malignant neoplasm of transverse colon)\n",
      "index 525 predicted label [0], but true label is 1\n",
      "('402635005', '363227003') Concept Pairs: (benign neoplasm of nail apparatus --- neoplasm of extremity)\n",
      "index 588 predicted label [0], but true label is 1\n",
      "('449293008', '424413001') Concept Pairs: (sarcoma of connective tissue --- sarcoma)\n",
      "index 597 predicted label [1], but true label is 0\n",
      "('162297001', '307214007') Concept Pairs: (headache site --- hematoma of temporal region)\n",
      "index 646 predicted label [0], but true label is 1\n",
      "('18854008', '86049000') Concept Pairs: (struma ovarii, malignant --- malignant neoplasm, primary)\n",
      "index 681 predicted label [0], but true label is 1\n",
      "('269460009', '363349007') Concept Pairs: (malignant tumor of greater curve of stomach --- malignant tumor of stomach)\n",
      "index 689 predicted label [0], but true label is 1\n",
      "('93829002', '94080006') Concept Pairs: (primary malignant neoplasm of hypopharyngeal aspect of aryepiglottic fold --- primary malignant neoplasm of supraglottis)\n",
      "index 721 predicted label [0], but true label is 1\n",
      "('700423003', '443961001') Concept Pairs: (adenocarcinoma of pancreas --- malignant adenomatous neoplasm)\n",
      "index 764 predicted label [0], but true label is 1\n",
      "('109557006', '109800008') Concept Pairs: (congenital anomaly of uvula --- disorder of uvula of palate)\n",
      "index 801 predicted label [0], but true label is 1\n",
      "('126858004', '126863000') Concept Pairs: (neoplasm of ampulla of vater --- neoplasm of pancreatic duct)\n",
      "index 831 predicted label [1], but true label is 0\n",
      "('646009', '63317005') Concept Pairs: (idiopathic cyst of anterior chamber --- exudative cyst of pars plana)\n",
      "index 833 predicted label [1], but true label is 0\n",
      "('733598001', '425869007') Concept Pairs: (acute myeloid leukemia with t(6;9)(p23;q34) translocation --- acute promyelocytic leukemia, fab m3, in remission)\n",
      "index 835 predicted label [1], but true label is 0\n",
      "('723455009', '399995006') Concept Pairs: (phakomatosis pigmentokeratotica --- inflammatory linear verrucous epidermal nevus)\n",
      "index 850 predicted label [1], but true label is 0\n",
      "('404200001', '87352006') Concept Pairs: (central abdominal mass --- hydrocele of canal of nuck)\n",
      "index 851 predicted label [1], but true label is 0\n",
      "('703270004', '231828003') Concept Pairs: (infantile hemangioma of rare localization --- capillary hemangioma of eyelid)\n",
      "index 854 predicted label [1], but true label is 0\n",
      "('403966009', '231828003') Concept Pairs: (arteriovenous hemangioma --- capillary hemangioma of eyelid)\n",
      "index 868 predicted label [0], but true label is 1\n",
      "('37206003', '425535000') Concept Pairs: (synovial sarcoma, spindle cell --- synovial sarcoma - category)\n",
      "index 911 predicted label [1], but true label is 0\n",
      "('268005002', '202909008') Concept Pairs: (synovial plica --- villonodular synovitis of tendon sheath)\n",
      "index 949 predicted label [0], but true label is 1\n",
      "('262650007', '300251004') Concept Pairs: (hematoma of gingivae --- lesion of gingivae)\n",
      "index 1013 predicted label [1], but true label is 0\n",
      "('95279007', '254879003') Concept Pairs: (submucous leiomyoma of uterus --- pseudo broad ligament fibroid)\n",
      "index 1042 predicted label [0], but true label is 1\n",
      "('126861003', '126859007') Concept Pairs: (neoplasm of body of pancreas --- neoplasm of pancreas)\n",
      "index 1073 predicted label [1], but true label is 0\n",
      "('33401000119102', '700302005') Concept Pairs: (inflammatory lesion of eyelid --- cyst of upper eyelid)\n",
      "index 1150 predicted label [1], but true label is 0\n",
      "('444911000', '285769009') Concept Pairs: (acute myeloid leukemia with t(9:11)(p22;q23); mllt3-mll --- acute promyelocytic leukemia - hypogranular variant)\n",
      "index 1192 predicted label [1], but true label is 0\n",
      "('703269000', '399965003') Concept Pairs: (giant infantile hemangioma --- deep hemangioma of skin)\n",
      "index 1196 predicted label [1], but true label is 0\n",
      "('403966009', '399965003') Concept Pairs: (arteriovenous hemangioma --- deep hemangioma of skin)\n"
     ]
    }
   ],
   "source": [
    "for i, (item, label) in enumerate(zip(test_list, test_label_list)):\n",
    "    result = clf.predict([item])\n",
    "    if result != label:\n",
    "        print(\"index %d predicted label %s, but true label is %s\" % (i, result, label))\n",
    "        idpair = test_list_ids[i] \n",
    "        concept1 = conceptLabelDict[idpair[0]]\n",
    "        concept2 = conceptLabelDict[idpair[1]]\n",
    "        print(\"%s Concept Pairs: (%s --- %s)\" % (idpair, concept1, concept2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "1200\n",
      "[1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# m3 = np.array((a,d))\n",
    "# m3 = np.reshape(m3, 400, order='F')\n",
    "print(len(test_list))\n",
    "result = clf.predict(test_list)\n",
    "\n",
    "print(result.size)\n",
    "\n",
    "\n",
    "print(result[:29])\n",
    "print(np.array(test_label_list[:29]))\n",
    "\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   30   48   89  157  166  176  205  212  302  364  366  372  379  442\n",
      "  505  525  588  597  646  681  689  721  764  801  831  833  835  850  851\n",
      "  854  868  911  949 1013 1042 1073 1150 1192 1196]\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "err_ids=np.flatnonzero(result != test_label_list)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n",
      "0.950172798216\n",
      "0.966666574074\n",
      "0.966666666667\n",
      "0.96666675926\n",
      "[ 0.96672213  0.96661102]\n",
      "[ 0.96833333  0.965     ]\n",
      "[ 0.96511628  0.96822742]\n",
      "0.966671851909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(accuracy_score(result, test_label_list))\n",
    "print(average_precision_score(result, test_label_list))\n",
    "\n",
    "print(f1_score(result, test_label_list, average='macro') ) \n",
    "\n",
    "print(f1_score(result, test_label_list, average='micro')  )\n",
    "\n",
    "print(f1_score(result, test_label_list, average='weighted') )\n",
    "\n",
    "print(f1_score(result, test_label_list, average=None))\n",
    "\n",
    "print(precision_score(result, test_label_list, average=None))\n",
    "print(recall_score(result, test_label_list, average=None))\n",
    "\n",
    "print(roc_auc_score(result, test_label_list, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label_list_m = np.eye(2)[train_label_list]\n",
    "# test_label_list_m = np.eye(2)[test_label_list]\n",
    "# print(test_label_list_m[10:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 100\n",
      "Train Loss: 0.681625 Acc: [0.75]\n",
      "None 200\n",
      "Train Loss: 0.654703 Acc: [0.80000001]\n",
      "None 300\n",
      "Train Loss: 0.556615 Acc: [0.88999999]\n",
      "None 400\n",
      "Train Loss: 0.389736 Acc: [0.93000001]\n",
      "None 500\n",
      "Train Loss: 0.249503 Acc: [0.94999999]\n",
      "None 600\n",
      "Train Loss: 0.157865 Acc: [0.99000001]\n",
      "None 700\n",
      "Train Loss: 0.110269 Acc: [0.97000003]\n",
      "None 800\n",
      "Train Loss: 0.118554 Acc: [0.95999998]\n",
      "None 900\n",
      "Train Loss: 0.0839552 Acc: [0.99000001]\n",
      "None 1000\n",
      "Train Loss: 0.0927155 Acc: [0.95999998]\n",
      "None 1100\n",
      "Train Loss: 0.07362 Acc: [0.97000003]\n",
      "None 1200\n",
      "Train Loss: 0.0723841 Acc: [0.98000002]\n",
      "None 1300\n",
      "Train Loss: 0.0571115 Acc: [0.98000002]\n",
      "None 1400\n",
      "Train Loss: 0.023836 Acc: [1.0]\n",
      "0.983333\n",
      "Model saved in file: ./model-noleaky.ckpt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "In the data, there are 2 classes and every class has 3000 samples and every sample has 512 features\n",
    "The first 3000 samples are from class 0, second 3000 are from class 1\n",
    "'''\n",
    "# DATA_DIR = ''\n",
    "CLASS_NUM = 2       #there are 2 classes\n",
    "SPLIT_PERCENT = 0.8     #split the data into 80% for training and 20% for testing\n",
    "FEATURE_NUM = 1024   \n",
    "TRAIN_ITER = 1500    #the number of iterations for training\n",
    "display_step = 100        #how many iterations to display the results\n",
    "\n",
    "\n",
    "train_num = int(3000*SPLIT_PERCENT)     #the number of samples for training\n",
    "\n",
    "train_feature = train_list      #training features (list of list)\n",
    "train_y = train_label_list        #training lables    (list)\n",
    "test_feature = test_list       #test features  (list of list)\n",
    "test_y = test_label_list         #test labels    (list)\n",
    "\n",
    "\n",
    "y_m = np.eye(2)[train_y]\n",
    "test_y_m = np.eye(2)[test_y]\n",
    "\n",
    "'''\n",
    "y = wx+b        (vectors)\n",
    "'''\n",
    "#function to get variables 'w'\n",
    "def weight_variable(shape, num):\n",
    "    initial = tf.truncated_normal(shape, stddev=1/num)\n",
    "    return tf.Variable(initial, name='weight')\n",
    "\n",
    "#the bias 'b' in the equations\n",
    "def bias_variable(shape, num):\n",
    "    initial = tf.constant(0.0001, shape=shape)\n",
    "    return tf.Variable(initial, name='bias')\n",
    "\n",
    "#convolutional process\n",
    "def conv1d(x, W):\n",
    "    return tf.nn.conv1d(x, W, stride=1, padding='SAME')     #x: variable, w: weight, stride and padding (padding can be ignored currently) \n",
    "\n",
    "#pooling process\n",
    "def max_pool_1x1(x, shape):\n",
    "    x=tf.reshape(x,shape)       #it is transfered into four dimensions, but the other three are 1\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 4, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "'''\n",
    "The feature is 3 dimensional data.  [batch, length, channel] \n",
    "batch is usually ignored (for example there are 100 samples in a batch, so samples should not be modified mutually), length and channel are shown in the paper.\n",
    "At first, the length is 512, and channel is 1.\n",
    "Because our data are time series data, so length is enough, but for images, it may be [batch, length, width, channel]\n",
    "'''\n",
    "# the convolutional layer\n",
    "def layer(features, f, input_n, channel, hidden_units, layer_index):\n",
    "    \"\"\"Construct a convolutional layer\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    f: the length\n",
    "    input_n: Size of the features used in the convention.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    layer_index: the index of layer\n",
    "    Returns:\n",
    "    hidden units: The unit output for the next layer.\n",
    "    weights: the weights in the current hidden layer\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden'+str(layer_index)) as scope:     # name scope may be ignored first\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, channel, hidden_units], math.sqrt(f))\n",
    "\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(f))\n",
    "    hidden = relu(conv1d(features, weights) + biases, 0.01)\n",
    "    shape = [-1,1,f,hidden_units]\n",
    "    h_pool1 = max_pool_1x1(hidden,shape)\n",
    "    return h_pool1, weights\n",
    "\n",
    "# fully connected layer, here the data are two dimension, [batch, length]\n",
    "def densely_connect(features, input_n, hidden_units):\n",
    "    \"\"\"Construct a fully (densely) connected layer.\n",
    "    Args:\n",
    "    features: Features placeholder, from the previous layer.\n",
    "    input_n: Size of units in the previous layer.\n",
    "    hidden_units: Size of the current hidden layer.\n",
    "    Returns:\n",
    "    logits: The estimated output in last layer.\n",
    "    weights: the weights in the hidden layer\n",
    "    \"\"\"\n",
    "    with tf.name_scope('softmax_linear') as dense:\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    logits = relu(tf.matmul(features, weights) + biases, 0.01)      # the matrix product operation\n",
    "    return logits, weights\n",
    "\n",
    "# dropout layer (it is not necessary)\n",
    "# randomly set (1-keep_prob) percentage of units to be zero\n",
    "def dropout(features, input_n, hidden_units, keep_prob):\n",
    "    with tf.name_scope('dropout'):\n",
    "        with tf.name_scope(\"weight\"):\n",
    "            weights = weight_variable([input_n, hidden_units], math.sqrt(input_n))\n",
    "        with tf.name_scope(\"bias\"):\n",
    "            biases = bias_variable([hidden_units], math.sqrt(input_n))\n",
    "    h_fc1_drop = tf.nn.dropout(features, keep_prob)\n",
    "    drop_out = relu(tf.matmul(features, weights) + biases, 0.01)\n",
    "    return drop_out\n",
    "\n",
    "# calculate the loss in the neural network\n",
    "def loss(logits, labels):\n",
    "    \"\"\"Calculates the loss from the logits and the labels.\n",
    "    Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size, NUM_CLASSES].\n",
    "    Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        labels = tf.to_int64(labels)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=labels, logits=logits, name='xentropy')\n",
    "    # tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "def next_batch(data, label, num):\n",
    "    \"\"\"Generate the next batch randomly\n",
    "    Args:\n",
    "    data: training data.\n",
    "    label: training label.\n",
    "    num: the size in a batch\n",
    "    Returns:\n",
    "    next batch's training features and labels.\n",
    "    \"\"\"\n",
    "    index = np.arange(len(data))\n",
    "    np.random.shuffle(index)\n",
    "#     train_feature = data[np.array(index)[0:num]]\n",
    "#     train_label = label[np.array(index)[0:num]]\n",
    "    train_feature_batch = [data[b] for b in index[0:num]]\n",
    "    train_feature_batch = np.asarray(train_feature_batch)\n",
    "    train_label_batch = [label[b] for b in index[0:num]]\n",
    "    train_label_batch = np.asarray(train_label_batch)\n",
    "    return train_feature_batch, train_label_batch\n",
    "\n",
    "def relu(x, alpha=0., max_value=None):\n",
    "    '''ReLU.\n",
    "    alpha: slope of negative section.\n",
    "    '''\n",
    "    negative_part = tf.nn.relu(-x)\n",
    "    x = tf.nn.relu(x)\n",
    "    if max_value is not None:\n",
    "        x = tf.clip_by_value(x, tf.cast(0., dtype=tf.float32),\n",
    "                             tf.cast(max_value, dtype=tf.float32))\n",
    "    x -= tf.constant(alpha, dtype=tf.float32) * negative_part\n",
    "    return x\n",
    "\n",
    "#define a session to run the model\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#place holders for training features and label\n",
    "#None means the value is variable\n",
    "x = tf.placeholder(tf.float32, shape=[None, FEATURE_NUM])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, CLASS_NUM])\n",
    "\n",
    "# decide whether it is training or testing, it is not used in our model, but it may be used\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "#from [-1, 512, 1] -> [-1, 256, 32] -> [-1, 128, 64] -> [-1, 64, 64] -> [-1, 32, 64] -> [-1, 16, 64] -> [-1, 8, 64] -> [-1, 200]\n",
    "\n",
    "#6 hidden layers\n",
    "x_1 = tf.reshape(x, [-1,FEATURE_NUM,1])\n",
    "h_pool0, w0 = layer(x_1, FEATURE_NUM, 15, 1, 32, 0)\n",
    "h_pool0 = tf.reshape(h_pool0, [-1,512,32])\n",
    "h_pool1, w1 = layer(h_pool0, 512, 10, 32, 64, 1)\n",
    "\n",
    "h_pool1 = tf.reshape(h_pool1, [-1,256,64])\n",
    "h_pool2, w2 = layer(h_pool1, 256, 10, 64, 64, 2)\n",
    "h_pool2 = tf.reshape(h_pool2, [-1,128,64])\n",
    "h_pool3, w3 = layer(h_pool2, 128, 10, 64, 64, 3)\n",
    "h_pool3 = tf.reshape(h_pool3, [-1,64,64])\n",
    "h_pool4, w4 = layer(h_pool3, 64, 5, 64, 64, 4)\n",
    "h_pool4 = tf.reshape(h_pool4, [-1,32,64])\n",
    "h_pool5, w5 = layer(h_pool4, 32, 5, 64, 64, 5)\n",
    "h_pool5 = tf.reshape(h_pool5, [-1,16,64])\n",
    "h_pool6, w6 = layer(h_pool5, 16, 5, 64, 64, 6)\n",
    "h_pool6 = tf.reshape(h_pool6, [-1,8,64])\n",
    "\n",
    "#densely connected: 200 units\n",
    "h_pool_flat = tf.reshape(h_pool6, [-1, 8*64])\n",
    "h_dc, w_d = densely_connect(h_pool_flat, 8*64, 200)\n",
    "\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y_conv=dropout(h_dc, (int)(h_dc.get_shape()[1]), CLASS_NUM, keep_prob)\n",
    "\n",
    "\n",
    "beta = 0.001\n",
    "cross_entropy = loss(y_conv, y_)\n",
    "loss = cross_entropy +beta*(tf.nn.l2_loss(w0) + tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2)+tf.nn.l2_loss(w3)+tf.nn.l2_loss(w4)+tf.nn.l2_loss(w5)+tf.nn.l2_loss(w6)+tf.nn.l2_loss(w_d))  #L2 regularization\n",
    "epsilon = 1e-5      # learning rate\n",
    "train_step = tf.train.AdamOptimizer(epsilon).minimize(loss)     #optimization function, our goal is to minimize the loss\n",
    "\n",
    "predict = tf.argmax(y_conv,1)   #the predicted class\n",
    "\n",
    "# calculate the accuray, the corrected classified divided by the total size\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "#saver to save the training check point\n",
    "# variables can be restored in a new model by 'saver.restore(sess, save_path)'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())  #initialize the variables\n",
    "\n",
    "\n",
    "for i in range(1,TRAIN_ITER):       #training iterations\n",
    "    d, l = next_batch(train_feature, y_m, 100)      # get 100 samples in one batch\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d), len(l)))\n",
    "#     print(\"d size is %s, l size is %s \"% (len(d[0]), len(l[0])))\n",
    "    _, ls=sess.run([train_step,cross_entropy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:True})     #run the train step (optimization function), the second one is just to show the loss in this iteration.   THE FEED dictionary is to feed the place holders which are needed in the optimization function.\n",
    "    \n",
    "    if i%display_step==0:\n",
    "        print(_, i)\n",
    "        acc = sess.run([accuracy], feed_dict={x: d, y_: l, keep_prob: 1, is_training:False})\n",
    "        print(\"Train Loss:\", ls, \"Acc:\", acc)\n",
    "\n",
    "# sess.run  or tensor.eval are two ways\n",
    "# get the accuracy in the testing data\n",
    "print(accuracy.eval(session=sess, feed_dict={x:test_feature, y_:test_y_m, keep_prob: 1, is_training:False}))\n",
    "\n",
    "\n",
    "# save the model results\n",
    "save_path = saver.save(sess, \"./model-noleaky.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[  30  174  214  259  333  338  356  397  502  675  698  764  831  835  847\n",
      "  852  865  962 1030 1081]\n",
      "20\n",
      "index 30 predicted label 1, but true label is 0\n",
      "('188729005', '426248008') Concept Pairs: (adult t-cell leukemia --- aleukemic lymphoid leukemia in remission)\n",
      "index 174 predicted label 1, but true label is 0\n",
      "('264556007', '126966009') Concept Pairs: (occult neuropathy --- neoplasm of cranial nerve)\n",
      "index 214 predicted label 1, but true label is 0\n",
      "('126958000', '429033009') Concept Pairs: (neoplasm of cerebral ventricle --- malignant neoplasm of cerebrum)\n",
      "index 259 predicted label 1, but true label is 0\n",
      "('369514009', '369555007') Concept Pairs: (secondary malignant neoplasm of left fallopian tube --- malignant tumor involving right fallopian tube by separate metastasis from ovary)\n",
      "index 333 predicted label 1, but true label is 0\n",
      "('403869001', '403864006') Concept Pairs: (stucco keratosis --- basal cell papilloma solid (acanthotic) type)\n",
      "index 338 predicted label 1, but true label is 0\n",
      "('255184001', '403864006') Concept Pairs: (papilloma of skin --- basal cell papilloma solid (acanthotic) type)\n",
      "index 356 predicted label 1, but true label is 0\n",
      "('303017006', '402881008') Concept Pairs: (malignant lymphoma, convoluted cell type --- primary cutaneous b-cell lymphoma)\n",
      "index 397 predicted label 1, but true label is 0\n",
      "('285637005', '369571007') Concept Pairs: (metastasis to ovary of unknown primary --- malignant tumor involving right ovary by separate metastasis from uterine cervix)\n",
      "index 502 predicted label 0, but true label is 1\n",
      "('304594002', '6471006') Concept Pairs: (suicidal intent --- suicidal thoughts)\n",
      "index 675 predicted label 1, but true label is 0\n",
      "('369536007', '369538008') Concept Pairs: (secondary neoplasm of right broad ligament --- malignant tumor involving left broad ligament by metastasis from ovary)\n",
      "index 698 predicted label 1, but true label is 0\n",
      "('254749001', '255093003') Concept Pairs: (fibrohistiocytic tumor of skin --- malignant skin tumor with adnexal differentiation)\n",
      "index 764 predicted label 0, but true label is 1\n",
      "('109557006', '109800008') Concept Pairs: (congenital anomaly of uvula --- disorder of uvula of palate)\n",
      "index 831 predicted label 1, but true label is 0\n",
      "('646009', '63317005') Concept Pairs: (idiopathic cyst of anterior chamber --- exudative cyst of pars plana)\n",
      "index 835 predicted label 1, but true label is 0\n",
      "('723455009', '399995006') Concept Pairs: (phakomatosis pigmentokeratotica --- inflammatory linear verrucous epidermal nevus)\n",
      "index 847 predicted label 0, but true label is 1\n",
      "('409972000', '129125009') Concept Pairs: (pre-hospital care --- procedure with explicit context)\n",
      "index 852 predicted label 1, but true label is 0\n",
      "('34540009', '309525008') Concept Pairs: (flail limb --- mass of upper limb)\n",
      "index 865 predicted label 0, but true label is 1\n",
      "('384151000119104', '43204002') Concept Pairs: (screening mammography of bilateral breasts --- bilateral mammography)\n",
      "index 962 predicted label 0, but true label is 1\n",
      "('161464003', '312850006') Concept Pairs: (history of psychiatric disorder --- history of disorder)\n",
      "index 1030 predicted label 1, but true label is 0\n",
      "('369489005', '369491002') Concept Pairs: (malignant tumor involving seminal vesicle by direct extension from bladder --- malignant tumor involving seminal vesicle by separate metastasis from bladder)\n",
      "index 1081 predicted label 0, but true label is 1\n",
      "('102744001', '50371003') Concept Pairs: (homocarnosine --- carnosine)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = sess.run(predict, feed_dict={x:test_feature, keep_prob:1, is_training:False})\n",
    "print(y_pred[:20])\n",
    "print(test_y[:20])\n",
    "\n",
    "\n",
    "err_ids=np.flatnonzero(y_pred != test_y)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)\n",
    "for err_id in err_ids:\n",
    "    print(\"index %d predicted label %s, but true label is %s\" % (err_id, y_pred[err_id], test_y[err_id]))\n",
    "    idpair = test_list_ids[err_id] \n",
    "    concept1 = conceptLabelDict[idpair[0]]\n",
    "    concept2 = conceptLabelDict[idpair[1]]\n",
    "    print(\"%s Concept Pairs: (%s --- %s)\" % (idpair, concept1, concept2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.983333333333\n",
      "0.978870614035\n",
      "0.98333259256\n",
      "0.983333333333\n",
      "0.983334074107\n",
      "[ 0.98322148  0.98344371]\n",
      "[ 0.97666667  0.99      ]\n",
      "[ 0.98986486  0.97697368]\n",
      "0.983419274538\n"
     ]
    }
   ],
   "source": [
    "result = y_pred\n",
    "test_label_list = test_y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "print(accuracy_score(result, test_label_list))\n",
    "print(average_precision_score(result, test_label_list))\n",
    "\n",
    "print(f1_score(result, test_label_list, average='macro') ) \n",
    "\n",
    "print(f1_score(result, test_label_list, average='micro')  )\n",
    "\n",
    "print(f1_score(result, test_label_list, average='weighted') )\n",
    "\n",
    "print(f1_score(result, test_label_list, average=None))\n",
    "\n",
    "print(precision_score(result, test_label_list, average=None))\n",
    "print(recall_score(result, test_label_list, average=None))\n",
    "\n",
    "print(roc_auc_score(result, test_label_list, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "err_ids=np.flatnonzero(result != test_label_list)\n",
    "\n",
    "print(err_ids)\n",
    "print(err_ids.size)\n",
    "for err_id in err_ids:\n",
    "    idpair = test_list_ids[err_id] \n",
    "    print(idpair)\n",
    "    concept1 = conceptLabelDict[idpair[0]]\n",
    "    concept2 = conceptLabelDict[idpair[1]]\n",
    "    print(\"(Concept 1 %s ---- Concept 2 %s)\" % (concept1, concept2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = [[1,2,3], [3,4,5], [3,4,5], [5,6,7],[2,3,7]]\n",
    "\n",
    "plist.extend([1]*4)\n",
    "print(plist)\n",
    "\n",
    "index = np.arange(5)\n",
    "\n",
    "c = [plist[b] for b in index[:2]]\n",
    "print(c)\n",
    "c.extend([plist[b] for b in index[2:]])\n",
    "print(c)\n",
    "\n",
    "for i in range(3):      # i is the class index, for example, i==0 for class 0, i==1 for class 1 ...\n",
    "    index = np.arange(30)     #generate numbers from 0 to 2999\n",
    "    np.random.shuffle(index)        #shuffle the 3000 values\n",
    "    index = [int(300*i+j) for j in index]\n",
    "    print(index)\n",
    "    print(np.array(index)[0:5])\n",
    "    print(np.array(index)[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([1,2,3])\n",
    "y=np.append(y, [1]*4)\n",
    "y= np.append(y, [0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/MLOntology/model1\"\n",
    "\n",
    "print(path)\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(inferred_vector)\n",
    "print(inferred_vector.size)\n",
    "# pprint(model.docvecs.most_similar([inferred_vector], topn=20))\n",
    "\n",
    "\n",
    "path = \"D:/MLOntology/model0\"\n",
    "\n",
    "print(path)\n",
    "\n",
    "model = gensim.models.Doc2Vec.load(path)\n",
    "inferred_vector = model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(inferred_vector)\n",
    "print(inferred_vector.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs['SENT_5690']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "print(X)\n",
    "print(y)\n",
    "train_errors=[]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)  \n",
    "\n",
    "train_errors.append(clf.score(X, y))\n",
    "print(train_errors)\n",
    "X_test=[[2,2]]\n",
    "y_test = [1]\n",
    "test_errors=[]\n",
    "clf.predict(X_test)\n",
    "\n",
    "test_errors.append(clf.score(X_test, y_test))\n",
    "print(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "a= model.docvecs[0]\n",
    "b= model.docvecs[1]\n",
    "m1 = np.array((a, b))\n",
    "\n",
    "# print(np.reshape(m1, 1024))\n",
    "# print(np.reshape(m1, 400, order='F')) # two ways of reshape\n",
    "\n",
    "c= model.docvecs[2]\n",
    "d= model.docvecs[3]\n",
    "print(c.shape)\n",
    "print(d.shape)\n",
    "print(c.shape[0]+d.shape[0])\n",
    "m2 = np.array((c, d))\n",
    "\n",
    "m1 = np.reshape(m1, 1024)\n",
    "m2 = np.reshape(m2, 1024)\n",
    "# m1 = np.reshape(m1, 1024, order='F')\n",
    "# m2 = np.reshape(m2, 1024, order='F')\n",
    "\n",
    "print(m1)\n",
    "\n",
    "X = [m1, m2]\n",
    "print(X)\n",
    "\n",
    "XX = np.append(m1, m2)\n",
    "print(XX)\n",
    "\n",
    "y = [0, 1]\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X, y)\n",
    "\n",
    "m3 = np.array((a,d))\n",
    "m3 = np.reshape(m3, 1024, order='F')\n",
    "\n",
    "result = clf.predict([m3])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=  np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "m1 = np.array((a, b))\n",
    "print(m1)\n",
    "\n",
    "m2 = np.vstack((a, b)).T\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDict={}\n",
    "testDict[0] = (\"a\", \"b\")\n",
    "print(testDict[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,1,0,1,0,1,0])\n",
    "b = np.array([0,1,0,1,0,1,1])\n",
    "\n",
    "np.flatnonzero(a!=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
