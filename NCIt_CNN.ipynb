{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/anaconda3/envs/MLOntology/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variabls\n",
    "\n",
    "directory_path = \"/home/hao/AnacondaProjects/MLOntology/NCIt/\"\n",
    "data_path = directory_path + \"data/\"\n",
    "vector_model_path = directory_path +\"vectorModel/\"\n",
    "cnn_model_path = directory_path +\"cnnModel/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_trailing_number(s):\n",
    "    m = re.search(r'\\d+$', s)\n",
    "    return m.group() if m else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prostate carcinoma\n",
      "stage ia esophageal cancer ajcc v7\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#read class label file\n",
    "#create mapping from id to labels  \n",
    "#iso-8859-1 , encoding=\"iso-8859-1\"\n",
    "conceptLabelDict={}\n",
    "errors=[]\n",
    "\n",
    "def read_label(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptID = get_trailing_number(splitted[1])\n",
    "                conceptLabelDict[conceptID] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "label_file = data_path + \"ontClassLabels_owl_ncit.txt\"\n",
    "read_label(label_file)\n",
    "print(conceptLabelDict[\"4863\"])\n",
    "print(conceptLabelDict[\"115117\"])\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['4861', '7318', 1], ['87152', '87150', 1], ['87153', '140032', 1], ['87154', '87153', 1], ['87155', '87153', 1]]\n",
      "16533\n"
     ]
    }
   ],
   "source": [
    "conceptPairDict={}\n",
    "errors=[]\n",
    "conceptPairList=[]\n",
    "\n",
    "def read_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                childID = get_trailing_number(splitted[1])\n",
    "                parentID = get_trailing_number(splitted[2].replace(\"\\r\\n\", \"\"))\n",
    "                conceptPairList.append([childID, parentID , 1])\n",
    "#                 conceptPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "pair_file = data_path + \"ontHierarchy_owl_ncit.txt\"\n",
    "read_pair(pair_file)\n",
    "\n",
    "checkpairs = conceptPairList[10:15]\n",
    "print(checkpairs)\n",
    "print(len(conceptPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['7918', '9151', 0], ['7918', '48612', 0], ['7918', '48613', 0], ['7918', '91231', 0], ['7918', '66753', 0]]\n",
      "32197\n"
     ]
    }
   ],
   "source": [
    "conceptNotPairDict={}\n",
    "conceptNotPairList=[]\n",
    "\n",
    "def read_not_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                childID = get_trailing_number(splitted[0])\n",
    "                notparentID = get_trailing_number(splitted[1].replace(\"\\r\\n\", \"\"))\n",
    "                conceptNotPairList.append([childID, notparentID, 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "notPair_file = data_path + \"taxNotPairs_owl_ncit.txt\"\n",
    "read_not_pair(notPair_file)\n",
    "\n",
    "\n",
    "first2pairs =conceptNotPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16533\n"
     ]
    }
   ],
   "source": [
    "# In-place shuffle\n",
    "random.shuffle(conceptNotPairList)\n",
    "conceptNotPairList = conceptNotPairList[:len(conceptPairList)]\n",
    "\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9748, 0.718510627746582),\n",
      " (5881, 0.7043671011924744),\n",
      " (3393, 0.6982367038726807),\n",
      " (7167, 0.6774163246154785),\n",
      " (7095, 0.670405387878418),\n",
      " (4230, 0.6645699739456177),\n",
      " (3973, 0.6582036018371582),\n",
      " (9004, 0.6564831137657166),\n",
      " (2440, 0.6524202823638916),\n",
      " (3529, 0.6482406854629517)]\n"
     ]
    }
   ],
   "source": [
    "vector_model_file = vector_model_path + \"model0\"\n",
    "\n",
    "vector_model = gensim.models.Doc2Vec.load(vector_model_file)\n",
    "\n",
    "inferred_vector = vector_model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(vector_model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"tag '7918' not seen in training corpus/invalid\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fc8d6f104f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvector_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'7918'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tag '%s' not seen in training corpus/invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"tag '7918' not seen in training corpus/invalid\""
     ]
    }
   ],
   "source": [
    "vector_model.docvecs[7918]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_model_file = vector_model_path + \"model1\"\n",
    "\n",
    "vector_model = gensim.models.Doc2Vec.load(vector_model_file)\n",
    "\n",
    "inferred_vector = vector_model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(vector_model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "feature_number = 1024\n",
    "\n",
    "def readFromPairList(id_pair_list, id_notPair_list):\n",
    "    pair_list = id_pair_list + id_notPair_list\n",
    "    random.shuffle(pair_list)\n",
    "    idpairs_list =[]\n",
    "    label_list =[]\n",
    "    for i, line in enumerate(pair_list):      \n",
    "        idpairs_list.append([line[0], line[1]])\n",
    "        label_list.append(line[2])\n",
    "    return idpairs_list, label_list\n",
    "\n",
    "idpairs_list, label_list= readFromPairList(conceptPairList, conceptNotPairList)\n",
    "\n",
    "print(label_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['133442', '4024'], ['115033', '45709'], ['129707', '127155'], ['3540', '3219'], ['8949', '2919'], ['8863', '4341'], ['6149', '39850'], ['27843', '66950'], ['115138', '134515'], ['5280', '3139'], ['7995', '62277'], ['140418', '91202'], ['3876', '3157'], ['6587', '8964'], ['6429', '6421'], ['5512', '7399'], ['8927', '8853'], ['96948', '7124'], ['5636', '5637'], ['39998', '40009']]\n",
      "[['133442', '4024'], ['115033', '45709'], ['129707', '127155'], ['3540', '3219'], ['8949', '2919'], ['8863', '4341'], ['6149', '39850'], ['27843', '66950'], ['115138', '134515'], ['5280', '3139'], ['7995', '62277'], ['140418', '91202'], ['3876', '3157'], ['6587', '8964'], ['6429', '6421'], ['5512', '7399'], ['8927', '8853'], ['96948', '7124'], ['5636', '5637'], ['39998', '40009']]\n",
      "[1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0]\n",
      "[0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(idpairs_list, label_list, test_size = 0.2, shuffle= True)\n",
    "print(X_train[:20])\n",
    "print(X_train[:20])\n",
    "print(y_train[:20])\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 1024\n",
    "\n",
    "def getVectorFromModel(concept_id, conceptLabelDict, model):\n",
    "    if concept_id in model.docvecs:\n",
    "        concept_vector= model.docvecs[concept_id]\n",
    "    else:\n",
    "        print(\"%s not found, get inferred vector \"%(concept_id))\n",
    "        concept_label = conceptLabelDict[concept_id]\n",
    "        concept_vector= model.infer_vector(concept_label.split())\n",
    "    return concept_vector\n",
    "\n",
    "def getVector(line, conceptLabelDict, model):        \n",
    "    a = getVectorFromModel(line[0], conceptLabelDict, model)\n",
    "    b = getVectorFromModel(line[1], conceptLabelDict, model)\n",
    "    c = np.array((a, b))\n",
    "    c = c.T \n",
    "    c = np.expand_dims(c, axis=2)\n",
    "    print(c.shape)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackVector(vector):\n",
    "    from numpy import dstack\n",
    "    return dstack((vector, vector, vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes=2 \n",
    "\n",
    "def generator(x_samples, y_samples, train_flag, batch_size=64):\n",
    "    samples = list(zip(x_samples, y_samples))\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            X_samples = []\n",
    "            Y_samples= []\n",
    "            for batch_sample in batch_samples:\n",
    "                pair_list = batch_sample[0]\n",
    "                data_vector = getVector(pair_list, conceptLabelDict, vector_model)\n",
    "                data_vector = stackVector(data_vector)\n",
    "                X_samples.append(data_vector)\n",
    "                class_label = batch_sample[1] \n",
    "                Y_samples.append(class_label)\n",
    "                \n",
    "            X_samples = np.array(X_samples).astype('float32')\n",
    "            Y_samples = np.eye(nb_classes)[Y_samples]\n",
    "            print('one batch ready')\n",
    "            yield shuffle(X_samples, Y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_batch_size = 64\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(X_train, y_train, train_flag=True, batch_size=set_batch_size)\n",
    "validation_generator = generator(X_test, y_test, train_flag=False, batch_size=set_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = applications.resnet50.ResNet50(weights=None, include_top = True, classes=2)\n",
    "print('Model loaded')\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
    "csv_logger = CSVLogger(directory_path + 'resnet18_cifar10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10133442 not found, get inferred vector \n",
      "\n",
      "4024 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "115033 not found, get inferred vector \n",
      "45709 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "129707 not found, get inferred vector \n",
      "127155 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "3540 not found, get inferred vector \n",
      "3219 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "8949 not found, get inferred vector \n",
      "2919 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "8863 not found, get inferred vector \n",
      "4341 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "6149 not found, get inferred vector \n",
      "39850 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "27843 not found, get inferred vector \n",
      "66950 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "115138 not found, get inferred vector \n",
      "134515 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "5280 not found, get inferred vector \n",
      "3139 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "7995 not found, get inferred vector \n",
      "62277 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "140418 not found, get inferred vector \n",
      "91202 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "3876 not found, get inferred vector \n",
      "3157 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "6587 not found, get inferred vector \n",
      "8964 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "6429 not found, get inferred vector \n",
      "6421 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "5512 not found, get inferred vector \n",
      "7399 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "8927 not found, get inferred vector \n",
      "8853 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "96948 not found, get inferred vector \n",
      "7124 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "5636 not found, get inferred vector \n",
      "5637 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "39998 not found, get inferred vector \n",
      "40009 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "88090 not found, get inferred vector \n",
      "134810 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "9165 not found, get inferred vector \n",
      "27290 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "3528 not found, get inferred vector \n",
      "3530 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "114600 not found, get inferred vector \n",
      "70649 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "63622 not found, get inferred vector \n",
      "3680 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "7567 not found, get inferred vector \n",
      "4159 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "5381 not found, get inferred vector \n",
      "121619 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "6751 not found, get inferred vector \n",
      "3918 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "6238 not found, get inferred vector \n",
      "3541 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "9366 not found, get inferred vector \n",
      "45516 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "122716 not found, get inferred vector \n",
      "122688 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "5965 not found, get inferred vector \n",
      "7915 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "139881 not found, get inferred vector \n",
      "139875 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "27269 not found, get inferred vector \n",
      "7132 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "134753 not found, get inferred vector \n",
      "134747 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "6255 not found, get inferred vector \n",
      "27523 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "39991 not found, get inferred vector \n",
      "102870 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "114600 not found, get inferred vector \n",
      "114576 not found, get inferred vector \n",
      "(200, 2, 1)\n",
      "6034 not found, get inferred vector \n",
      "2893 not found, get inferred vector \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'2893'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c863a12fabe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mset_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     callbacks=[lr_reducer, early_stopper, csv_logger])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2190\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLOntology/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m_data_generator_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                             \u001b[0;31m# => Serialize calls to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                             \u001b[0;31m# infinite iterator/generator's next() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-6dd4a7b9c95c>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(x_samples, y_samples, train_flag, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_sample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mpair_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mdata_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconceptLabelDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mdata_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstackVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mX_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5dd6ec6ae8aa>\u001b[0m in \u001b[0;36mgetVector\u001b[0;34m(line, conceptLabelDict, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconceptLabelDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetVectorFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconceptLabelDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetVectorFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconceptLabelDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5dd6ec6ae8aa>\u001b[0m in \u001b[0;36mgetVectorFromModel\u001b[0;34m(concept_id, conceptLabelDict, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found, get inferred vector \"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mconcept_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconceptLabelDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconcept_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mconcept_vector\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcept_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '2893'"
     ]
    }
   ],
   "source": [
    "myepochs = 10\n",
    "\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=len(X_train)//set_batch_size, \n",
    "                    epochs=myepochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=len(X_test)//set_batch_size,\n",
    "                    callbacks=[lr_reducer, early_stopper, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# vstack\n",
    "from numpy import vstack\n",
    "from numpy import hstack\n",
    "\n",
    "a1 = np.zeros((1, 2,512))\n",
    "# print(a1)\n",
    "print(a1.shape)\n",
    "a2 = np.ones((1, 2,512))\n",
    "# print(a2)\n",
    "print(a2.shape)\n",
    "a3 = np.ones((1, 2,512))\n",
    "# print(a3)\n",
    "print(a3.shape)\n",
    "a4 = vstack((a1, a2, a3))\n",
    "print(a4)\n",
    "print(a4.shape)\n",
    "\n",
    "\n",
    "a1 = np.zeros((512,1))\n",
    "# print(a1)\n",
    "print(a1.shape)\n",
    "a2 = np.ones((512,1))\n",
    "# print(a2)\n",
    "print(a2.shape)\n",
    "a3 = np.ones((512,1))\n",
    "# print(a3)\n",
    "print(a3.shape)\n",
    "a4 = hstack((a1, a2, a3))\n",
    "print(a4)\n",
    "print(a4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# vstack\n",
    "from numpy import vstack\n",
    "from numpy import hstack\n",
    "from numpy import dstack\n",
    "\n",
    "a1 = np.zeros((2, 1))\n",
    "# print(a1)\n",
    "print(a1.shape)\n",
    "a2 = np.ones((2, 1))\n",
    "# print(a2)\n",
    "print(a2.shape)\n",
    "\n",
    "a3 = hstack((a1, a2))\n",
    "# print(a3)\n",
    "print(a3.shape)\n",
    "\n",
    "a4 = dstack((a3, a3, a3))\n",
    "print(a4.shape)\n",
    "print(a4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
