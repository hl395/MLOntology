{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\MLOntology\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "d:\\Anaconda3\\envs\\MLOntology\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variabls\n",
    "\n",
    "directory_path = \"D:/MLOntology/NCIt/\"\n",
    "data_path = directory_path + \"data/\"\n",
    "vector_model_path = directory_path +\"vectorModel/\"\n",
    "cnn_model_path = directory_path +\"cnnModel/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_trailing_number(s):\n",
    "    m = re.search(r'\\d+$', s)\n",
    "    return m.group() if m else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prostate carcinoma\n",
      "stage ia esophageal cancer ajcc v7\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#read class label file\n",
    "#create mapping from id to labels  \n",
    "#iso-8859-1 , encoding=\"iso-8859-1\"\n",
    "conceptLabelDict={}\n",
    "errors=[]\n",
    "\n",
    "def read_label(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                conceptID = get_trailing_number(splitted[1])\n",
    "                conceptLabelDict[conceptID] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "label_file = data_path + \"ontClassLabels_owl_ncit.txt\"\n",
    "read_label(label_file)\n",
    "print(conceptLabelDict[\"4863\"])\n",
    "print(conceptLabelDict[\"115117\"])\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['4861', '7318', 1], ['87152', '87150', 1], ['87153', '140032', 1], ['87154', '87153', 1], ['87155', '87153', 1]]\n",
      "16533\n"
     ]
    }
   ],
   "source": [
    "conceptPairDict={}\n",
    "errors=[]\n",
    "conceptPairList=[]\n",
    "\n",
    "def read_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==3:\n",
    "                childID = get_trailing_number(splitted[1])\n",
    "                parentID = get_trailing_number(splitted[2].replace(\"\\r\\n\", \"\"))\n",
    "                conceptPairList.append([childID, parentID , 1])\n",
    "#                 conceptPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "pair_file = data_path + \"ontHierarchy_owl_ncit.txt\"\n",
    "read_pair(pair_file)\n",
    "\n",
    "checkpairs = conceptPairList[10:15]\n",
    "print(checkpairs)\n",
    "print(len(conceptPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['86053', '88414', 0], ['8784', '7917', 0], ['141358', '141347', 0], ['141358', '141353', 0], ['141358', '141350', 0]]\n",
      "9533\n"
     ]
    }
   ],
   "source": [
    "conceptNotPairDict={}\n",
    "conceptNotPairList=[]\n",
    "\n",
    "def read_not_pair(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #get the id for each concept paragraph\n",
    "            splitted = line.decode(\"iso-8859-1\").split(\"\\t\")\n",
    "            if len(splitted)==2:\n",
    "                childID = get_trailing_number(splitted[0])\n",
    "                notparentID = get_trailing_number(splitted[1].replace(\"\\r\\n\", \"\"))\n",
    "                conceptNotPairList.append([childID, notparentID, 0])\n",
    "#                 conceptNotPairDict[splitted[1]] = splitted[2].replace(\"\\r\\n\", \"\")\n",
    "            else:\n",
    "                errors.append(splitted)\n",
    "\n",
    "notPair_file = data_path + \"taxNotPairs_owl_ncit.txt\"\n",
    "read_not_pair(notPair_file)\n",
    "\n",
    "\n",
    "first2pairs =conceptNotPairList[10:15]\n",
    "print(first2pairs)\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9533\n"
     ]
    }
   ],
   "source": [
    "# In-place shuffle\n",
    "random.shuffle(conceptNotPairList)\n",
    "conceptNotPairList = conceptNotPairList[:len(conceptPairList)]\n",
    "\n",
    "print(len(conceptNotPairList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('142823', 0.7903113961219788),\n",
      " ('3497', 0.7410315275192261),\n",
      " ('4566', 0.7290605902671814),\n",
      " ('5671', 0.7240594625473022),\n",
      " ('27710', 0.7174608707427979),\n",
      " ('5662', 0.7053307294845581),\n",
      " ('4888', 0.7046777009963989),\n",
      " ('4453', 0.7041112184524536),\n",
      " ('7527', 0.7026574611663818),\n",
      " ('8531', 0.6967900991439819)]\n"
     ]
    }
   ],
   "source": [
    "vector_model_file = vector_model_path + \"model0\"\n",
    "\n",
    "vector_model = gensim.models.Doc2Vec.load(vector_model_file)\n",
    "\n",
    "inferred_vector = vector_model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(vector_model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00466427, -0.2133093 , -0.13188908,  0.0122558 ,  0.06393942,\n",
       "       -0.13639419,  0.17377363,  0.20860074,  0.05809897,  0.04649761,\n",
       "       -0.16894959,  0.04225196,  0.02165174,  0.00213318,  0.23631908,\n",
       "        0.16286871, -0.05403031,  0.17041123, -0.05465656, -0.13113794,\n",
       "       -0.17594796, -0.01543587, -0.06278177,  0.06830045, -0.09180123,\n",
       "        0.13543412, -0.08404744, -0.05415916,  0.01433831,  0.18743008,\n",
       "        0.0728869 , -0.03073151,  0.01052943, -0.06885084, -0.07429604,\n",
       "       -0.04760221, -0.05925706, -0.02305547, -0.00136033,  0.09988423,\n",
       "        0.18765847,  0.07289595, -0.18184835,  0.06907446, -0.02444701,\n",
       "       -0.10360537,  0.06086358,  0.2055683 ,  0.02755826, -0.23605673,\n",
       "        0.03812493, -0.03434969,  0.05227038, -0.07879271, -0.08301589,\n",
       "       -0.02411955,  0.05062204,  0.14589028,  0.08253404, -0.09271763,\n",
       "        0.10263631, -0.02226311, -0.08953032, -0.16695775,  0.24429363,\n",
       "       -0.21860647, -0.0721235 , -0.22428198, -0.11841042,  0.0510509 ,\n",
       "        0.17960663,  0.07665799,  0.1609495 ,  0.20206827,  0.04820421,\n",
       "       -0.00157902, -0.18814544,  0.03194261, -0.03955375,  0.2632089 ,\n",
       "       -0.13085769,  0.0412775 ,  0.07151975, -0.0159458 , -0.15055297,\n",
       "        0.06773167, -0.06076983,  0.04802068,  0.18271074, -0.0355709 ,\n",
       "        0.18004274, -0.1961912 , -0.08216402, -0.2582803 ,  0.04467941,\n",
       "       -0.06278328,  0.1149148 , -0.23566185,  0.14411752,  0.0009003 ,\n",
       "        0.16107555, -0.11665135,  0.0449025 , -0.3369949 ,  0.03535606,\n",
       "       -0.03255864, -0.00659236, -0.00761896, -0.09850951,  0.2486117 ,\n",
       "       -0.22899896,  0.07819384,  0.11521312,  0.01756163, -0.0529011 ,\n",
       "       -0.00162916, -0.3619084 ,  0.25249463, -0.0917237 , -0.06498647,\n",
       "       -0.24325114, -0.16524708, -0.27361944,  0.0452979 , -0.04989029,\n",
       "       -0.06102203,  0.01067294, -0.03696042, -0.11552176, -0.29940182,\n",
       "       -0.09042829,  0.09071964,  0.05504733, -0.05555851,  0.08257476,\n",
       "        0.08394587, -0.1545548 , -0.08337624,  0.08647934,  0.15203401,\n",
       "       -0.05458821,  0.00483033,  0.11083694, -0.08633234,  0.13398974,\n",
       "       -0.03296721,  0.10768654, -0.16746   ,  0.13435012,  0.12789004,\n",
       "       -0.15163232,  0.07621377, -0.05344192,  0.1605579 , -0.00830739,\n",
       "       -0.10128653,  0.04618494,  0.09822309,  0.09121216, -0.09367496,\n",
       "        0.08758903,  0.10449179,  0.25650844,  0.1520968 , -0.15764229,\n",
       "        0.22386813,  0.10212433,  0.13855082,  0.19069918,  0.10451463,\n",
       "       -0.17858057, -0.04245194, -0.17185937,  0.02055128, -0.27498132,\n",
       "        0.07487769,  0.15048842,  0.23592913, -0.03006162, -0.00933949,\n",
       "        0.00165979, -0.15416671,  0.07406784,  0.04362661,  0.11354434,\n",
       "       -0.11135231,  0.19687515,  0.03925849,  0.07010745, -0.00321082,\n",
       "        0.0332619 ,  0.08476473,  0.22316521,  0.28012693, -0.07098033,\n",
       "        0.01729473,  0.16909517,  0.18178587,  0.03195855, -0.17325523],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_model.docvecs[7918]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3725', 0.6395902037620544),\n",
      " ('45514', 0.6395784616470337),\n",
      " ('139545', 0.5937005281448364),\n",
      " ('5665', 0.5892391800880432),\n",
      " ('45652', 0.5882729291915894),\n",
      " ('5658', 0.5669200420379639),\n",
      " ('3995', 0.5668214559555054),\n",
      " ('45631', 0.5628659725189209),\n",
      " ('90513', 0.5613369941711426),\n",
      " ('36305', 0.5602449774742126)]\n"
     ]
    }
   ],
   "source": [
    "vector_model_file = vector_model_path + \"model1\"\n",
    "\n",
    "vector_model = gensim.models.Doc2Vec.load(vector_model_file)\n",
    "\n",
    "inferred_vector = vector_model.infer_vector(['congenital', 'prolong', 'rupture', 'premature', 'membrane', 'lung'])\n",
    "pprint(vector_model.docvecs.most_similar([inferred_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "feature_number = 1024\n",
    "\n",
    "def readFromPairList(id_pair_list, id_notPair_list):\n",
    "    pair_list = id_pair_list + id_notPair_list\n",
    "    random.shuffle(pair_list)\n",
    "    idpairs_list =[]\n",
    "    label_list =[]\n",
    "    for i, line in enumerate(pair_list):      \n",
    "        idpairs_list.append([line[0], line[1]])\n",
    "        label_list.append(line[2])\n",
    "    return idpairs_list, label_list\n",
    "\n",
    "idpairs_list, label_list= readFromPairList(conceptPairList, conceptNotPairList)\n",
    "\n",
    "print(label_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5055', '7611'], ['8921', '27786'], ['68666', '27268'], ['5787', '135090'], ['38150', '7148'], ['139744', '139753'], ['5730', '8210'], ['7080', '7378'], ['8060', '7714'], ['6081', '4653'], ['3469', '6814'], ['9144', '68689'], ['8726', '8106'], ['96878', '7130'], ['5628', '5629'], ['3403', '121619'], ['5820', '6421'], ['7244', '7237'], ['3856', '9011'], ['9167', '3692']]\n",
      "[['5055', '7611'], ['8921', '27786'], ['68666', '27268'], ['5787', '135090'], ['38150', '7148'], ['139744', '139753'], ['5730', '8210'], ['7080', '7378'], ['8060', '7714'], ['6081', '4653'], ['3469', '6814'], ['9144', '68689'], ['8726', '8106'], ['96878', '7130'], ['5628', '5629'], ['3403', '121619'], ['5820', '6421'], ['7244', '7237'], ['3856', '9011'], ['9167', '3692']]\n",
      "[1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(idpairs_list, label_list, test_size = 0.2, shuffle= True)\n",
    "print(X_train[:20])\n",
    "print(X_train[:20])\n",
    "print(y_train[:20])\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 1024\n",
    "\n",
    "def getVectorFromModel(concept_id, conceptLabelDict, model):\n",
    "    if concept_id in model.docvecs:\n",
    "        concept_vector= model.docvecs[concept_id]\n",
    "    else:\n",
    "        print(\"%s not found, get inferred vector \"%(concept_id))\n",
    "        concept_label = conceptLabelDict[concept_id]\n",
    "        concept_vector= model.infer_vector(concept_label.split())\n",
    "    return concept_vector\n",
    "\n",
    "def getVector(line, conceptLabelDict, model):        \n",
    "    a = getVectorFromModel(line[0], conceptLabelDict, model)\n",
    "    b = getVectorFromModel(line[1], conceptLabelDict, model)\n",
    "    c = np.array((a, b))\n",
    "    c = c.T \n",
    "#     c = np.expand_dims(c, axis=2)\n",
    "    print(c.shape)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackVector(vector):\n",
    "    from numpy import dstack\n",
    "    return dstack((vector, vector, vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes=2 \n",
    "\n",
    "def generator(x_samples, y_samples, train_flag, batch_size=64):\n",
    "    samples = list(zip(x_samples, y_samples))\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            X_samples = []\n",
    "            Y_samples= []\n",
    "            for batch_sample in batch_samples:\n",
    "                pair_list = batch_sample[0]\n",
    "                data_vector = getVector(pair_list, conceptLabelDict, vector_model)\n",
    "                data_vector = stackVector(data_vector)\n",
    "                print(data_vector.shape)\n",
    "                X_samples.append(data_vector)\n",
    "                class_label = batch_sample[1] \n",
    "                Y_samples.append(class_label)\n",
    "                \n",
    "            X_samples = np.array(X_samples).astype('float32')\n",
    "            Y_samples = np.eye(nb_classes)[Y_samples]\n",
    "            print('one batch ready')\n",
    "            yield shuffle(X_samples, Y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_batch_size = 64\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(X_train, y_train, train_flag=True, batch_size=set_batch_size)\n",
    "validation_generator = generator(X_test, y_test, train_flag=False, batch_size=set_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model??\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = applications.resnet50.ResNet50(weights=None, include_top = True, classes=2)\n",
    "print('Model loaded')\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
    "csv_logger = CSVLogger(directory_path + 'resnet18_cifar10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "one batch ready\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "(200, 2)\n",
      "(200, 2, 3)\n",
      "one batch ready\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (224, 224, 3) but got array with shape (200, 2, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c863a12fabe0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mset_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     callbacks=[lr_reducer, early_stopper, csv_logger])\n\u001b[0m",
      "\u001b[1;32md:\\Anaconda3\\envs\\MLOntology\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\MLOntology\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\MLOntology\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1875\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1877\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1878\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1879\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\MLOntology\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\MLOntology\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    121\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (224, 224, 3) but got array with shape (200, 2, 3)"
     ]
    }
   ],
   "source": [
    "myepochs = 10\n",
    "\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=len(X_train)//set_batch_size, \n",
    "                    epochs=myepochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=len(X_test)//set_batch_size,\n",
    "                    callbacks=[lr_reducer, early_stopper, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# vstack\n",
    "from numpy import vstack\n",
    "from numpy import hstack\n",
    "\n",
    "a1 = np.zeros((1, 2,512))\n",
    "# print(a1)\n",
    "print(a1.shape)\n",
    "a2 = np.ones((1, 2,512))\n",
    "# print(a2)\n",
    "print(a2.shape)\n",
    "a3 = np.ones((1, 2,512))\n",
    "# print(a3)\n",
    "print(a3.shape)\n",
    "a4 = vstack((a1, a2, a3))\n",
    "print(a4)\n",
    "print(a4.shape)\n",
    "\n",
    "\n",
    "a1 = np.zeros((512,1))\n",
    "# print(a1)\n",
    "print(a1.shape)\n",
    "a2 = np.ones((512,1))\n",
    "# print(a2)\n",
    "print(a2.shape)\n",
    "a3 = np.ones((512,1))\n",
    "# print(a3)\n",
    "print(a3.shape)\n",
    "a4 = hstack((a1, a2, a3))\n",
    "print(a4)\n",
    "print(a4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# vstack\n",
    "from numpy import vstack\n",
    "from numpy import hstack\n",
    "from numpy import dstack\n",
    "\n",
    "a1 = np.zeros((2, 1))\n",
    "# print(a1)\n",
    "print(a1.shape)\n",
    "a2 = np.ones((2, 1))\n",
    "# print(a2)\n",
    "print(a2.shape)\n",
    "\n",
    "a3 = hstack((a1, a2))\n",
    "# print(a3)\n",
    "print(a3.shape)\n",
    "\n",
    "a4 = dstack((a3, a3, a3))\n",
    "print(a4.shape)\n",
    "print(a4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
